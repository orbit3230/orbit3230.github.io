---
layout: post
title: "[컴퓨터구조] 2주차 - Computer Abstractions and Technology"
excerpt: "Components of a Computer, Inside a Computer (Past & Now), Eight Great Ideas, End of Moore's Law, Future Opportunities"

tags:
  - [컴퓨터구조]

toc: true

date: 2025-09-09
last_modified_at: 2025-09-09
---
## Computer Abstractions and Technology
### 1. Components of a Computer
- Processor
- Memory
- Interconnects
  - NoC(Network-on-Chip), Processor-interconnect, large-scale network
- I/Os
  - User-interface devices: 디스플레이, 키보드, 마우스, 사운드 장치, 카메라, ...
- Storage devices: HDD, SSD, CD/DVD, ...
- Network adaptors: 이더넷, 3G/4G/5G, WiFi, Bluetooth, ...  

<br>

### 2. Inside a Computer (Past)
- Inside the Processor(CPU)  
![inside_cpu_1][def]  
  - Datapath : 데이터에 대한 연산을 수행
  - Control : Datapath와 메모리, I/O 장치들에게 무엇을 해야 할 지 지시
![inside_cpu_2][def2]  

<br>

- Inside the Memory  
  - Offchip memory controller (~2008)  
  ![offchip_memory_1][def3]  
  ![onchip_memory_2][def4]  
    - memory controller : DRAM 칩을 제어  

  - DRAM Technology  
  ![dram][def5]  

<br>

### 3. Inside a Computer (Now)
- What Happened?  
![what_happened][def6]  

<br>

- Reviewing 40 years of Moore's Law  
![moores_law][def7]  
  - 40+년간 매 해 1.4x의 performance 향상  

<br>

- Inside the Processor(CPU)
  - Status Quo (Intel I7-3960X)  
  ![inside_cpu_now][def8]  

  - Status Quo (AMD Ryzen 5000, Zen 3rd)  
  ![inside_cpu_amd][def9]  

<br>

- Inside the Memory
  - High-Bandwidth, 3D stacked memory  
  ![hmc_memory_chip_architecture][def10]  
  ![stacked_memory][def11]  

<br>

- Interconnect in CPU  
  - Interconnect는 compute unit의 수가 증가함에 따라 중요해진다.  
  ![interconnect][def12]  

  - Network-on-Chip (NoC)  
  ![noc][def13]  

  - Network-on-Chip (NoC) - Mesh topology  
  ![noc_mesh][def14]  

<br>

### 4. Eight Great Ideas
- Design for ***Moore's Law***
- Use ***Abstraction*** to Simplify Design
- Make the ***Common Case*** Fast
- Performance via ***Parallelism***
- Performance via ***Pipelining***
- Performance via ***Prediction***
- ***Hierarchy*** of Memories
- ***Dependability*** via Redundancy

<br>

#### [1] Abstractions  
![abstraction][def17]  
- 추상화는 복잡한 디테일을 숨겨 쉽게 이해하고 다룰 수 있게 한다.
- API (Application Programming Interface)
- ABI (Application Binary Interface)
- ISA (Instruction Set Architecture)

<br>

#### [2] Parallelism
- Implicit Parallelism : Instruction-Level Parallelism (ILP)  
  - 프로그래머의 관점에서, 일련의 명령어는 순차적으로 실행된다.
  - 그러나 실제로 하드웨어는 병렬로 실행한다.
  - Pipelining
  - Speculation (Prediction)
  - Caching
  - Superscalar (multiple instructions per cycle)
  - Dynamic Scheduling (out-of-order execution)

- Explicit Parallelism : Data & Thread-Level Parallelism
  - 하드웨어는 명령어를 동시에 실행하기 위해 병렬 리소스를 제공한다.
  - 왜? -> instruction-level parallelism의 한계  

<br>

### 5. End of Moore's Law  
- Uniprocessor Performance Growth (Single-core)  
![uniprocessor_performance_growth_1][def15]  
![uniprocessor_performance_growth_2][def16]  
  - 2003년 이후로 성능 향상이 거의 없음.  

  <br>

- Denard Scaling  
  - 트랜지스터의 크기가 작아짐에 따라 전력 밀도는 일정하게 유지된다는 이론.
  - `Power` = `α` * `C` * `F` * `V`<sup>`2`</sup>
    - `α` : 트랜지스터가 실제로 스위칭되는 시간의 비율
    - `C` : capacitance. 트랜지스터 크기에 비례
    - `F` : 동작 주파수 (클럭 속도)
    - `V` : 전압
  - Capacitance는 트랜지스터 크기에 비례하므로,  
  트랜지스터 크기가 작아지면 같은 전력으로 더 높은 주파수를 쓸 수 있어 성능이 향상되었다.

- End of Demand Scaling  
  - 그러나 Dennard Scaling은 누설 전류(leakage current)와 임계 전압(threshold voltage)을 고려하지 않았다.
  - 따라서 트랜지스터 당 최소 전력 소모량이 생겼다.
  - 결과적으로 발열이 심해져서 **Power Wall**에 부딪혔다.

- End of Dennard Scaling is a Crisis
  - 프로세스들은 power limit에 도달했다.  
  - 에너지 소비는 유저들에게 더 중요한 제약 조건이 되었다.  

<br>

### 6. Future Opportunities  
- Domain-Specific Architecture (DSA)  
  - 특정 도메인에 특화된 아키텍처
  - a.k.a. 'Accelerator'
  - GPU for graphic processing
  - Neural network processor for deep learning
  - Processor for software-defined network
  - [Note] 위들 모두 결국엔 general-purpose processor와 유사한 아키텍처 기법을 공유한다.

  <br>

- Domain-Specific Language (DSL)  
  - DSA는 아키텍처에 고수준 연산을 요구하지만, Python, Java, C와 같은 언어로는 어렵다. (컴파일 과정에서 결국 범용 CPU용 ISA로 변환됨)
  - 따라서 DSL은 이 과정을 가능하게 하여 DSA를 효율적으로 활용할 수 있게 한다.
  - Matlab -> 행렬 연산을 위한 언어
  - TensorFlow -> DNN(Deep Neural Network)을 위한 데이터 흐름 언어
  - P4 -> SDN(Software-Defined Network)을 위한 언어
  - Halide -> 이미지 프로세싱을 위한 언어

  <br>

- Secure Architecture & S/W  
  - Control Isolation (제어 격리)
  - Data Isolation (데이터 격리)
  - Constant-time execution (시간 예측 가능)
  - Avoiding speculative execution (투기적 실행 회피)
  - Avoiding shared resources (공유 자원 회피)

  <br>

- Energy-Efficient Architecture & H/W  
  - 명령어 개수 최소화
  - 더 적은 데이터 이동
  - 더 적은 Communications (e.g., NoC, interconnect, network)
  - Data-centric architecture (e.g., PIM(processor-in-memory))

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">  

이 포스팅은 강원대학교 송원준 교수님의 컴퓨터구조 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.  

</div>
</details>

[def]: https://i.imgur.com/sRNgMq3.png
[def2]: https://i.imgur.com/QvIOslR.png
[def3]: https://i.imgur.com/sGeUlKH.png
[def4]: https://i.imgur.com/cRqDFY9.png
[def5]: https://i.imgur.com/wmORDFP.png
[def6]: https://i.imgur.com/arAWZRE.png
[def7]: https://i.imgur.com/ZOta6xP.png
[def8]: https://i.imgur.com/gUx2tSP.png
[def9]: https://i.imgur.com/2P6OmX4.png
[def10]: https://i.imgur.com/0ctAntL.png
[def11]: https://i.imgur.com/N9Ddjpj.png
[def12]: https://i.imgur.com/eI47v1o.png
[def13]: https://i.imgur.com/fYL6QpR.png
[def14]: https://i.imgur.com/SV0FWxh.png
[def15]: https://i.imgur.com/a165P4y.png
[def16]: https://i.imgur.com/IYuOXM8.png
[def17]: https://i.imgur.com/ZMUSEHp.png