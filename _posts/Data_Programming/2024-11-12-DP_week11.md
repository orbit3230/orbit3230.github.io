---
layout: post
title: "[데이터 분석 프로그래밍] 11주차 - Regression Analysis"
excerpt: "Linear Regression, Simple Regression, Multiple Regression, Logistic Regression, Ridge Regression, Lasso Regression, Polynomial Regression, K-Nearest Neighbors (KNN) Regression"

tags:
  - [데이터 분석 프로그래밍, Python]

toc: true

date: 2024-11-12
last_modified_at: 2024-11-13
---
## Regression Analysis
### 1. Linear Regression
#### Machine Learning
- Traditional program은 사람이 inputs과 outputs 간의 관게를 정의하는 규칙을 만들어내는 것이었다.  

- 반면 Machine Learning은, 커다란 양의 inputs&outputs 데이터를 이용하여  
machine learning models를 학습시켜 서비스에 적용하는 것이다.  

![traditional_vs_machine_learning][def]  

<br>

- Machine Learning의 학습 방식은 크게 두 카테고리로 나뉜다.  
  - **Supervised Learning** : labeled data를 이용하여 학습  
  <sup>labeled data : 데이터에 대한 답이 주어져 있는 것</sup>  
    - input 값들이 주어지면, 이에 상응하는 labels들이 학습을 위해 주어진다.  
    - 주로 classification이나 regression 문제에 사용된다.  

  - **Unsupervised Learning** : labeled data 없이 학습
    - 주로 데이터와 그룹 데이터 포인트들 속에서 패턴을 찾는 데 사용된다.  
    - clustering과 dimensionality reduction으로 유명하다.  

    <br>

#### Regression Analysis
- **Regression analysis**는 종속 변수와 독립 변수와의 **관계**를 이해하고, 종속 변수의 값을 **예측**하는 데에 사용된다.  
  - 주된 목적은 독립변수가 어떻게 변화하냐에 따라 종속변수에 얼마나 영향을 미치는 지 이해하고,  
  이해를 통해 종속변수의 값을 예측하는 것이다.  
  - 종속 변수는 numerical data가 될 수도 있고,  
  독립 변수는 numerical data, categorical data가 될 수 있다.  

  ![regression_analysis][def2]  

  <br>

- Type of Regression Analysis
  - **Simpile Regression** : 하나의 독립 변수와 하나의 종속 변수 간의 관계를 분석
  - **Multiple Regression** : 여러 개의 독립 변수와 하나의 종속 변수 간의 관계를 분석
  - **Logistic Regression** : 종속 변수가 categorical data인 경우 사용
  - **Ridge Regression** & **Lasso Regression** : Regularization을 통해 overfitting을 방지하는 regression  
  - **Polynomial Regression** : 독립 변수와 종속 변수가 nonlinear 관계일 때 사용  

  <br>

#### Linear Regression
- **Linear regression**은 두 변수 간의 관게를 예측하는 통계적 분석 기법이다.  
  - 독립 변수들과 종속 변수 간의 **선형 관계**를 가정한다.  
  - 관게를 표현하는 **best-fitting line**을 찾는 데에 초점을 둔다.
  - 목표는 예측 값과 실제 값의 차이의 sum of squares를 최소화 하는 것이다.  

  ![linear_regression][def3]  

  <br>

- ***Simple Linear Regression***: input variable `X`가 단 하나일 때
  - `y = β`<sub>`0`</sub> + `β`<sub>`1`</sub>`x`
    - `y` : Dependent variable
    - `β`<sub>`0`</sub> : constant / intercept
    - `β`<sub>`1`</sub> : slope / intercept
    - `x` : Independent variable  

  - 목표는 best-fitting line을 구성하는 `β`<sub>`0`</sub>와 `β`<sub>`1`</sub>을 찾는 것이다.  

  ![simple_regression][def4]

  <br>

- best-fit line은 error를 최소화하는 것이고, 이 말인 즉슨 predicted values와 actual values의 discrepancies를 최소화하는 것이다.  

- error를 표현하기 위해 Residual(`ε`<sub>`i`</sub>)이 정의되었다.  
`ε`<sub>`i`</sub> = `y`<sub>`i`</sub> - `ŷ`<sub>`i`</sub>  
  - `y`<sub>`i`</sub> : actual value
  - `ŷ`<sub>`i`</sub> : predicted value  

  ![residual][def5]

  <br>

- 즉, 수학적으로 best fit line은  
Residual Sum of Squares(RSS)를 최소화한다.  

- Regression model 평가 metrics
  - (1) Mean Absolute Error (MAE)  
  ![MAE][def6]  
    - error의 절댓값의 평균  
  ![MAE_graph][def7]
  
  - (2) Mean Squared Error (MSE)  
  ![MSE][def8]  
    - error의 제곱의 평균 
  ![MSE_graph][def9]

  - (3) Root Mean Squared Error (RMSE)  
  ![RMSE][def10]  
    - MSE의 제곱의 루트  

  - (4) R<sup>2</sup> (R-squared)  
  <sup>`R` : 결정 계수(coefficient of determination)</sup>  
  ![R_squared][def11]  
    - 1에 가까울수록 모델이 데이터를 잘 설명한다.  
    - Total Sum of Squares(TSS)  
    ![TSS][def12]
    - Residual Sum of Squares(RSS)  
    ![RSS][def13]  
  ![R_squared_graph][def14]  

  <br>

- Simple regression in Python

```py
# data preprocessing
df = df['alcohol', 'density']

# data visualization
plt.scatter(df['alcohol'], df['density'])
plt.xlabel('Alcohol')
plt.ylabel('Density')
plt.title('Alcohol vs Density')
plt.show()  
```

![single_regression_scatter][def15]  

<br>

```py
# Building a Simple Linear Regression Model
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X = df[['alcohol']]  # Must be made into 2D array
y = df['density']

# Seperate into training data and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train models
model = LinearRegression()
model.fit(X_train, y_train)
```

<br>

```py
# Model Evaluation
from sklearn.metrics import mean_squared_error, r2_score

# Perform prediction
y_pred = model.predict(X_test)

# Calculate evaluation indicators
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')
```

![single_regression_result][def18]

<br>

```py
# Interpretation of results
# Draw a regression line
plt.scatter(X_test, y_test, color='blue')
plt.plot(X_test, y_pred, color='red', linewidth=2)
plt.xlabel('Alcohol')
plt.ylabel('Density')
plt.title('Alcohol vs Density (with Regression Line)')
plt.show()
```

![single_regression_result][def16]  

<br>

- ***Multiple Linear Regression***: input variable `X`가 여러 개일 때
  - `y = β`<sub>`0`</sub> + `β`<sub>`1`</sub>`x`<sub>`1`</sub> + `β`<sub>`2`</sub>`x`<sub>`2`</sub> + ... + `β`<sub>`n`</sub>`x`<sub>`n`</sub>  
    - `y` : Dependent variable
    - `β`<sub>`0`</sub> : constant / intercept
    - `β`<sub>`1`</sub>, `β`<sub>`2`</sub>, ... : coefficients
    - `x`<sub>`1`</sub>, `x`<sub>`2`</sub>, ... : Independent variables  

  <br>

- Multiple regression의 한계와 주의사항  
  - **Multicollinearity** : 독립 변수들 간 상관관계가 강할 때, regression model이 불안정할 수 있다.  
  - **Overfitting** : model에 너무 많은 변수들이 포함되면, model이 데이터를 overfitting할 위험이 있다.  
  overfitting 되면, 모델이 현재 training data에 대해서'만' 잘 작동하고,  
  새로운 데이터에 대한 예측력이 떨어지게 된다.  
  - **Selection of Independent variables** : 중요한 독립 변수들만 선택하는 것은 매우 중요하다.  
  불필요한 변수를 포함하는 것은 모델을 복잡하게 만들고 예측 성능을 떨어뜨린다.  

  <br>

- Multiple regression in Python  

```py
# data preprocessing
X = df[['alcohol', 'sulphates', 'citric acid', 'volatile acidity', 'dencity', 'chlorides', 'total sulfur dioxide']]
y = df['quality']
```

<br>

```py
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Data partitioning
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Building Multiple Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)
```

<br>

```py
# Model Evaluation
from sklearn.metrics import mean_squared_error, r2_score

# Perform prediction
y_pred = model.predict(X_test)

# Calculate evaluation indicators
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')
```

<br>

```py
# Interpretation of results
coefficients = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])  
print(coefficients)
```

![multiple_regression_result][def17]  

<br>

### 2. Polynomial Regression
- **Polynomial regression** : Linear Regression의 variation으로,  
non-linear 데이터를 모델링하는 데에 쓰인다.  

- 이 접근법은 polynomial function을 사용하여 데이터를 추정한다.  
  - `y = β`<sub>`0`</sub> + `β`<sub>`1`</sub>`x` + `β`<sub>`2`</sub>`x`<sup>`2`</sup> + ... + `β`<sub>`n`</sub>`x`<sup>`n`</sup>
    - `y` : Dependent variable
    - `x` : Independent variable
    - `β`<sub>`0`</sub> : intercept
    - `β`<sub>`1`</sub>, `β`<sub>`2`</sub>, ... : regression coefficients  

    <br>

- Polynomial  regression의 Pros/Cons
  - 장점
    - non-linear 데이터에 적합
    - Model flexibility
    - 높은 예측 정확도
  - 단점
    - Overfitting의 위험
    - 해석이 어려움
    - degree selection이 어려움  

    <br>

- Polynomial regression in Python

```py
# data preprocessing
df = df[['alcohol', 'quality']]
```

```py
from sklearn.preprocessing import PolynomialFeatures

# Setting variables
X = df[['alcohol']]
y = df['quality']

# Create polynomial features
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
X_poly
```

![polynomial_features](https://i.imgur.com/iT16NQl.png)  

```py
# Building Polynomial Regression Model
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Separate into training data and testing data
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Create and train models
model = LinearRegression()
model.fit(X_train, y_train)
```

```py
# Model Evaluation
from sklearn.metrics import mean_squared_error, r2_score

# Perform prediction
y_pred = model.predict(X_test)

# Calculate evaluation indicators
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')
```

![polynomial_regression_result][def19]  

- Simple Regression 및 Multiple Regression과 비교해보자면,  
![comparison][def20]  

```py
# Result visualization
plt.scatter(X, y, color='blue')
X_sorted = np.sort(X, axis=0)
y_poly_pred = model.predict(poly.transform(X_sorted))
plt.plot(X_sorted, y_poly_pred, color='red', linewidth=2)

plt.xlabel('Alcohol')
plt.ylabel('Quality')
plt.title('Polynomial Regression: Alcohol vs Quality')
plt.show()
```

![polynomial_regression_result][def21]  

```py
# Find the optimal degree
max_degree = 6  # Set maximum degree
mse_list = []
# Train the model for each degree and record
for degree in range(1, max_degree + 1) :
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    y_pred = model.predict(X_test_poly)
    mse = mean_squared_error(y_test, y_pred)
    mse_list.append(mse)
    print(f'Degree {degree} - MSE: {mse}')
```

![polynomial_regression_result2][def22]

<br>

- **Multivariate Polynomial Regression**
  - 관계 모델을 형성하는데 있어  
  하나의 종속 변수로 두 개 이상의 독립 변수를 사용하는 것이다.  
  - 각 독립 변수들의 interaction을 고려하여 더 정교한 모델을 형성한다.  

- 두 개의 독립 변수 `x`<sub>`1`</sub>과 `x`<sub>`2`</sub>이 있다고 하자.  
- Quadratic multivariate polynomial regression  
`y = β`<sub>`0`</sub>+ `β`<sub>`1`</sub>`x`<sub>`1`</sub> + `β`<sub>`2`</sub>`x`<sub>`2`</sub> + `β`<sub>`3`</sub>`x`<sub>`1`</sub><sup>`2`</sup> + `β`<sub>`4`</sub>`x`<sub>`2`</sub><sup>`2`</sup> + `β`<sub>`5`</sub>`x`<sub>`1`</sub>`x`<sub>`2`</sub>  
  - `x`<sub>`1`</sub>`x`<sub>`2`</sub> : interaction term  

- 실제 세계의 복잡한 데이터를 설명하는 데에 유용한 도구이다.  

<br>

- Multivariate Polynomial Regression in Python

```py
X = df[['alcohol', 'sulphates']]
y = df['quality']
# Separate into training data and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create polynomial features
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)
```

```py
# Create and train models
model = LinearRegression()
model.fit(X_train_poly, y_train)
# Perform prediction
y_pred = model.predict(X_test_poly)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')
```

![multivariate_polynomial_regression_result][def23]  

<br>

### 3. K-Nearest Neighbors (KNN) Regression
- **K-Nearest Neighbors (KNN) Regression** : `k`개의 가장 가까운 neighbors 데이터들을 찾고 그들 간의 평균값을 이용하여 값을 예측하는 regression 방법.  
  - KNN algorithm을 사용하여 nearset neighbors를 찾기 때문에 이러한 이름이 붙었다.  

- KNN regression은 non-parametric regression 수단이다.  

- Parametric Model vs. Non-parametric Model  
  - Parametric Model
    - 데이터가 특정한 분포를 따른다고 가정한다.  
    - 데이터를 고정된 개수의 파라미터로 설명한다.
    - Simple Regression `y = β`<sub>`0`</sub> + `β`<sub>`1`</sub>`x`

  - Non-parametric Model
    - 데이터가 특정한 분포를 따른다고 가정하지 않는다.
    - 고정된 개수의 파라미터란 없다.
    - KNN Regression

    <br>

- Let's understand KNN
  - `height`, `age`, `weight` 세 가지의 attributes로 이루어진 데이터들이 존재한다.  
  - 우리의 목표는, 키가 `5.7`ft & `37`살인 사람의 `weight`을 예측하는 것이다.  
  ![KNN_point][def24]  
  
  - 거리를 계산하는 데에는 Euclidean distance가 주로 사용된다.  
  ![euclidean_distance][def25]  

  - 우선 예측하고자 하는 데이터와, 다른 모든 데이터들 간의 거리를 계산한다.  
  - 거리를 기반으로 `k`개의 가장 가까운 data points를 선택한다.  
  ![KNN_selection][def26]  

  - `k`-nearest neighbors들 간의 평균 값으로 예측을 완료한다.  
  ![KNN_prediction][def27]  

  <br>

- KNN regression in Python  

```py
# Data preprocessing
X = df[['alcohol', 'sulphates', 'citric acid', 'volatile acidity', 'dencity', 'chlorides', 'total sulfur dioxide']]
y = df['quality']
```

```py
# Data partitioning
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

```py
# Building KNN Regression Model
from sklearn.neighbors import KNeighborsRegressor

# Create KNN Regression model (set k=5)
knn_model = KNeighborsRegressor(n_neighbors=5)

# Model training
knn_model.fit(X_train, y_train)
```

```py
# Model Evaluation
from sklearn.metrics import mean_squared_error, r2_score

# Perform prediction
y_pred = knn_model.predict(X_test)

# Calculate evaluation indicators
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')
```

![KNN_regression_result][def28]  
- 정확도가 다소 떨어지긴 한다. (`k` 값이 충분히 크지 못했음)

- Find the optimal `k` value  

```py
# Compare model performance according to various k values
k_values = [1, 3, 5, 7, 9]
for k in k_values :
    knn_model = KNeighborsRegressor(n_neighbors=k)
    knn_model.fit(X_train, y_train)
    y_pred = knn_model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f'k = {k}: MSE={mse}, R-squared={r2}')
```

![KNN_regression_result][def29]  

<br>

- Weighted KNN Regression
  - Basic concepts : prediction 시에 nearby neighbors들 중 영향력이 큰 데이터들에 더 높은 가중치를 부여하여 더 높은 정확도의 예측을 수행.  

  - basic KNN의 문제점
    - 일반적인 KNN regression 에서는 모든 neighbors들이 같은 중요도를 가졌음.
    - 만약 두 neightbors들이 아주 아주 가까워도 세 번째 neighbor가 상대적으로 멀리 떨어져 있으면,  
    단순히 이들 간의 평균을 내는 것은 비효율적일 수 있음.
    - 가까운 neightbors들이 더 중요한 정보를 제공할 가능성이 높기 때문.  

  - Solution for weighted KNN regression  
    - Weighted KNN regression은 각 neighbor들에 가중치를 부여하여 해결.  
      - Nearby neighbors : higher weight
      - Distant neighbors : lower weight  

<br>

- Weighted KNN regression in Python  

```py
# Create KNN model (k=9 setting, distance-based weight)
knn_model_weighted = KNeighborsRegressor(n_neighbors=9, weights='distance')
# Model training
knn_model_weighted.fit(X_train, y_train)
# Perform prediction
y_pred_weighted = knn_model_weighted.predict(X_test)
# Calculate evaluation indicators
mse_weighted = mean_squared_error(y_test, y_pred_weighted)
r2_weighted = r2_score(y_test, y_pred_weighted)
print(f'MSE : {mse_weighted}, R-squared : {r2_weighted}')  
```

![weighted_KNN_regression_result][def30]  

- 훨씬 더 높은 정확도를 보여준다.  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 장홍준 교수님의 데이터분석프로그래밍 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/ndw8pnx.png
[def2]: https://i.imgur.com/juOw1iR.png
[def3]: https://i.imgur.com/DFyYbgk.png
[def4]: https://i.imgur.com/AbMZlpx.png
[def5]: https://i.imgur.com/PUSdtMW.png
[def6]: https://i.imgur.com/LHGiUy9.png
[def7]: https://i.imgur.com/1i9bhz8.png
[def8]: https://i.imgur.com/aubaEtp.png
[def9]: https://i.imgur.com/aINweFa.png
[def10]: https://i.imgur.com/68MWzrh.png
[def11]: https://i.imgur.com/Cm8Lcz9.png
[def12]: https://i.imgur.com/xiOoo2d.png
[def13]: https://i.imgur.com/9urRsuq.png
[def14]: https://i.imgur.com/AaDX8FL.png
[def15]: https://i.imgur.com/YzDofPo.png
[def16]: https://i.imgur.com/8raH46W.png
[def17]: https://i.imgur.com/pybrypQ.png
[def18]: https://i.imgur.com/rwQQGFn.png
[def19]: https://i.imgur.com/LW8S95t.png
[def20]: https://i.imgur.com/dFu9krW.png
[def21]: https://i.imgur.com/gAzllx2.png
[def22]: https://i.imgur.com/aGjRmH8.png
[def23]: https://i.imgur.com/CkMnIHN.png
[def24]: https://i.imgur.com/ccuhXcQ.png
[def25]: https://i.imgur.com/wMmybXo.png
[def26]: https://i.imgur.com/YZJVIUX.png
[def27]: https://i.imgur.com/DtFNwVx.png
[def28]: https://i.imgur.com/KmkCCh0.png
[def29]: https://i.imgur.com/YJGbIYz.png
[def30]: https://i.imgur.com/bY2tmil.png