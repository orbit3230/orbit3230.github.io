---
layout: post
title: "[데이터 분석 프로그래밍] 11주차 - Regression Analysis"
excerpt: ""

tags:
  - [데이터 분석 프로그래밍, Python]

toc: true

date: 2024-11-12
last_modified_at: 2024-11-12
---
## Regression Analysis
### 1. Linear Regression
#### Machine Learning
- Traditional program은 사람이 inputs과 outputs 간의 관게를 정의하는 규칙을 만들어내는 것이었다.  

- 반면 Machine Learning은, 커다란 양의 inputs&outputs 데이터를 이용하여  
machine learning models를 학습시켜 서비스에 적용하는 것이다.  

![traditional_vs_machine_learning](TODO)  

<br>

- Machine Learning의 학습 방식은 크게 두 카테고리로 나뉜다.  
  - Supervised Learning : labeled data를 이용하여 학습  
  <sup>labeled data : 데이터에 대한 답이 주어져 있는 것</sup>  
    - input 값들이 주어지면, 이에 상응하는 labels들이 학습을 위해 주어진다.  
    - 주로 classification이나 regression 문제에 사용된다.  

  - Unsupervised Learning : labeled data 없이 학습
    - 주로 데이터와 그룹 데이터 포인트들 속에서 패턴을 찾는 데 사용된다.  
    - clustering과 dimensionality reduction으로 유명하다.  

    <br>

#### Regression Analysis
- Regression analysis는 종속 변수와 독립 변수와의 관계를 이해하고, 종속 변수의 값을 예측하는 데에 사용된다.  
  - 주된 목적은 독립변수가 어떻게 변화하냐에 따라 종속변수에 얼마나 영향을 미치는 지 이해하고,  
  이해를 통해 종속변수의 값을 예측하는 것이다.  
  - 종속 변수는 numerical data가 될 수도 있고,  
  독립 변수는 numerical data, categorical data가 될 수 있다.  

  ![regression_analysis](TODO)  

  <br>

- Type of Regression Analysis
  - Simpile Regression : 하나의 독립 변수와 하나의 종속 변수 간의 관계를 분석
  - Multiple Regression : 여러 개의 독립 변수와 하나의 종속 변수 간의 관계를 분석
  - Logistic Regression : 종속 변수가 categorical data인 경우 사용
  - Ridge Regression & Lasso Regression : Regularization을 통해 overfitting을 방지하는 regression  
  - Polynomial Regression : 독립 변수와 종속 변수가 nonlinear 관계일 때 사용  

  <br>

#### Linear Regression
- Linear regression은 두 변수 간의 관게를 예측하는 통계적 분석 기법이다.  
  - 독립 변수들과 종속 변수 간의 선형 관계를 가정한다.  
  - 관게를 표현하는 best-fitting line을 찾는 데에 초점을 둔다.
  - 목표는 예측 값과 실제 값의 차이의 sum of squares를 최소화 하는 것이다.  

  ![linear_regression](TODO)  

  <br>

- ***Simple Linear Regression***: input variable `X`가 단 하나일 때
  - `y = β`<sub>`0`</sub> + `β`<sub>`1`</sub>`x`
    - `y` : Dependent variable
    - `β`<sub>`0`</sub> : constant / intercept
    - `β`<sub>`1`</sub> : slope / intercept
    - `x` : Independent variable  

  - 목표는 best-fitting line을 구성하는 `β`<sub>`0`</sub>와 `β`<sub>`1`</sub>을 찾는 것이다.

  <br>

- best-fit line은 error를 최소화하는 것이고, 이 말인 즉슨 predicted values와 actual values의 discrepancies를 최소화하는 것이다.  

- error를 표현하기 위해 Residual(`ε`<sub>`i`</sub>)이 정의되었다.  
`ε`<sub>`i`</sub> = `y`<sub>`i`</sub> - `ŷ`<sub>`i`</sub>  
  - `y`<sub>`i`</sub> : actual value
  - `ŷ`<sub>`i`</sub> : predicted value  

  <br>

- 즉, 수학적으로 best fit line은  
Residual Sum of Squares(RSS)를 최소화한다.  

- Regression model 평가 metrics
  - (1) Mean Absolute Error (MAE)  
  ![MAE](TODO)  
    - error의 절댓값의 평균  
  ![MAE_graph](TODO)
  
  - (2) Mean Squared Error (MSE)  
  ![MSE](TODO)  
    - error의 제곱의 평균 
  ![MSE_graph](TODO)

  - (3) Root Mean Squared Error (RMSE)  
  ![RMSE](TODO)  
    - MSE의 제곱의 루트  

  - (4) R<sup>2</sup> (R-squared)  
  <sup>`R` : 결정 계수(coefficient of determination)</sup>  
  ![R_squared](TODO)  
    - 1에 가까울수록 모델이 데이터를 잘 설명한다.  
    - Total Sum of Squares(TSS)  
    ![TSS](TODO)
    - Residual Sum of Squares(RSS)  
    ![RSS](TODO)  
  ![R_squared_graph](TODO)  

  <br>

- Simple regression in Python

```py
# data preprocessing
df = df['alcohol', 'density']

# data visualization
plt.scatter(df['alcohol'], df['density'])
plt.xlabel('Alcohol')
plt.ylabel('Density')
plt.title('Alcohol vs Density')
plt.show()  
```

![single_regression_scatter](TODO)  

<br>

```py
# Building a Simple Linear Regression Model
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X = df[['alcohol']]  # Must be made into 2D array
y = df['density']

# Seperate into training data and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train models
model = LinearRegression()
model.fit(X_train, y_train)
```

<br>

```py
# Model Evaluation
from sklearn.metrics import mean_squared_error, r2_score

# Perform prediction
y_pred = model.predict(X_test)

# Calculate evaluation indicators
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')
```

<br>

```py
# Interpretation of results
# Draw a regression line
plt.scatter(X_test, y_test, color='blue')
plt.plot(X_test, y_pred, color='red', linewidth=2)
plt.xlabel('Alcohol')
plt.ylabel('Density')
plt.title('Alcohol vs Density (with Regression Line)')
plt.show()
```

![single_regression_result](TODO)  

<br>

- ***Multiple Linear Regression***: input variable `X`가 여러 개일 때
  - `y = β`<sub>`0`</sub> + `β`<sub>`1`</sub>`x`<sub>`1`</sub> + `β`<sub>`2`</sub>`x`<sub>`2`</sub> + ... + `β`<sub>`n`</sub>`x`<sub>`n`</sub>  
    - `y` : Dependent variable
    - `β`<sub>`0`</sub> : constant / intercept
    - `β`<sub>`1`</sub>, `β`<sub>`2`</sub>, ... : coefficients
    - `x`<sub>`1`</sub>, `x`<sub>`2`</sub>, ... : Independent variables  

  <br>

- Multiple regression의 한계와 주의사항  
  - Multicollinearity : 독립 변수들 간 상관관계가 강할 때, regression model이 불안정할 수 있다.  
  - Overfitting : model에 너무 많은 변수들이 포함되면, model이 데이터를 overfitting할 위험이 있다.  
  overfitting 되면, 모델이 현재 training data에 대해서'만' 잘 작동하고,  
  새로운 데이터에 대한 예측력이 떨어지게 된다.  
  - Selection of Independent variables : 중요한 독립 변수들만  선택하는 것은 매우 중요하다.  
  불필요한 변수를 포함하는 것은 모델을 복잡하게 만들고 예측 성능을 떨어뜨린다.  

  <br>

- Multiple regression in Python  

```py
# data preprocessing
X = df[['alcohol', 'sulphates', 'citric acid', 'volatile acidity', 'dencity', 'chlorides', 'total sulfur dioxide']]
y = df['quality']
```

<br>

```py
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Data partitioning
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Building Multiple Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)
```

<br>

```py
# Model Evaluation
from sklearn.metrics import mean_squared_error, r2_score

# Perform prediction
y_pred = model.predict(X_test)

# Calculate evaluation indicators
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')
```

<br>

```py
# Interpretation of results
coefficients = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])  
print(coefficients)
```

![multiple_regression_result](TODO)  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 장홍준 교수님의 데이터분석프로그래밍 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 