---
layout: post
title: "[데이터 분석 프로그래밍] 14주차 - Clustering"
excerpt: "K-Means, Hierarchical Clustering, DBSCAN"

tags:
  - [데이터 분석 프로그래밍, Python]

toc: true

date: 2024-12-03
last_modified_at: 2024-12-04
---
## Clustering
- Clustering은 unlabeded data나 data points를 서로 다른 cluster로 나누는 작업으로서,  
서로 떨어져 있던 비슷한 data points들을 같은 cluster로 묶는 것이다.  

- 다시 말해, clustering process의 목표는 비슷한 특성을 가진 group들로 분리하여, 이들에게 cluster label을 부여하는 것이다.  

<br>

- Cluster models의 종류
  - Connectivity models
  ![connectivity][def]  
    - e.g., Hierarchical Clustering

  - Centroid models  
  ![centroid][def2]  
    - e.g., K-Means algorithm

  - Density models  
    ![density][def3]  
        - e.g., DBSCAN  

<br>

### 1. K-Means
- K-Means는 가장 간단하면서도 가장 자주 사용되는 clustering algorithm 중 하나이다.  

- 사용자가 정의한 clusters의 개수 `k`로 dataset을 그룹화한다.  

- 단계
  - Step 1 : Initialize Centroids
    - 임의로 `k`개의 centroids를 선택한다.  
    ![centroids][def4]  

  - Step 2 : Assign Data points
    - 각 data points 들에 대하여, cluster 형성을 위해 가장 가까운 centroid를 할당한다.  
    - Euclidean distance를 주로 사용하며, 이를 통해 "nearest" centroid를 찾는다.  
    ![euclidean_distance][def5]  
    ![assign][def6]  

  - Step 3 : Calculate new Centroids  
    - 각각의 cluster에 대하여, 이들 중에서 새로운 centroid를 계산한다.  
    - 새로운 centroid는 cluster 내 모든 data points를 대표하는 data point 이다.  
    - 새로운 centroid는 cluster 내 모든 data points의 평균값으로 계산된다.  
    ![new_centroids][def7]

  - Step 4 : Repeat assignment and Calculate new Centroids
    - 새로운 centroid가 결정되면, 각 data points에 대하여 또 다시 가장 가까운 centroid를 할당한다.  
    ![repeat][def8]

  - Step 5 : Termination
    - 위 Step 3, 4를 반복하되, centroid가 더 이상 변하지 않을 때까지 반복한다.  

    <br>

- K-Means의 장점과 단점  
  - 장점
    - 구현이 상대적으로 간단하다.  
    - 큰 dataset에 대해서도 적용 가능하다.  
    - 새로운 data가 추가되었을 때도 쉽게 adapt할 수 있다.  

- 단점
  - `k`는 수동으로 선택해야 한다.  
  - clustering 결과가 초기 값에 따라 달라질 수 있다.  
  - 다양한 크기와 밀도를 가진 data는 잘 cluster해내지 못한다.  
  ![variance][def9]  
  - outliers에 민감하다.  
  ![outliers][def10]  
  - dimension 수에 민감하다.  

<br>

- K-Means in python

```py
from sklearn.preprocessing import scale
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

iris = load_iris()
irisDF = pd.DataFrame(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
irisDF.head()
```

![iris_head][def11]  

```py
iris.target
```

![iris_target][def12]  

```py
iris.target_names
# array(['setosa', 'versicolor', 'virginica'], dtype='<U10')
```

```py
irisDF.info()
```

![iris_info][def13]  

```py
# K-Means algorithm
kmeans = KMeans(n_clusters=3, max_iter=300, random_state=0)
kmeans.fit(irisDF)

print(kmeans.labels_)
```

![kmeans_labels][def14]  

<br>

#### Evaluation - The Silhouette Coefficient
- clustering의 성능을 평가하는 것은 주어진 dataset에 대한 clustering algorithm이 효과적인 지를 판단하는 데 중요한 단계이다.  

- **Silhouette Coefficient**
  - 각각의 data point가 할당된 cluster 내에 얼마나 가깝게 잘 모여있는 지,  
  그리고 다른 cluster의 data point와는 얼마나 잘 분리되어 있는 지를 측정한다.  
  - 가능한 최댓값은 `1`, 최솟값은 `-1`이다.  
  - `0`에 가까울수록 cluster 간의 중첩(overlap)이 많다는 것을 의미한다.  
  - 음수 값은 일반적으로 주어진 sample이 잘못된 cluster에 할당되었음을 의미한다.  
  즉, 해당 sample은 다른 cluster와 더 유사하다는 것이다.  
  - 계산식  
  ![silhouette][def15]  
  - Example  
  ![silhouette_example][def16]  

<br>

- in python

```py
from sklearn.metrics import silhouette_samples, silhouette_score
silhouette_vals = silhouette_samples(irisDF, kmeans.labels_)
avg_score = np.mean(silhouette_vals)  
print(f'{avg_score:.4f}')
# 0.5819

kmenas4 = KMeans(n_clusters=4, max_iter=300, random_state=0)
kmeans4.fit(irisDF)
avg_score = silhouette_score(irisDF, kmeans4.labels_)
print(avg_score)
# 0.5427

kmeans2 = KMeans(n_clusters=2, max_iter=300, random_state=0)
kmeans2.fit(irisDF)
avg_score = silhouette_score(irisDF, kmeans2.labels_)
print(avg_score)
# 0.6801
```

<br>

#### Evaluation - Davies-Bouldin Index  
- **Davis-Bouldin index (DBI)**  
  - 각 cluster와 가장 가까운 cluster 사이 거리를 측정하여 평균을 낸 것으로,  
  유사도는 cluster 내의 거리와 cluster 간의 거리의 비율로 계산된다.  
  ![dbi][def17]  

- DBI 계산은 아래와 같은 절차로 이루어진다.  
  - 각 cluster의 centroid를 계산한다.  
  - 각 cluster 내에서 평균 거리를 계산한다.
  - 각 cluster centroids 쌍의 거리를 계산한다.  
  - 각 cluster에 대하여 가장 가까운(유사한) cluster를 찾는다.  
  - 모든 cluster에 대한 평균 값을 계산한다.  

<br>

- in python

```py
from sklearn.metrics import davies_bouldin_score
print(f'{davies_bouldin_score(irisDF, kmeans.labels_):.4f}')
# 0.6449

print(f'{davies_bouldin_score(irisDF, kmeans4.labels_):.4f}')
# 0.7027

print(f'{davies_bouldin_score(irisDF, kmeans2.labels_):.4f}')
# 0.3981
```

<br>

#### How do we choose the # of clusters, K ?
- Elbow Method
  - K-Means에서, 한 가지 접근방법은 다양한 `K` 값에 대하여 inertia를 표시하는 것이다.  
  ![elbow][def18]  
    - Inertia : 각 data point와 center 간의 거리의 제곱의 합(Sum of squared distances)
    - return 값이 줄어드는, 즉 "elbow" 위치에 존재하는 `K` 값을 선택하는 것이다. 
    - 주의점 : 크고 복잡한 data는 종종 elbow point를 찾기 어려울 수 있다.  

  <br>

  ### 2. Hierarchical Clustering
  - Hierarchical clustering(HCA)는 clusters들의 계층 구조를 설립하고자 cluster를 분석하는 방법이다.  

  - 두 가지 카테고리가 존재한다.  
    - Agglomerative : "bottom-up" 방식으로서, 자신의 cluster에서 시작하여 게층적으로 올라가며 cluster 쌍이 합쳐진다.  
    - Divisive : "top-down" 방식으로서, 전체 dataset에서 시작하여 내려가면서 cluster를 분할한다.  

- 일반적으로, merge와 split은 greedy 방식으로 결정된다.  
- hierarchical clustering의 결과는 주로 dendrogram으로 제공된다.  
![dendrogram][def19]  

<br>

#### Linkage
- Single Linkage
  - cluster 쌍 내에서 가장 가까운 두 data point 사이의 거리를 측정하는 방식.  
  두 cluster를 합칠 때, 가장 가까운 두 data point 사이의 거리가 가장 짧은 cluster를 선택한다.   
  `D(X, Y) = min d(x, y)`  
  ![single_linkage][def20]  

- Complete Linkage
  - cluster 쌍 내에서 가장 먼 두 data point 사이의 거리를 측정하는 방식.  
  두 cluster를 합칠 때, 가장 먼 두 data point 사이의 거리가 가장 짧은 cluster를 선택한다.  
  `D(X, Y) = max d(x, y)`  
  ![complete_linkage][def21]  

  <br>

- How it works?  
  - (1) 모든 samples에 대한 distance matrix를 계산한다.  

  - (2) 각 data point를 singleton cluster로 대표한다.  
  ![step1_2][def22]  

  - (3) distance를 기반으로 두 개의 가장 가까운 cluster를 합친다.(aggolomerative)  

  - (4) distance matrix를 업데이트한다.  
  ![step3_4][def23]  

  - (5) (3), (4)를 반복한다.  
  ![step_5_1][def24]  
  ![step_5_2][def25]  

<br>

#### Pros and Cons
  - 장점
    - cluster의 개수를 특정해주지 않아도 된다.  
    - 생성된 dendrogram은 데이터를 이해해하는 데에 매우 유용하다.  

  - 단점  
    - 시간 복잡도가 매우 높다. (quadratic)
    - 새로운 data point가 들어왔을 때, 바로 적용하는 것이 불가능하다.  
    - 큰 dataset에 대해서는 dendrogram으로 정확한 cluster 수를 알아보기가 어렵다.  
    ![dendrogram_difficult][def19]  

<br>

- in python

```py
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

iris = load_iris()
X = iris.data
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
```

```py
clt = AgglomerativeClustering(linkage='single')
model = clt.fit(X_std)
```

```py
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import linkage

dendrogram_plot = dendrogram(linkage(X_std, method='single'))
```

![dendrogram_plot][def26]  

<br>

### 3. DBSCAN
- DBSCAN은 density-based clustering algorithm으로 가장 널리 사용된다.  
- Density는 `n`-dimensional 공간의 한 unit에서 data point의 개수로 정의될 수 있다.  
- dimension의 수 `n`은 dataset의 attribute 수를 의미한다.  
![density][def27]  

- DBSCAN은 density를 측정할 때, data point를 중심으로 반지름이 `ε`(epsilon)인 원에서 측정한다.  
  - 이렇게 측정한 것을 center-based density라고 부른다.  

![center_based_density][def28]  

<br>

- How it works?  
  - (1) Epsilon(`ε`)과 MinPoints를 정의한다.  
    - high-density 인지 low-density 인지를 판단하기 위해,  
    data points의 threshold인 MinPoints를 정의해주어야 하고, 이 값보다 높으면 high-density로 판단한다.  

  - (2) Data points를 분류한다.
    - 모든 data points 들은 세 가지 상태로 분류될 수 있다.  
      - Core points : high-density 지역 내의 모든 data points  
      - Border points : MinPoints threshold는 넘지 못했지만, core point의 근처에 있는 data points  
      - Noise points : core point도 아니고, border point도 아닌 data points  
      
    ![classification][def29]  

  - (3) Clustering  
    - core points의 그룹들은 구분된 clusters를 형성한다.  
    - 만약 두 개의 core points가 서로 `ε` 내에 있다면, 두 core points는 같은 cluster에 속한다.  
    - noise points들은 어떠한 cluster로도 분류되지 않는다.  
  
  ![clustering][def30]  

  <br>

- Pros and Cons
  - 장점
    - cluster의 개수를 미리 정해주지 않아도 된다.
    - 임의로 모양이 잡힌 cluster를 찾을 수 있다.  
    - noise의 존재를 감지할 수 있고, outliers에도 면역력이 있다.  

  ![dbscan_vs_k_means][def31]  

  - 단점
    - DBSCAN의 퀄리티는 distance 측정에 의존한다.  
    - density에 큰 차이가 있는 dataset에 대해서는 cluster를 잘 수행하지 못한다.
    - 만약 data와 scale이 제대로 파악되지 않았다면, 의미있는 distance threshold `ε`를 선정하는 것이 어렵다.  

    <br>

- in python

```py
from sklearn_datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN

iris = load_iris()
feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
irisDF = pd.DataFrame(iris.data, columns=feature_names)

dbscan = DBSCAN(eps=0.6, min_samples=8, metric='euclidean')
dbscan_labels = dbscan.fit_predict(irisDF)
```

```py
irisDF['cluster'] = dbscan_labels
plt.figure(figsize=(8, 8))

for i in range(-1, irisDF['cluster'].max()+1):
    plt.scatter(irisDF.loc[irisDF['cluster']==i, 'sepal_length'], irisDF.loc[irisDF['cluster']==i, 'sepal_width'], label='cluster'+str(i))
plt.legend()
plt.title('eps = 0.6, min_samples = 8', size=15)
plt.xlabel('sepal_length', size=12)
plt.ylabel('sepal_width', size=12)
plt.show()
```

![dbscan_plot][def32]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 장홍준 교수님의 데이터분석프로그래밍 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/UxdDCIW.png
[def2]: https://i.imgur.com/9diNU7b.png
[def3]: https://i.imgur.com/BCVvgE1.png
[def4]: https://i.imgur.com/JsNpiQX.png
[def5]: https://i.imgur.com/MNVEr7l.png
[def6]: https://i.imgur.com/f0PHPlN.png
[def7]: https://i.imgur.com/LVwzrZh.png
[def8]: https://i.imgur.com/nETWdPY.png
[def9]: https://i.imgur.com/yErBXe8.png
[def10]: https://i.imgur.com/kcqiOGw.png
[def11]: https://i.imgur.com/Cko6RHx.png
[def12]: https://i.imgur.com/29kJfUi.png
[def13]: https://i.imgur.com/3MdhxZR.png
[def14]: https://i.imgur.com/5Y7fNCA.png
[def15]: https://i.imgur.com/5nsgjZy.png
[def16]: https://i.imgur.com/gRPYk19.png
[def17]: https://i.imgur.com/PVKzV1x.png
[def18]: https://i.imgur.com/yV0Pmgn.png
[def19]: https://i.imgur.com/WYngfcQ.png
[def20]: https://i.imgur.com/RcE7Ipu.png
[def21]: https://i.imgur.com/5xRsHhU.png
[def22]: https://i.imgur.com/rjtdIP8.png
[def23]: https://i.imgur.com/qvaKxsv.png
[def24]: https://i.imgur.com/yBRSdUD.png
[def25]: https://i.imgur.com/AZpRfVF.png
[def26]: https://i.imgur.com/BNnfTue.png
[def27]: https://i.imgur.com/A64DJDO.png
[def28]: https://i.imgur.com/H4LC42X.png
[def29]: https://i.imgur.com/qB4AWMW.png
[def30]: https://i.imgur.com/bl4UjtY.png
[def31]: https://i.imgur.com/vw2C52n.png
[def32]: https://i.imgur.com/Ywyq8pO.png