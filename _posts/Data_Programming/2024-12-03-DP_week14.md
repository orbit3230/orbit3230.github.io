---
layout: post
title: "[데이터 분석 프로그래밍] 14주차 - Clustering"
excerpt: "K-Means, Hierarchical Clustering, DBSCAN"

tags:
  - [데이터 분석 프로그래밍, Python]

toc: true

date: 2024-12-03
last_modified_at: 2024-12-04
---
## Clustering
- Clustering은 unlabeded data나 data points를 서로 다른 cluster로 나누는 작업으로서,  
서로 떨어져 있던 비슷한 data points들을 같은 cluster로 묶는 것이다.  

- 다시 말해, clustering process의 목표는 비슷한 특성을 가진 group들로 분리하여, 이들에게 cluster label을 부여하는 것이다.  

<br>

- Cluster models의 종류
  - Connectivity models
  ![connectivity](TODO)  
    - e.g., Hierarchical Clustering

  - Centroid models  
  ![centroid](TODO)  
    - e.g., K-Means algorithm

  - Density models  
    ![density](TODO)  
        - e.g., DBSCAN  

<br>

### 1. K-Means
- K-Means는 가장 간단하면서도 가장 자주 사용되는 clustering algorithm 중 하나이다.  

- 사용자가 정의한 clusters의 개수 `k`로 dataset을 그룹화한다.  

- 단계
  - Step 1 : Initialize Centroids
    - 임의로 `k`개의 centroids를 선택한다.  
    ![centroids](TODO)  

  - Step 2 : Assign Data points
    - 각 data points 들에 대하여, cluster 형성을 위해 가장 가까운 centroid를 할당한다.  
    - Euclidean distance를 주로 사용하며, 이를 통해 "nearest" centroid를 찾는다.  
    ![euclidean_distance](TODO)  
    ![assign](TODO)  

  - Step 3 : Calculate new Centroids  
    - 각각의 cluster에 대하여, 이들 중에서 새로운 centroid를 계산한다.  
    - 새로운 centroid는 cluster 내 모든 data points를 대표하는 data point 이다.  
    - 새로운 centroid는 cluster 내 모든 data points의 평균값으로 계산된다.  

  - Step 4 : Repeat assignment and Calculate new Centroids
    - 새로운 centroid가 결정되면, 각 data points에 대하여 또 다시 가장 가까운 centroid를 할당한다.  

  - Step 5 : Termination
    - 위 Step 3, 4를 반복하되, centroid가 더 이상 변하지 않을 때까지 반복한다.  

    <br>

- K-Means의 장점과 단점  
  - 장점
    - 구현이 상대적으로 간단하다.  
    - 큰 dataset에 대해서도 적용 가능하다.  
    - 새로운 data가 추가되었을 때도 쉽게 adapt할 수 있다.  

- 단점
  - `k`는 수동으로 선택해야 한다.  
  - clustering 결과가 초기 값에 따라 달라질 수 있다.  
  - 다양한 크기와 밀도를 가진 data는 잘 cluster해내지 못한다.  
  ![variance](TODO)  
  - outliers에 민감하다.  
  ![outliers](TODO)  
  - dimension 수에 민감하다.  

<br>

- K-Means in python

```py
from sklearn.preprocessing import scale
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

iris = load_iris()
irisDF = pd.DataFrame(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
irisDF.head()
```

![iris_head](TODO)  

```py
iris.target
```

![iris_target](TODO)  

```py
iris.target_names
# array(['setosa', 'versicolor', 'virginica'], dtype='<U10')
```

```py
irisDF.info()
```

![iris_info](TODO)  

```py
# K-Means algorithm
kmeans = KMeans(n_clusters=3, max_iter=300, random_state=0)
kmeans.fit(irisDF)

print(kmeans.labels_)
```

![kmeans_labels](TODO)  

<br>

#### Evaluation - The Silhouette Coefficient
- clustering의 성능을 평가하는 것은 주어진 dataset에 대한 clustering algorithm이 효과적인 지를 판단하는 데 중요한 단계이다.  

- **Silhouette Coefficient**
  - 각각의 data point가 할당된 cluster 내에 얼마나 가깝게 잘 모여있는 지,  
  그리고 다른 cluster의 data point와는 얼마나 잘 분리되어 있는 지를 측정한다.  
  - 가능한 최댓값은 `1`, 최솟값은 `-1`이다.  
  - `0`에 가까울수록 cluster 간의 중첩(overlap)이 많다는 것을 의미한다.  
  - 음수 값은 일반적으로 주어진 sample이 잘못된 cluster에 할당되었음을 의미한다.  
  즉, 해당 sample은 다른 cluster와 더 유사하다는 것이다.  
  - 계산식  
  ![silhouette](TODO)  
  - Example  
  ![silhouette_example](TODO)  

<br>

- in python

```py
from sklearn.metrics import silhouette_samples, silhouette_score
silhouette_vals = silhouette_samples(irisDF, kmeans.labels_)
avg_score = np.mean(silhouette_vals)  
print(f'{avg_score:.4f}')
# 0.5819

kmenas4 = KMeans(n_clusters=4, max_iter=300, random_state=0)
kmeans4.fit(irisDF)
avg_score = silhouette_score(irisDF, kmeans4.labels_)
print(avg_score)
# 0.5427

kmeans2 = KMeans(n_clusters=2, max_iter=300, random_state=0)
kmeans2.fit(irisDF)
avg_score = silhouette_score(irisDF, kmeans2.labels_)
print(avg_score)
# 0.6801
```

<br>

#### Evaluation - Davies-Bouldin Index  
- Davis-Bouldin index (DBI)  
  - 각 cluster와 가장 가까운 cluster 사이 거리를 측정하여 평균을 낸 것으로,  
  유사도는 cluster 내의 거리와 cluster 간의 거리의 비율로 계산된다.  
  ![dbi](TODO)  

- DBI 계산은 아래와 같은 절차로 이루어진다.  
  - 각 cluster의 centroid를 계산한다.  
  - 각 cluster 내에서 평균 거리를 계산한다.
  - 각 cluster centroids 쌍의 거리를 계산한다.  
  - 각 cluster에 대하여 가장 가까운(유사한) cluster를 찾는다.  
  - 모든 cluster에 대한 평균 값을 계산한다.  

<br>

- in python

```py
from sklearn.metrics import davies_bouldin_score
print(f'{davies_bouldin_score(irisDF, kmeans.labels_):.4f}')
# 0.6449

print(f'{davies_bouldin_score(irisDF, kmeans4.labels_):.4f}')
# 0.7027

print(f'{davies_bouldin_score(irisDF, kmeans2.labels_):.4f}')
# 0.3981
```

<br>

#### How do we choose the # of clusters, K ?
- Elbow Method
  - K-Means에서, 한 가지 접근방법은 다양한 `K` 값에 대하여 inertia를 표시하는 것이다.  
  ![elbow](TODO)  
    - Inertia : 각 data point와 center 간의 거리의 제곱의 합(Sum of squared distances)
    - return 값이 줄어드는, 즉 "elbow" 위치에 존재하는 `K` 값을 선택하는 것이다. 
    - 주의점 : 크고 복잡한 data는 종종 elbow point를 찾기 어려울 수 있다.  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 장홍준 교수님의 데이터분석프로그래밍 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 