---
layout: post
title: "[데이터 분석 프로그래밍] 15주차 - Neural Network"
excerpt: "Artificial Neuron, Neural Network, Training of Neural Network"

tags:
  - [데이터 분석 프로그래밍, Python]

toc: true

date: 2024-12-10
last_modified_at: 2024-12-11
---
## Neural Network
### 1. Artificial Neuron
- 우리 사람의 뇌는 많은 과학자들에 의해 광범위하게 연구되어 왔다.  
- 그럼에도 불구하고, 뇌의 엄청난 복잡성때문에 가장 기초적인 이해밖에 도달하지 못했다.  
- 심지어는 뇌 속의 뉴런 하나하나의 동작 마저도 너무나도 복잡하다.  
  - 각 뉴런은 수많은 inputs들을 받아서 처리해, outputs을 만들어 낸다.  
  - 이 과정은 전기적 신호 뿐만 아니라, 복잡한 화학적/생물학적 과정을 포함한다.  

<br>

- Distributed and Parallel Processing
  - Distributed Nature
    - 뇌는 정보를 분리하여 처리한다.  
    - 여러 뉴런들이 동시에 작업을 수행하고, 동시에 여러 다른 정보를 다룬다.  
  - Parallel Processing
    - paralled processing은 뇌로 하여금 복잡한 정보를 효율적이고 빠르게 저리하도록 해준다.  

- 각각의 뉴런들은,
  - 하나의 작고 단순한 프로세서이다.
  - 각각은 너무 많은 정보를 담아두지 않는다.  
  - 하지만 종합적으로 모여서 강력한 네트워크를 형성하고, 복잡한 정보 처리도 가능해진다.  

<br>

- Biological Neuron의 핵심 요소
  - Inputs
    - Dendrites(수상)이 다른 뉴런에서 온 신호를 받는다. 
  - Summation of Inputs
    - 수상으로부터 들어온 여러 신호들은 Cell Body(Soma)에서 합쳐진다.  
  - Processing Unit
    - Cell Body는 뉴런의 메인 처리 유닛이다. 여기서 inputs에 기반한 잠재적인 action을 결정한다.
  - Output
    - 결정된 action은 Axon(축삭)을 통해 다른 뉴런이나 근육 세포로 전달된다. 신호는 축삭을 지나 synapse를 통해 다음 뉴런으로 전달된다.  
![neuron](TODO)  

<br>

- Artificial Neuron
  - Artificial Neuron은 Biological Neuron을 모방한 것이다.
  - 각 input `x`<sub>`1`</sub>, `x`<sub>`2`</sub>, ..., `x`<sub>`d`</sub>은 weight `θ`<sub>`1`</sub>, `θ`<sub>`2`</sub>, ..., `θ`<sub>`d`</sub>과 곱해지고 합쳐진다.  

- 합쳐진 input은 activation function `f`를 통해 최종 output `y`를 만들어낸다.  
`y` = `f(θ`<sub>`0`</sub>+`θ`<sub>`1`</sub>`x`<sub>`1`</sub>+`θ`<sub>`2`</sub>`x`<sub>`2`</sub>+...+`θ`<sub>`d`</sub>`x`<sub>`d`</sub>`)`  
  - `θ`<sub>`0`</sub>은 bias라고 불리며, input 값에 상관 없이 output에 영향을 준다.  
![artificial_neuron](TODO)  

<br>

- 가장 간단한 추정(approximation)  
  - 가장 간단한 neural model은, inputs와 각 weights의 곱으로 이루어진 linear combination이다.  
  - 각 combination은 뉴런의 활성화된 단계를 나타낸다.  
  ![simple_neural](TODO)  

  - 이를 시각화하자면,  
  ![simple_neural_visual](TODO)  

  - Artificial neuron은 이러한 summation unit과 activation function의 조합이다.  
  정리하자면,  
    - inputs들이 summation unit으로 weights와 곱해져 linear combination을 이룬다.  
    - 이 결과가 activation function을 통과해 neuron의 전반적인 활성화를 나타낸다.  
  - 이렇게 하나의 뉴런을 구성하는 artificial neural network model을 **Perceptron**이라고 한다.  
  ![perceptron](TODO)  

<br>

- 가장 간단한 activation function은 **Step Function**이다.  
  - 만약 input combination이 음수(`-`)라면 결과는 `0`이고, 뉴런이 비활성화 상태임을 의미한다.  
  - 만약 combination이 양수(`+`)라면 결과는 `1`이고, 뉴런이 활성화 되었음을 의미한다.  
![step_function](TODO)  

<br>

- Perception은 linear classification 문제를 해결하는데 사용되는 learning algorithm이다.  
  - 이 알고리즘은 주어진 데이터를 기반으로 두 개의 클래스로 분리하는 linear boundary를 학습한다.  
  - Perception learning 과정
    - (1) 좌표평면에 임의의 line을 그린다.  
    - (2) 좌표평면에 데이터를 하나씩 입력한다.  
    - (3) 입력값에 따른 예측 결과를 정답과 비교한다. 만약 틀렸다면, line을 다시 그린다.  
    - (4) 2~3을 반복한다. 모든 training data에 대해 잘못된 분류가 없을 때까지.  
  ![perceptron_learning](TODO)  

  - 일단 perception이 training 완료되면 linear boundary 또는 decision boundary가 만들어진다.  
  이들은 데이터를 두 개의 카테고리로 나눈다.  
  ![perceptron_boundary](TODO)  

<br>

- Step function의 단점
  - Step function은 미분이 불가능하다.  
  ![step_function_diff](TODO)  

- 따라서 **Sigmoid Function**을 사용할 수 있다.  
  - Sigmoid function은 0과 1 사이 값으로 매핑시키는 non-linear activation function이다.  
  - Sigmoid function은 부드러운 곡선의 미분 가능한     연속함수이며, x값이 커지면 `1`로 수렴하고 x값이 작아지면 `0`으로 수렴한다.  
  ![sigmoid_function](TODO)  

  <br>

- Logical 연산을 Perceptrons로 수행해보자.  
  - Boolean AND  
  ![and_perceptron](TODO)  

  ```py
  def AND(x1, x2)
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1*w1 + x2*w2
    if tmp >= theta:
      return 1
    elif tmp < theta:
      return 0
  ```

  - Boolean OR
  ![or_perceptron](TODO)  

  ```py
  def OR(x1, x2)
    w1, w2, theta = 0.5, 0.5, 0.3
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
      return 1
    elif tmp > theta:
      return 0
  ```

  - Boolean XOR
    - XOR은 Non-linear Decision Boundary를 필요로 한다. (No linear separability)  
    - 따라서 여러개의 뉴런들로 구현할 수 있다.  
  ![xor_perceptron](TODO)  

  ```py
  import numpy as np
  import tensorflow as tf
  from tensorflow import keras

  X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
  y = np.array([[0], [1], [1], [0]])

  model = keras.Sequential()
  model.add(keras.layers.Dense(2, input_dim=2, activation='sigmoid'))
  model.add(keras.layers.Dense(1, activation='sigmoid'))

  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
  model.fit(X, y, epochs=5000, verbose=0)
  ```

  ```py
  _, accuracy = model.evaluate(X, y)
  print(f'Accuracy: {accuracy*100:.2f}%')
  ```

  ![xor_perceptron_result](TODO)  

  ```py
  predictions = model.predict(X)
  predictions = np.round(predictions).astype(int)

  print('Predictions:')
  for i in range(len(X)) :
    print(f'input: {X[i]} => Predicted Output: {predictions[i]}, Actual Output: {y[i]}')
  ```

  ![xor_perceptron_predictions](TODO)  


<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 장홍준 교수님의 데이터분석프로그래밍 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 