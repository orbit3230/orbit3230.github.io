---
layout: post
title: "[데이터 분석 프로그래밍] 15주차 - Neural Network"
excerpt: "Artificial Neuron, Neural Network, Training of Neural Network"

tags:
  - [데이터 분석 프로그래밍, Python]

toc: true

date: 2024-12-10
last_modified_at: 2024-12-11
---
## Neural Network
### 1. Artificial Neuron
- 우리 사람의 뇌는 많은 과학자들에 의해 광범위하게 연구되어 왔다.  
- 그럼에도 불구하고, 뇌의 엄청난 복잡성때문에 가장 기초적인 이해밖에 도달하지 못했다.  
- 심지어는 뇌 속의 뉴런 하나하나의 동작 마저도 너무나도 복잡하다.  
  - 각 뉴런은 수많은 inputs들을 받아서 처리해, outputs을 만들어 낸다.  
  - 이 과정은 전기적 신호 뿐만 아니라, 복잡한 화학적/생물학적 과정을 포함한다.  

<br>

- Distributed and Parallel Processing
  - Distributed Nature
    - 뇌는 정보를 분리하여 처리한다.  
    - 여러 뉴런들이 동시에 작업을 수행하고, 동시에 여러 다른 정보를 다룬다.  
  - Parallel Processing
    - paralled processing은 뇌로 하여금 복잡한 정보를 효율적이고 빠르게 저리하도록 해준다.  

- 각각의 뉴런들은,
  - 하나의 작고 단순한 프로세서이다.
  - 각각은 너무 많은 정보를 담아두지 않는다.  
  - 하지만 종합적으로 모여서 강력한 네트워크를 형성하고, 복잡한 정보 처리도 가능해진다.  

<br>

- Biological Neuron의 핵심 요소
  - Inputs
    - Dendrites(수상)이 다른 뉴런에서 온 신호를 받는다. 
  - Summation of Inputs
    - 수상으로부터 들어온 여러 신호들은 Cell Body(Soma)에서 합쳐진다.  
  - Processing Unit
    - Cell Body는 뉴런의 메인 처리 유닛이다. 여기서 inputs에 기반한 잠재적인 action을 결정한다.
  - Output
    - 결정된 action은 Axon(축삭)을 통해 다른 뉴런이나 근육 세포로 전달된다. 신호는 축삭을 지나 synapse를 통해 다음 뉴런으로 전달된다.  
![neuron][def]  

<br>

- Artificial Neuron
  - Artificial Neuron은 Biological Neuron을 모방한 것이다.
  - 각 input `x`<sub>`1`</sub>, `x`<sub>`2`</sub>, ..., `x`<sub>`d`</sub>은 weight `θ`<sub>`1`</sub>, `θ`<sub>`2`</sub>, ..., `θ`<sub>`d`</sub>과 곱해지고 합쳐진다.  

- 합쳐진 input은 activation function `f`를 통해 최종 output `y`를 만들어낸다.  
`y` = `f(θ`<sub>`0`</sub>+`θ`<sub>`1`</sub>`x`<sub>`1`</sub>+`θ`<sub>`2`</sub>`x`<sub>`2`</sub>+...+`θ`<sub>`d`</sub>`x`<sub>`d`</sub>`)`  
  - `θ`<sub>`0`</sub>은 bias라고 불리며, input 값에 상관 없이 output에 영향을 준다.  
![artificial_neuron][def2]  

<br>

- 가장 간단한 추정(approximation)  
  - 가장 간단한 neural model은, inputs와 각 weights의 곱으로 이루어진 linear combination이다.  
  - 각 combination은 뉴런의 활성화된 단계를 나타낸다.  
  ![simple_neural][def3]  

  - 이를 시각화하자면,  
  ![simple_neural_visual][def4]  

  - Artificial neuron은 이러한 summation unit과 activation function의 조합이다.  
  정리하자면,  
    - inputs들이 summation unit으로 weights와 곱해져 linear combination을 이룬다.  
    - 이 결과가 activation function을 통과해 neuron의 전반적인 활성화를 나타낸다.  
  - 이렇게 하나의 뉴런을 구성하는 artificial neural network model을 **Perceptron**이라고 한다.  
  ![perceptron][def6]  

<br>

- 가장 간단한 activation function은 **Step Function**이다.  
  - 만약 input combination이 음수(`-`)라면 결과는 `0`이고, 뉴런이 비활성화 상태임을 의미한다.  
  - 만약 combination이 양수(`+`)라면 결과는 `1`이고, 뉴런이 활성화 되었음을 의미한다.  
![step_function][def5]  

<br>

- Perception은 linear classification 문제를 해결하는데 사용되는 learning algorithm이다.  
  - 이 알고리즘은 주어진 데이터를 기반으로 두 개의 클래스로 분리하는 linear boundary를 학습한다.  
  - Perception learning 과정
    - (1) 좌표평면에 임의의 line을 그린다.  
    - (2) 좌표평면에 데이터를 하나씩 입력한다.  
    - (3) 입력값에 따른 예측 결과를 정답과 비교한다. 만약 틀렸다면, line을 다시 그린다.  
    - (4) 2~3을 반복한다. 모든 training data에 대해 잘못된 분류가 없을 때까지.  
  ![perceptron_learning][def7]  

  - 일단 perception이 training 완료되면 linear boundary 또는 decision boundary가 만들어진다.  
  이들은 데이터를 두 개의 카테고리로 나눈다.  
  ![perceptron_boundary][def8]  

<br>

- Step function의 단점
  - Step function은 미분이 불가능하다.  
  ![step_function_diff][def9]  

- 따라서 **Sigmoid Function**을 사용할 수 있다.  
  - Sigmoid function은 0과 1 사이 값으로 매핑시키는 non-linear activation function이다.  
  - Sigmoid function은 부드러운 곡선의 미분 가능한 연속함수이며, x값이 커지면 `1`로 수렴하고 x값이 작아지면 `0`으로 수렴한다.  
  ![sigmoid_function][def10]  

  <br>

- Logical 연산을 Perceptrons로 수행해보자.  
  - Boolean AND  
  ![and_perceptron][def11]  

  ```py
  def AND(x1, x2)
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1*w1 + x2*w2
    if tmp >= theta:
      return 1
    elif tmp < theta:
      return 0
  ```

  - Boolean OR
  ![or_perceptron][def12]  

  ```py
  def OR(x1, x2)
    w1, w2, theta = 0.5, 0.5, 0.3
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
      return 1
    elif tmp > theta:
      return 0
  ```

  - Boolean XOR
    - XOR은 Non-linear Decision Boundary를 필요로 한다. (No linear separability)  
    - 따라서 여러개의 뉴런들로 구현할 수 있다.  
    
  ![xor_perceptron][def13]  

  ```py
  import numpy as np
  import tensorflow as tf
  from tensorflow import keras

  X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
  y = np.array([[0], [1], [1], [0]])

  model = keras.Sequential()
  model.add(keras.layers.Dense(2, input_dim=2, activation='sigmoid'))
  model.add(keras.layers.Dense(1, activation='sigmoid'))

  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
  model.fit(X, y, epochs=5000, verbose=0)
  ```

  ```py
  _, accuracy = model.evaluate(X, y)
  print(f'Accuracy: {accuracy*100:.2f}%')
  ```

  ![xor_perceptron_result][def14]  

  ```py
  predictions = model.predict(X)
  predictions = np.round(predictions).astype(int)

  print('Predictions:')
  for i in range(len(X)) :
    print(f'input: {X[i]} => Predicted Output: {predictions[i]}, Actual Output: {y[i]}')
  ```

  ![xor_perceptron_predictions][def15]  

  <br>

### 2. Neural Network
- Neural Network는 인간 뇌의 구조와 기능에서 영감을 받은 인공지능 모델이다.  
  - 사람의 뇌는 수십억 개의 뉴런들로 구성되어 있으며, 정보 처리를 위해 상호 연결되어 있다.  
  - Neural Network는 이러한 뇌의 구조를 수학적으로 계산하는 형태로 모방시킴으로써,  
  컴퓨터로 하여금 데이터를 처리하고 학습하도록 만든 것이다.  

- Neural Network는 주로 세 가지 요소들로 구성된다.  
![neural_network][def16]  
  - **Input layer**
    - 외부 데이터를 받는 역할
    - 각 input node(뉴런)는 input data의 feature를 나타낸다.  
    - e.g., 이미지 인식을 할 때, input node는 이미지의 각 픽셀을 나타낸다.  

  - **Hidden layers**
    - Input data를 처리하고 중요한 features를 추출하는 역할
    - neural network의 depth는 바로 이 hidden layers의 수에 의해 결정된다.  
    - network가 깊어질 수록, 더 복잡한 문제를 해결할 수 있다.  

  - **Output layer**
    - 최종 결과를 만들어내는 역할
    - output nodes의 수는 해결할 문제의 종류에 따라 달라진다.  
    - e.g., 이진 분류 문제의 경우 output node는 1개이고, 다중 분류 문제의 경우 output node는 여러 개이다.

    <br>

- **2-Layer perception**
  - 2개의 inputs와 1개의 output
  - 각 뉴런의 계산 과정은 logistic regression과 비슷하다.  
![2_layer_perception][def17]  

- 모델 간 비교  
  - Linear Regression  
  ![linear_regression][def18]  

  - Logistic Regression  
  ![logistic_regression][def19]  

  - Neural Network(2-Layer)  
  ![2_layer_perception][def20]  

  <br>

- **Activation Function**
  - Activation function은 neural network의 매우 중요한 요소이다.  
  - 각 뉴런의 output을 결정하고, 네트워크에 non-linearity를 추가한다.  
  - 종류  
  ![activation_function][def21]  

<br>

- in python

```py
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("X_train shape:", X_train.shape, "X_test shape:", X_test.shape)
# X_train shape: (120, 4) X_test shape: (30, 4)

mlp = MLPClassifier(hidden_layer_sizes=(2,), max_iter=10000, random_state=42)
mlp.fit(X_train, y_train)

y_pred = mlp.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"y_pred: {y_pred}")
print(f"y_test: {y_test}")
print(f"Accuracy: {accuracy}")
```

![neural_network_result][def22]  

```py
mlp = MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=10000, random_state=42)
mlp.fit(X_train, y_train)

y_pred = mlp.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"y_pred: {y_pred}")
print(f"y_test: {y_test}")
print(f"Accuracy: {accuracy}")
```

![neural_network_result2][def23]  

<br>

- 이처럼 Neural Network가 Deep할 수록 정확도가 상승하는 것을 확인할 수 있다.  

- Simple Neural Networks의 한계
  - 간단한 패턴 인식이나 간단한 분류 문제를 해결하는 데에는 충분하다.
  - 하지만 복잡하고, non-linear한 패턴이나 고차원 데이터(e.g., video, text)에서는 성능이 감소한다.  

- Deep Neural Networks의 필요성
  - Deep Neural Networks(DNNs)는 여러 개의 hidden layers를 거치며 다양한 추상 데이터도 학습할 수 있다.
  - DNNs는 이미지 인식, 자연어 처리, 음성 인식 등의 복잡한 업무에서 매우 뛰어나다.  

  <br>

### 3. Training of Neural Network
- TensorFlow (Google Brain)
  - 특징
    - 큰 데이터 처리와 분류 학습을 지원
    - 다양한 플랫폼에서 사용 가능

  - 핵심 기능
    - Tensor manipulation, model design, training, deployment
    - CPU와 GPU 모두 지원

- PyTorch (Facebook AI Research)
  - 특징
    - Research와 Prototyping에 이상적
    - Python 기반 인터페이스로써, 사용하기 직관적

  - 핵심 기능
    - 딥러닝 모델 디자인, 학습, 추론
    - 풍부한 커뮤니티와 충분한 튜토리얼  

- Keras
  - 특징
    - 고수준 neural networks API, Backend로 딥러닝 도구 지원
    - 코드가 간결하고 직관적. 빠른 Prototyping에 이상적

  - 핵심 기능
    - 간단한 API로 복잡한 모델 구현도 Easy
    - 다양한 pre-trained 모델 제공  

    <br>

- Fashion MNIST Dataset을 가지고 구현해보자.  
  - 10개의 class로 구성된 70,000개의 grayscale images  
  (60000개의 training set, 10000개의 test set)  
  - 각 sample은 28x28 pixel로 이루어져 있다.
  - pixel 값은 0~255 사이의 정수로 표현된다.  
  ![fashion_mnist][def24]  

```py
import tensorflow as tf
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
print(train_input.shape, train_target.shape)
# (60000, 28, 28) (60000,)
print(test_input.shape, test_target.shape)
# (10000, 28, 28) (10000,)
```

```py
import matplotlib.pyplot as plt
import numpy as np
# Function to draw an image
def plot_images(images, labels, num_images=5) :
  plt.figure(figsize=(10, 10))
  for i in range(num_images) :
    plt.subplot(1, num_images, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(images[i], cmap=plt.cm.binary)
    plt.xlabel(labels[i])
  plt.show()

# Draw the first 5 images
plot_images(train_input, train_target)
```

![fashion_mnist_images][def25]  

```py
# Reshape 28x28 -> 1x784
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)
test_scaled = test_input / 255.0
test_scaled = test_scaled.reshape(-1, 28*28)

print(train_scaled.shape)
# (60000, 784)
print(test_scaled.shape)
# (10000, 784)
```

```py
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
model = keras.Sequential([dense])

model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(train_scaled, train_target, epochs=20)
```

![fashion_mnist_result][def26]  

```py
y_pred_probs = model.predict(test_scaled)
y_pred_probs[:4]
```

![fashion_mnist_result2][def27]  

```py
# Convert predicted probabilities to class labels
y_pred = np.argmax(y_pred_probs, axis=1)
y_pred[:4]
# array([9, 2, 1, 1])
```

```py
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(test_target, y_pred)
print(f'Test Accuracy: {accuracy}')
# Test Accuracy: 0.8395
```

<br>

- Validation Data는 test data를 따라 모델의 성능을 평가하는데 사용되는 중요한 데이터이다.  
  - 다만, 이 두 데이터 간에는 차이가 있다.  

- Test data는 학습이 끝난 후에 모델의 최종 성능 평가에 사용된다.  
- Validation data는 학습이 진행되는 동안, 적절히 진행되고 있는 지를 평가하는 데 사용된다.  
![validation_data][def28]  

<br>

- Epoch and Batch
  - **Epoch**
    - 전체 training data set을 한 번 학습하는 것
    - 다시말해, one Epoch는 전체 training data set을 한 번 통과하는 것이다.  
    - Epoch가 너무 작으면 충분히 학습하지 못하고 underfitting이 발생할 수 있다.
    - Epoch가 너무 크면 overfitting이 발생할 수 있다.  

  - **Batch**
    - 한 번에 처리하는 데이터의 양
    - 전체 dataset을 한 번에 처리하는 대신, 데이터는 작은 부분들로 나누어지는데, 이를 **batch**라고 한다.  
    - Batch size는 한 번에 들어가는 sample의 수를 의미한다.
    - Batch size가 작으면 학습 속도가 느려지지만, 더 정확한 결과를 얻을 수 있다.
    - Batch size가 크면 학습 속도가 빨라지지만, 더 부정확한 결과를 얻을 수 있다.  

    <br>

- Epoch와 Batch는 학습 과정에서 함께 작용한다.  
  - Epoch 동안 dataset은 batches로 나누어지고, 각 batch가 하나씩 처리된다.  

- Example of Epoch and Batch
  - Dataset size : 1000 samples
  - Batch size : 100
  - `#` of Epoch : 5  

- Training Flow
  - Epoch 1 : 각 Batch마다 100개의 sample씩 처리. 총 10개의 Batch가 하나의 Epoch을 이룬다.
  - Epoch 2-5 : 같은 방식을 5번 반복, 총 50개의 Batch가 처리된다.  
![epoch_batch][def29]  

<br>

- Loss and Accuracy
  - Loss : 모델의 예측이 실제 값과 얼마나 멀리 떨어져 있는 지를 측정한 것
  - Accuracy : 모델에 의해 만들어진 예측의 정확도

- Loss Curve, Accuracy Curve  
  - 두 curve 모두 머신 러닝 모델의 시간 별 성능을 추적하는데 도움을 준다.  
  - 이들은 모델이 제대로 학습하고 있는 지, underfitting 인 지, overfitting 인 지에 대한 직관을 제공해준다.  
![loss_accuracy_curve][def30]  

<br>

- Deep Neural Network에서 overfitting을 방지하는 기법으로서, 두 가지가 주로 사용된다.  
  - (1) Dropout
    - 학습 과정에서 무작위로 선택된 뉴런을 제거/비활성화 시키는 정규화 기법
    - Network가 특정 뉴런에만 의존하지 않도록 하여, 일반화를 도와준다.
    - Dropout rate (주로 `0.2` ~ `0.5`)  

  ![dropout][def31]  

  - (2) Early Stopping
    - 모델의 성능이 validation set에서 향상을 멈추었을 때 학습을 중지하는 기법으로
    - 작동 방식
      - 학습하는 동안, 각 epoch마다 validation set에서 모델의 성능을 평가
      - 만약 validation loss가 상승하기 시작했다면, 모델은 아마도 overfitting 되었을 것.
      - 따라서 validation loss가 몇 번의 epochs 동안 평평해지거나 상승하면 학습을 중단  
    
  ![early_stopping][def32]  

  <br>

- in python

```py
from sklearn.model_selection import train_test_split
train_scaled, val_scaled, train_target, val_target = train_test_split(
  train_scaled, train_target, test_size=0.2, random_state=42)
print(train_scaled.shape)
# (48000, 784)
print(val_scaled.shape)
# (12000, 784)
```

```py
model = keras.Sequential()
model.add(keras.layers.Input(shape=(784,), name='input'))
model.add(keras.layers.Dense(100, activation='relu', name='hidden'))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(10, activation='softmax', name='output'))
model.summary()
```

![dropout_model][def33]  

```py
model.compile(optimizer='RMSprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.keras', save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)

history = model.fit(train_scaled, train_target, epochs=20, verbose=2, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb])
```

![dropout_result][def34]  

```py
# Find the best validation loss value and validation accuracy at the corresponding epoch
best_epoch = np.argmin(history.history['val_loss'])
best_val_loss = history.history['val_loss'][best_epoch]
best_val_accuracy = history.history['val_accuracy'][best_epoch]

print(f'Best val_loss: {best_val_loss:.4} at epoch {best_epoch+1}')
# Best val_loss: 0.3787 at epoch 7
print(f'Best val_accuracy: {best_val_accuracy:.4}')
# Best val_accuracy: 0.8732
```

```py
import matplotlib.pyplot as plt
plt.plot(range(1, len(history.history['loss'])+1), history.history['loss'])
plt.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.xticks(range(1, len(history.history['loss'])+1))
plt.legend(['train', 'val'])
plt.show()
```

![dropout_loss_curve][def35]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 장홍준 교수님의 데이터분석프로그래밍 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/YjmsrqL.png
[def2]: https://i.imgur.com/zFuyMEx.png
[def3]: https://i.imgur.com/v1UapYN.png
[def4]: https://i.imgur.com/1Rtgsni.png
[def5]: https://i.imgur.com/5kfwByl.png
[def6]: https://i.imgur.com/9KjtkEe.png
[def7]: https://i.imgur.com/HiP6tYi.png
[def8]: https://i.imgur.com/MqBggWq.png
[def9]: https://i.imgur.com/EYLQb5A.png
[def10]: https://i.imgur.com/KizBxD3.png
[def11]: https://i.imgur.com/0ItfrI9.png
[def12]: https://i.imgur.com/hWUve0C.png
[def13]: https://i.imgur.com/5jYZNxD.png
[def14]: https://i.imgur.com/aHIEmbm.png
[def15]: https://i.imgur.com/vzq1nLN.png
[def16]: https://i.imgur.com/3t7Xoel.png
[def17]: https://i.imgur.com/fc4a0E4.png
[def18]: https://i.imgur.com/pxT7meZ.png
[def19]: https://i.imgur.com/vJG6E2D.png
[def20]: https://i.imgur.com/63OPLvN.png
[def21]: https://i.imgur.com/x73JVxw.png
[def22]: https://i.imgur.com/SoPnsbh.png
[def23]: https://i.imgur.com/hG4kTrt.png
[def24]: https://i.imgur.com/vSKHjdR.png
[def25]: https://i.imgur.com/zzGENHZ.png
[def26]: https://i.imgur.com/I7QTU9J.png
[def27]: https://i.imgur.com/prApU8k.png
[def28]: https://i.imgur.com/LVRZQGr.png
[def29]: https://i.imgur.com/goj8sgK.png
[def30]: https://i.imgur.com/6BuCZHX.png
[def31]: https://i.imgur.com/3umdVfv.png
[def32]: https://i.imgur.com/nEXHAXv.png
[def33]: https://i.imgur.com/MHpf8lu.png
[def34]: https://i.imgur.com/ZYoZxdw.png
[def35]: https://i.imgur.com/y0t9Pxb.png