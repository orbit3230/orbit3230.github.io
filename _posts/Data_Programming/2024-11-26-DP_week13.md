---
layout: post
title: "[데이터 분석 프로그래밍] 13주차 - Classification (2)"
excerpt: "Decision Tree, Ensemble Model, Random Forest"

tags:
  - [데이터 분석 프로그래밍, Python]

toc: true

date: 2024-11-26
last_modified_at: 2024-11-27
---
## Classification
### 1. Decision Tree
- Decision Tree는 결정을 도와주는 예측 분석 기법이다.  

- Tree graph의 노드들은 event나 choice를 나타내고, edge들은 decision rules나 conditions을 나타낸다.  

![decision_tree][def]  

<br>

- 지난번 dataset을 이용하여 Decision Tree를 만들어보자.  
![tennis_dataset][def2]  

<br>

- Internal nodes : 특정 feature `x`<sub>`i`</sub>의 값을 test 했을 때, test의 결과에 따라 branch 된다.  

- Leaf nodes : class `h(x)`를 나타낸다.  

![decision_tree_example][def3]  
  - feature들이 `Outlook(x`<sub>`1`</sub>`)`, `Temperature(x`<sub>`2`</sub>`)`, `Humidity(x`<sub>`3`</sub>`)`, `Wind(x`<sub>`4`</sub>`)`라고 하자.  
  - 그러면 feature vector `x` = `{Sunny, Hot, High, Strong}`은 `No`로 분류된다.  
  - `Temperature` feature는 관계가 없다.  

<br>

- Decision Algorithm의 핵심은 데이터에 기반한 decision tree를 만드는 것이다.  
  - 가장 중요한 정보는 어떻게 decision tree의 노드를 구조화 할 것인가이다.  

- Internal node는 `if`-`else`문 내 조건에 대응되고, 이 조건을 "splitting attribute" 라고 부른다.  
  - 바로 이 splitting attribute를 선택하는 것이 중요하다.  
![splitting_attribute][def4]  

<br>

#### Selecting the Best Splitting Attribute
- 어느 attribute가 더 훌륭한 classifier인가?  

- 전체 데이터를 특정한 attribute를 기반으로 나눈다.  
![splitting_attribute_example][def5] 
  - 만약 왼쪽처럼 각 class가 서로 다른 노드들로 나뉘어졌다면, 단 하나의 condition judgement로 class를 구분할 수 있다.  
  - 반면 오른쪽처럼 서로 다른 노드들에 섞여있다면, 재분류가 필요해질 것이다.  
 
<br>

#### Entropy
- Entropy는 불확실성을 나타내는 척도이다.  
  - Entropy는 아래와 같이 계산된다.  
  ![entropy][def6]  
  - Examples  
  ![entropy_example][def7]  

<br>

#### Information Gain
- Information gain은 데이터가 특정 attribute로 나뉘어 졌을 때, 얻는 이익의 양을 측정하는 수단이다.  
  - 분할 시 dataset의 purity가 얼마나 증가하는 지를 보여주고, 어느 attribute가 가장 유용한 지를 결정하는 데 사용된다.  

- Information gain은 아래와 같이 계산된다.  
![information_gain][def8]  

<br>

#### Determine the Root Attribute
- Root attribute는 가장 큰 information gain을 가진 attribute로 선택하는 것이 좋다.    

- 각 attribute `humidity`, `outlook`, `wind`, `temperature`의 information gain을 계산해보자.  
![root_attribute_1][def9]  
![root_attribute_2][def10]  

- `outlook`이 가장 큰 information gain을 가지므로, Root attribute로 채택되었다.  

<br>

#### Gini Index
- CART는 decision tress에 기반한 machine learning 알고리즘이다.  
   - Gini index는 경제학에서 수입의 불평등을 측정하는데 쓰이는 지수이다.  
   - CART 알고리즘은 각 attribute의 불순도를 측정하는 데에 Gini index를 사용한다.  

- Gini index는 아래와 같이 계산된다.  
![gini_index][def11]  
- Examples  
![gini_index_example][def12]  

<br>

#### Decision Tree in Python
- 아래와 같은 Heart Disease dataset을 이용하여 Decision Tree를 만들어보자.  
![heart_disease_dataset][def13]  

<br>

- Data preparation and exploration

```py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score
from sklearn import tree
import matplotlib.pyplot as plt

df.head()
df.info()
```

![heart_disease_dataset][def14]  
![heart_disease_info][def15]  

```py
# Categorical variables
categorical_features = ['sex', 'cp', 'fbs', 'restecg']
X = df[categorical_features]
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create Decision tree model
clf = DecisionTreeClassifier(random, state=42)
# Model training
clf.fit(X_train, y_train)
```

```py
# Prediction
y_pred = clf.predict(X_test)
y_pred
```

![heart_disease_prediction][def16]  

```py
# Evaluation
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.3f}")
```

![heart_disease_evaluation][def17]  

```py
# Visualization
plt.figure(figsize=(40, 20))
tree.plot_tree(clf, feature_names=X.columns, class_names=['No Disease', 'Disease'], filled=True)
plt.show()
```

![heart_disease_tree][def18]  

<br>

### 2. Ensemble Model
- Ensemble Learning은, machine learning에서 여러개의 learning algorithms 들을 조합하여 더 정확한 예측을 하는 방법이다.  

- 이 방식은 single model의 한계를 극복하고, 여러 모델로부터 예측을 결합하여 성능을 향상하는 데 초점을 둔다.  

![ensemble_model][def19]  

<br>

#### Voting
- Voting은 하나의 강력한 prediction을 만들기 위해 여러가지 다른 machine learningn model들을 합치는 방식이다.  

- 이 접근법은 주로 classificatio 문제에서 사용된다. 각각의 model에 의해 만들어진 prediction에 기반하여 최종 결론을 내린다.  

![voting](TODO)  

<br>

- **Hard voting** : 각 model의 predictions들이 모여서 가장 자주 선택된 class를 선택한다.  
![hard_voting](TODO)  

- **Soft voting** : 각 model의 prediction 확률의 평균을 내어, 가장 높은 평균 확률을 가진 class를 선택한다.  
![soft_voting](TODO)  

<br>

- Voting in python

```py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.emsemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score

df.head()
```

![heart_disease_dataset][def14]

```py
# Categorical variables
categorical_features = ['sex', 'cp', 'fbs', 'restecg']
X = df[categorical_features]
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

log_clf = LogisticRegression(random_state=42, max_iter=1000)
knn_clf = KNeighborsClassifier(n_neighbors=5)
dt_clf = DecisionTreeClassifier(random_state=42)

voting_cfl = VotingClassifier(estimators=[('lr', log_clf), ('knn', knn_clf), ('dt', dt_clf)], voting='hard')
# Ensemble model training
voting_cfl.fit(X_train, y_train)
```

```py
# Performance comparison
for clf in (log_clf, knn_clf, dt_clf, voting_cfl):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, f"Accuracy: {accuracy_score(y_test, y_pred):.3f}", f"F1 Score: {f1_score(y_test, y_pred):.3f}")  
```

![voting_performance](TODO)  

<br>

- 새로운 feature 투입

```py
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

log_clf = LogisticRegression(random_state=42, max_iter=1000)
knn_clf = KNeighborsClassifier(n_neighbors=5)
dt_clf = DecisionTreeClassifier(random_state=42)

voting_cfl = VotingClassifier(estimators=[('lr', log_clf), ('knn', knn_clf), ('dt', dt_clf)], voting='hard')
# Ensemble model training
voting_cfl.fit(X_train, y_train)

# Performance comparison
for clf in (log_clf, knn_clf, dt_clf, voting_cfl):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, f"Accuracy: {accuracy_score(y_test, y_pred):.3f}", f"F1 Score: {f1_score(y_test, y_pred):.3f}")
```

![voting_performance_2](TODO)  

<br>

### 3. Random Forest
#### Bagging
- Bagging은 여러 개의 classifiers를 만들어 voting을 통해 최종 결정을 내리는 알고리즘이다.  
  - bagging에서 서로 다른 datasets 들은 원본 dataset 으로부터 임의로 추출한 sample들이다. 이들을 가지고 model을 training한다.  
  - bagging의 대표적인 알고리즘이 **Random Forest**이다.  
  - 여러 decisiont tree classifier들이 데이터의 일부를 추출하여 bootstrapping을 사용해 학습한다.  
  - classifiers들의 예측 결과에 기반하여 최종 class value가 voting을 통해 결정된다.  

![bagging](TODO)  

<br>

#### Bootstrapping
- Bootstrapping은 원본 dataset에서 새로운 data samples들을 만드는 방법이다.  

- Random Sampling with Replacement
  - 데이터를 추출할 때 할 때 중복을 허용한다.  
  - 같은 데이터가 여러번 추출될 수 있다.  

- Sample Size : boootstrap sample은 원본 dataset과 크기가 같다.  

<br>

- 중복을 허용하여 추출하기 때문에, 전혀 선택되지 못한 데이터도 존재할 수 있다.  
-> 이렇게 선택되지 못한 데이터를 **Out-of-Bag(OOB)** sample이라고 한다.  
  - OOB samples 들은 주로 ensemble learning에서 model을 평가하는 데 쓰인다.  

- OOB는 더 효율적인 평가와 random forest models 성능 향상에 유용하다.  

<br>

#### Hyperparameters of Random Forest
- Random Forest 알고리즘의 컨트롤과 최적화에 있어 중요한 역할을 하는 다양한 **hyperparameters**가 존재한다.  

- (1) Number of Trees
  - Random forest model에 사용되는 decision trees의 수.
  - 많은 수의 trees는 일반적으로 model의 성능을 향상하지만, 계산 시간과 메모리 사용량이 증가한다.  
  - 추가적으로, 일정 수준 이상의 trees는 성능 향상에 미미한 영향을 줄 수도 있다.  
![number_of_trees](TODO)  

- (2) Max Depth
  - 각 decision tree의 최대 깊이.
  - 깊이가 깊을수록 model이 데이터를 더 상세하게 학습하지만, overfitting의 위험도 커진다.  
  - 반면 너무 깊이가 얕다면, underfitting의 위험이 커진다.  

- (3) OOB Score
  - OOB sample을 사용하여 model의 성능을 평가하는 파라미터.
  - separate validation set 없이 model의 성능을 평가할 수 있다.  
  - separate verification data의 필요를 없앰으로서 데이터 사용의 효율성을 높인다.  

<br>

- **Parameter** : model이 데이터로 부터 학습하는 값.
  - linear regression을 예로 들자면, slope와 intercept가 이에 해당한다.  

- **Hyperparameter** : model의 학습 이전에 반드시 설정되어야 하는 값.
  - 학습 과정과 model의 구조를 제어한다.  
  - Random Forest에서 decision tree의 max depth, number of trees 등이 이에 해당한다.  
  
- Hyperparameters는 model의 성능에 매우 큰 영향을 미친다.  
  - 적절한 값을 찾는 것이 매우 중요하다.  

- **Grid Search**와 **Random Search**가 이에 사용된다.  
  - (1) **Grid Search** : 모든 가능한 hyperparameter 조합을 시도하여 최적의 조합을 찾는다.
    - 방법
      - 1. Hyperparameter 범위 지정 : 각 hyperparameter에 대한 가능한 값의 범위를 정의한다.  
      - 2. 모든 조합 탐색 : 범위 내 모든 가능한 조합을 생성한다.  
      - 3. model을 학습하고 평가 : 모든 조합에 대해 평가해보고, evaluation metric에 기반해 최적의 조합을 선택한다.  

    - Necessity & Advantages
      - 모든 가능한 조합을 탐색하기 때문에, 직관적이며 이해가 쉽고 최적의 조합을 찾을 수 있다.  

    - Disadvantages
      - 시간 & 자원 소모 : 조합의 규모가 커지면 시간이 매우 오래 걸리고 계산기 expensive해진다.  

  - (2) **Random Search** : hyperparameter 조합을 무작위로 선택하여 최적의 조합을 찾는다.
    - 방법
      - 1. Hyperparameter 범위 지정 : 각 hyperparameter에 대한 가능한 값의 범위를 정의한다.  
      - 2. 무작위로 조합 선택 : 무작위로 조합을 선택한다.  
      - 3. model을 학습하고 평가 : 선택된 조합에 대해 평가해보고, evaluation metric에 기반해 그 중 최적의 조합을 선택한다.  

    - Necessity & Advantages
      - 효율성 : grid search와 비교하여 훨씬 덜 expensive하다. 특히 space가 클 때 유용하다.  
      - 탐색의 유연성 : 무작위로 선택하기 때문에, 다양한 조합을 탐색할 수 있다.  
    
    - Disadvantages
      - 최적의 조합이라는 보장이 없다.  

<br>

- Random Forest in Python

```py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

df.head()
```

![heart_disease_dataset][def14]

```py
# Separate features and labels
categorical_features = ['sex', 'cp', 'fbs', 'restecg']
X = df[categorical_features]
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  

rf_clf = RandomForestClassifier(n_estimators=5, max_depth=20, random_state=42)
# Model training
rf_clf.fit(X_train, y_train)

# Prediction
y_pred = rf_clf.predict(X_test)
y_pred
```

![heart_disease_prediction](TODO)

```py
# Evaluation
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.3f}")
```

![heart_disease_evaluation](TODO)

```py
# Create a Random Forest model
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
rf_clf = RandomForestClassifier(random_state=42)

# Grid Search
param_grid = {
  'n_estimators': [10, 50, 100, 200],
  'max_depth': [None, 10, 20, 30]
  'bootstrap': [True, False]
}
grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')
grid_search.fit(X_train, y_train)
```

```py
# Output the best hyperparameters
print(f"Best hyperparameters: {grid_search.best_params_}")

best_grid = grid_search.best_estimator_
y_pred = best_grid.predict(X_test)
```

![heart_disease_evaluation_2](TODO)

```py
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.3f}")
```

![heart_disease_evaluation_3](TODO)

```py
# Random Search
param_dist = {
  'n_estimators': [int(x) for x in range(10, 151, 10)]
  'max_depth': [None] + [int(x) for x in range(10, 41, 10), 'bootstrap': [True, False]]
}
random_search = RandomizedSearchCV(estimator=rf_clf, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')
random_search.fit(X_train, y_train)
```

```py
# Output the best hyperparameters
print(f"Best hyperparameters: {random_search.best_params_}")

best_random = random_search.best_estimator_
y_pred = best_random.predict(X_test)
```

![heart_disease_evaluation_4](TODO)

```py
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.3f}")
```

![heart_disease_evaluation_5](TODO)  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 장홍준 교수님의 데이터분석프로그래밍 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/R4ILOLf.png
[def2]: https://i.imgur.com/0meMWi5.png
[def3]: https://i.imgur.com/jigdJyw.png
[def4]: https://i.imgur.com/PdUIvqe.png
[def5]: https://i.imgur.com/6e97TbZ.png
[def6]: https://i.imgur.com/iRlWjBc.png
[def7]: https://i.imgur.com/7UwPjjM.png
[def8]: https://i.imgur.com/Y5jZzlz.png
[def9]: https://i.imgur.com/O345wRL.png
[def10]: https://i.imgur.com/upkjFJs.png
[def11]: https://i.imgur.com/SBrIgFv.png
[def12]: https://i.imgur.com/HAoy9ak.png
[def13]: https://i.imgur.com/lyNcdT0.png
[def14]: https://i.imgur.com/NnRZQpJ.png
[def15]: https://i.imgur.com/NOoaGs1.png
[def16]: https://i.imgur.com/2YCBKZH.png
[def17]: TOhttps://i.imgur.com/y6E7b3i.pngDO
[def18]: https://i.imgur.com/7DjJxlo.png
[def19]: https://i.imgur.com/BSwj4o1.png