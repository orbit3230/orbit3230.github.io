---
layout: post
title: "[도커&쿠버네티스] 6주차 - 쿠버네티스의 로드 밸런싱"
excerpt: "로드 밸런싱, Kubernetes에서의 로드 밸런싱, kube-proxy, 외부 로드 밸런서, MetalLB, Ingress, Nginx Ingress Controller, 다중 서비스 아키텍처"

tags:
  - [Docker, Kubernetes]

toc: true

date: 2025-02-24
last_modified_at: 2025-02-27
---
## 쿠버네티스의 로드 밸런싱
### 1. 로드 밸런싱
- 로드 밸런싱은 왜 필요한가?
  - 클라이언트의 요청이 많아지면, 서버의 부하가 증가하게 되고, 서버가 다운될 수 있음  
  - **Scale-up** : 서버의 성능을 높이는 방법
  - **Scale-out** : 서버의 대수를 늘리는 방법  

- 로드 밸런싱의 개념  
  - 위와 같이 트래픽이 많은 상황에서 서버의 부하를 적절히 분산시키는 것.  
  - Kubernetes에서는 여러 개의 Pod로 트래픽을 분배하여 부하를 줄이고 Availability를 높임  

  <br>

### 2. Kubernetes에서의 로드 밸런싱
- Service (LoadBalancer)
  - 역할 : 클러스터 외부의 트래픽을 받아 외부 로드 밸런서로 전달
  - 특징 : 웹 애플리케이션, API 게이트웨이, 외부에서 접근해야 하는 서비스에 사용 (Django, Flask 등)

- Ingress
  - 역할 : 도메인과 URL 경로에 따라 요청을 여러 서비스로 라우팅
  - 특징 : 하나의 엔드포인트에서 여러 서비스를 효과적으로 분기하며 관리

- kube-proxy
  - 역할 : Kubernetes의 서비스와 Pod 간의 네트워크 트래픽을 연결
  - 특징 : 트래픽 로드 밸런싱과 라우팅을 통해 내부/외부 통신을 관리  

- 외부 로드 밸런서
  - 역할 : 클러스터 외부의 트래픽을 받아 안정적이고 효율적으로 클러스터 내부로 전달.
  - 특징
    - 클라우드 환경 : AWS, GCP, Azure 등의 클라우드 서비스에서 제공하는 자체 로드 밸런서 사용  
    `트래픽 -> 외부 로드 밸런서(클라우드 자체) (-> Ingress) -> kube-proxy -> Pod -> 응답`  
    - 온프레미스 환경 : 외부 IP를 할당해 줄 MetalLB나 HAProxy 등의 별도의 외부 로드 밸런서의 세팅이 필요.  
    `트래픽 -> 외부 로드 밸런서(MetalLB, HAProxy) (-> Ingress) -> kube-proxy -> Pod -> 응답`  

    <br>

### 3. `kube-proxy` 실습
- `kube-proxy`의 default mode인 `IPTables` 모드는 **라운드 로빈** 방식으로 요청을 분산 전달하기 때문에, 이 방식으로도 간단한 로드 밸런싱이 가능함.  
  - 모드 확인  
  `$ kubectl get configmap -n kube-system kube-proxy -o yaml | grep "mode"`  

- IPVS 모드를 사용하면 더 정교한 로드 밸런싱도 가능.  

<br>

- **Apache Bench** : 웹 서버의 성능 측정을 위한 부하(load) 테스트 도구.  
  - 설치  
  `$ sudo apt-get install -y apache2-utils`  
  - 부하 테스트  
  `$ ab -n [요청 횟수] -c [동시 요청 수] [URL]`  
  - 요청 분배 숫자 확인  
  `$ kubectl logs -l app=[Pod 이름] --tail=[요청 횟수] --prefix | grep "GET /" | sed 's/^\[pod\/\([^]]*\)\/[^]]*\].*/\1/' | sort | uniq -c`  

  <br>

### 4. `MetalLB` 실습  
- 온프레미스 환경인 경우, `MetalLB`와 같은 외부 로드 밸런서를 사용해야 함.  
  
- 우선, 클러스터와 디플로이먼트를 생성  

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
    - role: control-plane
    - role: worker
    - role: worker
networking:
    disableDefaultCNI: true
    podSubnet: "192.168.0.0/16"
```

`$ kind create cluster --config [클러스터 구성 파일] --name [클러스터 이름]`  

`$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml`  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: [Deployment 이름]  
spec:
  replicas: 3
  selector:
    matchLabels:
      app: [Deployment 이름]
  template:
    metadata:
      labels:
        app: [Deployment 이름]
    spec:
      containers:
      - name: [Container 이름]
        image: nginx:latest  
        ports:
        - containerPort: 80
```

`$ kubectl apply -f [디플로이먼트 파일]`  

<br>

- MetalLB 설치  
`$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/config/manifests/metallb-native.yaml`  

<br>

- MetalLB의 Components
  - **Controller** : LoadBalancer 관련 리소스를 감시하며, **Speaker**에게 정보를 전달해 외부 IP가 어떻게 advertise 될 지 결정  
  - **Speaker** : 각 노드에서 동작하며, **Controller**로부터 받은 정보를 바탕으로 외부 IP를 advertise  

  <br>

- Nginx 기반의 웹 애플리케이션 배포  
  - **IPAddressPool** : 외부 IP 주소의 범위를 정의  
  - **L2Advertisement** : 외부 네트워크에 서비스 IP 주소를 알림  

  ```yaml
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: [IPAddressPool 이름]
    namespace: metallb-system
  spec:
    addresses:  
      - [IP 범위 시작]-[IP 범위 끝]
  ---
  apiVersion: metallb.io/v1beta1  
  kind: L2Advertisement
  metadata:
    name: [L2Advertisement 이름]
    namespace: metallb-system
  spec:
    ipAddressPools:
      - [IPAddressPool 이름]  
  ```

  `$ kubectl apply -f [IP 설정 파일]`  

  - 제대로 생성되었는 지 확인  
  `$ kubectl get ipaddresspool -n metallb-system`  
  `$ kubectl get l2advertisement -n metallb-system`  

  <br>

- LoadBalance Service 생성  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: [Service 이름]
spec:
    type: LoadBalancer
    selector:
        app: [Deployment 이름]
    loadBalancerIP: [IP 주소]  # MetalLB에서 할당할 외부 IP
    ports:
        - protocol: TCP
            port: 80
            targetPort: 80
```
    
`$ kubectl apply -f [Service 파일]`  

<br>

- 배포가 잘 되었는 지 확인  
`$ docker exec -it [Pod 이름] curl -s [Cluster IP]`  
`$ docker exec -it [Pod 이름] curl -s [External IP]`  

  - IP 확인 : `$ kubectl get svc`  

  - 포트 포워딩을 사용하면 `localhost`로도 접속 가능하다!  
  `$ kubectl port-forward svc/[Service 이름] [Local 포트]:[Service 포트]`  

<br>

### 5. `Ingress` 실습  
- Ingress
  - 외부 트래픽 라우팅 : 도메인과 URL 경로에 따라 요청을 Kubernetes 서비스로 라우팅
  - 호스트 기반 라우팅 : 도메인별로 요청을 다르게 처리
  - 경로 기반 라우팅 : URL 경로별로 요청을 다르게 처리  
  - Rewrite & Redirect : URL을 재작성하거나 리다이렉트 가능  
  - 로드 밸런싱 : 여러 Node/Pod에 트래픽을 분산  
  - TLS/SSL Endpoint : 인증서를 통해 HTTPS 트래픽을 처리  

- Nginx Ingress Controller : Kubernetes 외부에서 들어오는 HTTP(S) 요청을 클러스터 내부 서비스로 라우팅하는 Ingress Controller  

<br>

- kind로 클러스터 생성 + calico 설치  

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    labels:
      ingress-ready: "true"
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: ingress-ready=true
          taints: []  # Control-plane의 taint 제거
  - role: worker
    labels:
      ingress-ready: "true"
  - role: worker
    labels:
      ingress-ready: "true"
networking:
  disableDefaultCNI: true
  podSubnet: "192.168.0.0/16"
```

`$ kind create cluster --config [클러스터 구성 파일] --name [클러스터 이름]`  

`$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml`  

<br>

- Metallb 설치 및 IP 주소 pool 생성

```yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: [IPAddressPool 이름]
  namespace: metallb-system
spec:
  addresses:  
    - [IP 범위 시작]-[IP 범위 끝]
---
apiVersion: metallb.io/v1beta1  
kind: L2Advertisement
metadata:
  name: [L2Advertisement 이름]
  namespace: metallb-system
spec:
  ipAddressPools:
    - [IPAddressPool 이름]  
```

`$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/config/manifests/metallb-native.yaml`

`$ kubectl apply -f [IP 설정 파일]`  

<br>

- Deployment 생성  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: [Deployment 이름]
spec:
  replicas: 3
  selector:
    matchLabels:
      app: [Deployment 이름]
  template:
    metadata:
      labels:
        app: [Deployment 이름]
    spec:
      containers:
      - name: [Container 이름]
        image: nginx:latest  
        ports:
        - containerPort: 80
```

`$ kubectl apply -f [디플로이먼트 파일]`

<br>

- Nginx Ingress Controller 설치  
`kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml`  

<br>

- 서비스 생성  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: [Service 이름]
spec:
  type: ClusterIP
  selector:
    app: [Deployment 이름]
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: LoadBalancer
  loadBalancerIP: [MetalLB에서 할당한 외부 IP]
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
```

`$ kubectl apply -f [Service 파일]`  

<br>

- Ingress 생성  

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: [Ingress 이름]
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: [도메인].local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: [Service 이름]
                port:
                  number: 80
```

`$ kubectl apply -f [Ingress 파일]`  

<br>

- 포트 포워딩을 사용하면 `localhost`로도 접속 가능하다!  
`$ kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller [Local 포트]:[Service 포트]`  

  - 단, 아래 두 경로에서 `127.0.0.1 [도메인].local`을 추가해야 함  
    - Windows : `C:\Windows\System32\drivers\etc\hosts`  
    - Linux : `/etc/hosts` (`sudo`)  

    <br>

### 6. 다중 서비스 아키텍처 실습  
- Nginx Ingress Controller를 활용한 다중 서비스 아키텍처  

- (1) frontend-service (Nginx)  
- (2) backend-service (Django)
- (3) db-service (PostgreSQL)  

<br>

- kind로 클러스터 생성 + calico 설치  

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    labels:
      ingress-ready: "true"
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: ingress-ready=true
          taints: []  # Control-plane의 taint 제거
  - role: worker
    labels:
      ingress-ready: "true"
  - role: worker
    labels:
      ingress-ready: "true"
networking:
  disableDefaultCNI: true
  podSubnet: "192.168.0.0/16"
```

`$ kind create cluster --config [클러스터 구성 파일] --name [클러스터 이름]`  

`$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml`  

<br>

- Metallb 설치 및 IP 주소 pool 생성

```yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: [IPAddressPool 이름]
  namespace: metallb-system
spec:
  addresses:  
    - [IP 범위 시작]-[IP 범위 끝]
---
apiVersion: metallb.io/v1beta1  
kind: L2Advertisement
metadata:
  name: [L2Advertisement 이름]
  namespace: metallb-system
spec:
  ipAddressPools:
    - [IPAddressPool 이름]  
```

`$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/config/manifests/metallb-native.yaml`

`$ kubectl apply -f [IP 설정 파일]`  

<br>

- Frontend (Nginx + Logger) Deployment 생성, Service 배포  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
        - name: logger
          image: busybox
          command: ["sh", "-c", "while true; do echo 'Logging frontend...'; sleep 10; done"]

---
apiVersion: v1
kind: Service
metadata:
  name: [Service 이름]  
spec:
  type: ClusterIP
  selector:
    app: frontend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
```

`$ kubectl apply -f [디플로이먼트+서비스 파일]`  

<br>

- Backend (Django + Logger) 배포  
  - (1) [backend app 프로젝트 생성 (Django)][def2]  
  
  - (2) kind 클러스터에 이미지 로드  
  `$ kind load docker-image [이미지 이름] --name [클러스터 이름]`  
  또는, 이미지 파일을 생성하는 스크립트 제작  

<br>

- Backend (Django + Logger) 배포  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
        - name: django
          image: [이미지 이름]:latest
          ports:
            - containerPort: 8000
          env:
            - name: DATABASE_URL
              value: "postgresql://postgres:password@db-service:5432/mydb"
        - name: logger
          image: busybox
          command: ["sh", "-c", "while true; do echo 'Logging backend...'; sleep 10; done"]

---
apiVersion: v1
kind: Service
metadata:
  name: [Service 이름]  
spec:
  type: ClusterIP
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
```

`$ kubectl apply -f [디플로이먼트+서비스 파일]`  

<br>

- Database (PostgreSQL + Backup) 배포  

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: "db-service"
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      labels:
        app: db
    spec:
      containers:
        - name: postgres
          image: postgres:13
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: "mydb"
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              value: "password"
          volumeMounts:
            - name: db-storage
              mountPath: /var/lib/postgresql/data
        - name: backup
          image: busybox
          command: ["sh", "-c", "while true; do echo 'Backing up DB...'; sleep 3600; done"]

      volumes:
        - name: db-storage
          persistentVolumeClaim:
            claimName: db-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: db-service
spec:
  type: ClusterIP
  selector:
    app: db
  ports:
    - protocol: TCP
      port: 5432
      targetPort: 5432
```

`$ kubectl apply -f [디플로이먼트+서비스 파일]`  

<br>

- Nginx Ingress Controller 설치
`$ kubeclt apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml`  

<br>

- Ingress 생성 및 호스트 규칙 설정  

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: [Ingress 이름]
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: [도메인].local  
      http:
        paths:
          - path: /frontend
            pathType: Prefix
            backend:
              service:
                name: [프론트엔드 서비스 이름]
                port:
                  number: 80
          - path: /backend
            pathType: Prefix
            backend:
              service:
                name: [백엔드 서비스 이름]
                port:
                  number: 8000
          - path: /db
            pathType: Prefix
            backend:
              service:
                name: [데이터베이스 서비스 이름]
                port:
                  number: 5432
```

`$ kubectl apply -f [Ingress 파일]`  

<br>

- 포트 포워딩을 사용하면 `[도메인].local`로도 접속 가능하다!  
`$ kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller [Local 포트]:[Service 포트]`  

  - 단, 아래 두 경로에서 `127.0.0.1 [도메인].local`을 추가해야 함  
    - Windows : `C:\Windows\System32\drivers\etc\hosts`  
    - Linux : `/etc/hosts` (`sudo`)  

    <br>  


<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 이다영 교수님의 도커&쿠버네티스 스터디 내용을 정리 한 것입니다.  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details>

[def]: https://i.imgur.com/gPngBWG.png
[def2]: https://orbit3230.github.io/2025/02/10/DK_week4/