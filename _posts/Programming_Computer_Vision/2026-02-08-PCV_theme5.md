---
layout: post
title: "[프로그래밍 컴퓨터 비전] Theme 5 - Multiple View Geometry"
excerpt: "Epipolar Geometry, Computing with Cameras and 3D Structure, Multiple View Reconstruction, Stereo Images"

tags:
  - [프로그래밍 컴퓨터 비전]

toc: true

date: 2026-02-08
last_modified_at: 2026-02-08
---
## Multiple View Geometry
- 챕터 목표 : Multiple View를 다루고 이들 간의 Geometric 관계를 사용하여 카메라 위치와 3D 구조를 추정하는 방법 이해하기
  - 여러 위치에서 촬영된 이미지를 사용하여 feature matches를 통해 3D 씬 포인트 뿐만 아니라 카메라 위치도 추정 가능 

### 1. Epipolar Geometry
- Multiple View Geometry : 여러 서로 다른 viewpoints에서 촬영된 이미지들 간의 대응관계가 존재할 때, 카메라와 feature 간 관계를 연구하는 분야

- 하나의 씬에 대한 두 views가 주어지고 이들 간에 대응되는 points가 있다면, 이미지의 points에 기하학적인 **제약조건**이 존재한다.  
(카메라들의 상대적인 방향(orientation)과 특성(properties), 그리고 위치(position) 때문에)  

- 여기서 이 기하학적인 관계는 **Epipolar Geometry** 라고 불리는 것으로 설명된다.  

<br>

- 카메라에 대한 사전 지식이 전혀 없다면, 내재된 모호함이 존재한다.  
  - 어떠한 3차원 point `X`에 대하여 임의의 `4`x`4` 행렬 `H`를 적용해 `HX`로 변환시키더라도,  
  카메라를 `P`에서 `PH`<sup>`-1`</sup>으로 바꾸면 원래 카메라 `P`에서와 동일한 이미지 points를 얻게 된다.  
  `x̂` = `PX` = `PH`<sup>`-1`</sup>`HX` = `P̂X̂`
    - `X` : 실제 3D point
    - `P` : 실제 카메라 행렬
    - `x̂` : 이미지 point

  - 그러니까, 대응되는 points로부터 `P`와 `X`를 복원하려 하지만 실제로는 무한히 많은 해가 존재한다.  
    - 이 중 어떤 해가 진짜인지는 카메라 내부 정보(**internal calibration**) 없이는 절대 알 수 없다.
    - 그래서 **projective reconstruction** 까지만 가능하다.
    - **internal calibration**이 주어지면, **metric reconstruction**이 가능하다.
      - **metric reconstruction**은 거리와 각도를 정확하게 표현하는 3D reconstruction을 의미한다.

  - 비유하자면, 세상을 찌그러뜨리고 늘리고 비틀어도, 카메라 위치를 그에 맞게 바꾸면 똑같은 이미지가 나올 수 있다는 뜻이다.  

<br>

- 이러한 모호함 때문에, 카메라에 homography 변환을 적용하여 문제를 단순화한다.  
여기서 homography 변환은 주로 좌표계를 바꾸기 위한 **rigid transformation**이다.  

- 한 가지 좋은 선택은, 좌표계의 원점과 축을 첫 번째 카메라에 맞추는 것이다.  
`P`<sub>`1`</sub> = `K`<sub>`1`</sub>`[I | 0]` and `P`<sub>`2`</sub> = `K`<sub>`2`</sub>`[R | t]`  
  - `[R | t]`는 `R` 행렬과 `t` 벡터를 옆으로 붙인 것 (column-wise concatenation)
  - `P`<sub>`1`</sub> : 첫 번째 카메라 행렬
  - `P`<sub>`2`</sub> : 두 번째 카메라 행렬
  - `K`<sub>`1`</sub>, `K`<sub>`2`</sub> : 각각 첫 번째, 두 번째 카메라의 내부 파라미터 행렬
  - `R` : Rotation matrix (첫 번째 카메라 좌표계에서 두 번째 카메라 좌표계로의 회전)
  - `t` : Translation vector (첫 번째 카메라 좌표계에서 두 번째 카메라 좌표계로의 이동)
  - `[I | 0]` : Do Nothing !

- 따라서 실제 `X` points의 projections인 `x`<sub>`1`</sub>과 `x`<sub>`2`</sub> (각각 `P`<sub>`1`</sub>과 `P`<sub>`2`</sub>에 의해 투영된 것)은  
반드시 하나의 수식적 제약을 만족해야한다.  
  - 이 제약은 **Epipolar Constraint** 라고 불리며, 다음과 같이 표현된다.  
  `x`<sub>`2`</sub><sup>`T`</sup>`F``x`<sub>`1`</sub> = `0`  
    - `F` : Fundamental Matrix (3x3 행렬)
    - `x`<sub>`1`</sub> : 첫 번째 이미지의 point (Homogeneous coordinates)  
    - `x`<sub>`2`</sub> : 두 번째 이미지의 point (Homogeneous coordinates)  

  - 여기서 `F` = `K`<sub>`2`</sub><sup>`-T`</sup>`S`<sub>`t`</sub>`R``K`<sub>`1`</sub><sup>`-1`</sup>  
    - `S`<sub>`t`</sub> : `t` 벡터에 해당하는 Skew-symmetric matrix  
    ![skew_symmetric_matrix][def]  
    - `F`는 rank `2`의 행렬이며, `det(F)` = `0` 이다. (따라서 역행렬이 존재하지 않음)  
    - `F`는 두 카메라 사이의 모든 기하학적 관계를 `3`x`3` 행렬 하나에 압축시킨 것이다.  

  - 위 수식을 통해 `F`로부터 카메라 행렬들을 복원할 수 있다.
    - 차후 알게 되겠지만, 결과적으로 `F`는 이미지 대응점(point correspondences)으로부터 계산할 수 있다.

<br>

- 이 이론을 실제로 이미지 데이터에 적용하러 가기 전에 한 가지 마지막 기하학적 개념이 필요하다.

- 예를들어 `x`<sub>`2`</sub>가 두 번째 이미지의 point라고 하면,  
아래 수식은 첫 번째 이미지에서 `x`<sub>`1`</sub>이 위치할 수 있는 모든 점들의 집합, 즉 하나의 직선을 정의하며 이를 **epipolar line** 이라고 부른다.  
`x`<sub>`2`</sub><sup>`T`</sup>`F``x`<sub>`1`</sub> = `l`<sub>`1`</sub><sup>`T`</sup>`x`<sub>`1`</sub> = `0`  
  - `x`<sub>`2`</sub> : 두 번째 이미지의 point
  - `F` : Fundamental Matrix
  - `x`<sub>`1`</sub> : 첫 번째 이미지의 point
  - `l`<sub>`1`</sub> : 첫 번째 이미지에서의 epipolar line  

![epipolar_line][def2]

- 수식 `l`<sub>`1`</sub><sup>`T`</sup>`x`<sub>`1`</sub> = `0`는 첫 번째 이미지에서 수식을 만족하는 모든 `x`<sub>`1`</sub>으로 직선을 정의한다.  
  - 다시 말해 `x`<sub>`2`</sub>에 대응되는 epipolar line을 정의한다.  
  - 따라서, `x`<sub>`2`</sub>에 대한 대응점(corresponding point)가 반드시 이 직선 위에 존재해야한다는 제약이 생긴다.

- 따라서 Fundatmental matrix `F`는 대응점을 찾는 문제를 이 직선 위에서로 제한시켜줌으로써 단순화시켜준다.  

<br>

- 모든 Epipolar lines는 `e`라고 불리는 한 점에서 만나는데, 이 점을 **epipole**이라고 부른다.  
  - 이 **epipole**은 다른 카메라의 중심(위치)을 projection한 이미지 상의 점이다.
    - `e`<sub>`1`</sub>은 카메라 `P`<sub>`2`</sub>의 중심 `C`<sub>`2`</sub>를 카메라 `P`<sub>`1`</sub>의 이미지로 투영한 점이다.
    - `e`<sub>`2`</sub>는 카메라 `P`<sub>`1`</sub>의 중심 `C`<sub>`1`</sub>를 카메라 `P`<sub>`2`</sub>의 이미지로 투영한 점이다.

  - 이 점은 카메라들의 상대적인 방향에 따라 실제 이미지 영역 밖에 위치할 수도 있다.

  - **Epipole**은 모든 epipolar lines가 만나는 점이므로, 모든 epipolar lines 위에 놓여있기 때문에 반드시 다음 조건을 만족해야 한다  
  `F``e`<sub>`1`</sub> = `0`  
    - 따라서, epipole은 Fundamental matrix `F`의 null vector로 계산할 수 있다. (차후에 보게될 것이다)

  - 반대쪽 epipole도 마찬가지로 `e`<sub>`2`</sub><sup>`T`</sup>`F` = `0` 으로부터 계산할 수 있다.  

  <br>  

#### A Sample Data Set

```py
import camera
import numpy as np
from PIL import Image
from numpy import loadtxt
from numpy import genfromtxt

# load some images
image1 = np.array(Image.open('../../images/001.jpg'))
image2 = np.array(Image.open('../../images/002.jpg'))

# load 2D points for each view to a list
points2D = [loadtxt('../../2D/00' + str(i+1) + '.corners').T for i in range(3)]

# load 3D points
points3D = loadtxt('../../3D/p3d').T

# load correspondences
correspondences = genfromtxt('../../2D/nview-corners', dtype=int, missing_values='*', filling_values=-1)

# load cameras to a list of Camera objects
P = [camera.Camera(loadtxt('../../2D/00' + str(i+1) + '.P')) for i in range(3)]
```

- 위 코드는
  - (1) 세 개 중 두 개의 이미지 (`image1`, `image2`)
  - (2) 세 가지 view에서의 모든 이미지 feature points (`points2D`, `points3D`)
  - (3) 각 image points에 대응되는 reconstructed 3D points (`correspondences`)
  - (4) 그리고 카메라 행렬을 불러오는 코드이다. (`P`)  

- `loadtxt()` : 텍스트 파일을 읽어 `NumPy` 배열로 반환하는 함수
- `genfromtxt()` : `loadtxt()`와 유사하지만, 누락된 데이터나 다른 데이터 유형을 처리할 수 있는 함수
  - `correspondences`는 missing data를 포함할 수 있기 때문에 사용한다. (모든 points가 보이지는 않을 수 있고, 완벽하게 매치되지 않을 수 있기 때문)

- 위 코드를 가령 `load_vggdata.py`라는 이름의 파일로 저장하고, `execfile()` 명령어를 서두에 붙여서 간편하게 실행할 수 있다.  

```py
execfile('load_vggdata.py')
```

- 다만 `execfile()`은 Python 3에서는 제거되었으므로, Python 3에서는 `import`하여 다음과 같이 사용할 수 있다.  

```py
import load_vggdata

# 가령
im1 = load_vggdata.image1
```

<br>

- 이제 3D points를 하나의 view로 투영해보고, 관찰된 image points와 비교해보자.  

```py
import load_vggdata as data
from numpy import vstack, ones
from matplotlib.pyplot import *

X = vstack((data.points3D, ones(data.points3D.shape[1])))
x = data.P[0].project(X)

figure()
imshow(data.image1)
plot(data.points2D[0][0], data.points2D[0][1], '*')  # 실제로 관찰된 image points
axis('off')

figure()
imshow(data.image1)
plot(x[0], x[1], 'r.')  # 계산한 예측된 image points  
axis('off')

show()
```

- 첫 번째 카메라 view의 이미지와 그 이미지에서 관찰된 points들(`points2D[0]`)을 함께 보여주는 그림을 만든다.

- 비교를 위해 3D points를 첫 번째 카메라로 투영하여 계산된 image points(`x`)도 보여주는 그림을 만든다.  

- 실제로 관찰된 points와 계산된 points가 거의 일치한다.  
![projection_comparison][def3]

- 비교를 위해 계산된 points의 개수가 미세하게 더 많은 것을 볼 수 있는데, 이는 계산된 3D points 중 일부가 첫 번째 카메라 view에서 보이지 않기 때문이다.  

#### Plotting 3D data with Matplotlib
- `matplotlib`의 `mplot3d` 모듈을 사용하여 3D data를 시각화할 수 있다.
  - `get_test_data()`로 샘플 points를 생성하여 3D scatter plot을 만들어보자.
  - `fig.gca(projection='3d')`는 최신 버전에서 deprecated 되었으므로, `fig.add_subplot(111, projection='3d')`로 대체하여 사용한다.  

```py
from mpl_toolkits.mplot3d import axes3d
from matplotlib.pyplot import figure, show

fig = figure()
# ax = fig.gca(projection='3d')
ax = fig.add_subplot(111, projection='3d')

# generate 3D sample data
X, Y, Z = axes3d.get_test_data(0.25)

# plot the points in 3D
ax.plot(X.flatten(), Y.flatten(), Z.flatten(), 'o')

show()
```

![3d_scatter_plot_1][def4]  
![3d_scatter_plot_2][def5]  

- 이제 3D points를 시각화해보자.  

```py
from mpl_toolkits.mplot3d import axes3d
from matplotlib.pyplot import figure, show
import load_vggdata as data

fig = figure()
# ax = fig.gca(projection='3d')
ax = fig.add_subplot(111, projection='3d')

# plotting 3D points
ax.plot(data.points3D[0], data.points3D[1], data.points3D[2], 'k.')

show()
```  

![3d_points_1][def6]  
![3d_points_2][def7]  
![3d_points_3][def8]  

<br>

#### Computing F - The Eight Point Algorithm
- **Eight Point Algorithm**은 이미지 대응점(correspondences)으로부터 Fundamental matrix `F`를 계산하는 알고리즘이다.  

### 2. Computing with Cameras and 3D Structure

<br>

### 3. Multiple View Reconstruction

<br>

### 4. Stereo Images

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

- Jan Erik Solem, "Programming Computer Vision with Python", 2012  
  - 본 포스팅은 위 도서의 내용을 정리한 것입니다.
  - 다른 곳으로의 무분별한 내용 복사를 자제해 주세요.  

</div>
</details> 

[def]: https://i.imgur.com/xkXuch2.png
[def2]: https://i.imgur.com/GUBBOFm.png
[def3]: https://i.imgur.com/KQmk1FZ.png
[def4]: https://i.imgur.com/FnzrH8Z.png
[def5]: https://i.imgur.com/UYRgitp.png
[def6]: https://i.imgur.com/1EA0kJi.png
[def7]: https://i.imgur.com/5GyyguV.png
[def8]: https://i.imgur.com/2ced7sE.png