---
layout: post
title: "[인공지능수학] 15주차 - 머신러닝"
excerpt: "머신러닝 소개, 분류 알고리즘, 회귀분석 알고리즘"

tags:
  - [인공지능수학, Math]

toc: true

date: 2025-12-04
last_modified_at: 2025-12-08
---
## 머신러닝
### 1. 머신러닝 소개  
- 머신러닝 : 에이전트가 데이터를 분석하고 이해하는 알고리즘  

- 머신러닝의 분류
  - (1) 지도학습(supervised learning) : 레이블된 훈련 데이터를 활용해 모델을 학습하여, 학습하지 않은 데이터를 예측 or 분류  
    - 분류(classification) : 클래스를 예측하는 지도학습
    - 회귀(regression) : 연속적인 값을 출력하는 지도학습

  - (2) 비지도학습(unsupervised learning) : 지도학습과 달리 레이블되지 않았거나 구조를 파악할 수 없는 데이터에서 의미있는 정보를 추출하는 방법
    - 군집화(clustering) : 사전 정보 없이 모여있는 그룹의 정보를 의미 있는 서브 그룹 또는 클러스터로 탐색하는 데이터 분석 기법
    - 주성분분석(principal component analysis) : 고차원 데이터의 정보를 유지하면서 작은 차원의 부분공간으로 데이터를 압축하는 기법

  - (3) 강화학습(reinforcement learning) : 에이전트가 환경과 상호작용하며 보상을 최대화하는 행동을 학습하는 방법  

<br>

- 특성 행렬  
![feature_matrix][def4]  
  - 행 : 샘플(sample) or 예제(example)
  - 열 : 특성(feature)

<br>

- 훈련 데이터셋 : 머신러닝 모델을 훈련하고 최적화하는 데 사용
- 테스트 데이터셋 : 훈련 데이터셋과 달리 훈련에 사용하는 것이 아니라 훈련이 완료된 모델을 평가할 때 사용  

<br>

- 과소적합(underfitting) : 머신러닝 시스템의 용량이 작아서 큰 오차가 발생, 최적화가 제대로 이루어지지 않는 상태
- 과대적합(overfitting) : 주어진 데이터 분포와 비교했을 때 머신러닝 시스템의 용량이 커서 잡음(noise)까지 수용하는 것

<br>

- 교차검증(cross-validation) : 데이터를 수집하는 데는 많은 비용이 들기 때문에 데이터의 양이 부족한 경우가 많다. 데이터가 부족한 경우에는 검증 데이터셋을 따로 확보하기 어려우므로, 훈련 데이터셋을 여러 개의 부분집합으로 나누어 교차검증을 수행한다.  
  - k-교차검증(k-cross validation) : 훈련 데이터셋을 같은 크기의 `k`개의 그룹으로 나누어 `k-1`개의 그룹은 훈련에 이용하고, 나머지 `1`개의 그룹은 성능을 평가하는 데 이용

  <br>

- 데이터의 수가 `n`개이고 학습한 모델이 정답을 맞힌 데이터 수가 `k`개라고 할 때, 모델의 **정확도**(accuracy)는 다음과 같다.  
![accuracy][def]  

- 혼동행렬(confusion matrix)  
![confusion_matrix][def2]  
  - TP(True Positive) : 실제 *양성*인 경우를 *양성*으로 정확히 예측한 경우 (참)
  - FP(False Positive) : 실제 *음성*인 경우를 *양성*으로 잘못 예측한 경우 (거짓)
  - TN(True Negative) : 실제 *음성*인 경우를 *음성*으로 정확히 예측한 경우 (참)
  - FN(False Negative) : 실제 *양성*인 경우를 *음성*으로 잘못 예측한 경우 (거짓)

- **정확도**(accuracy), **정밀도**(precision), **재현율**(recall), **F1 점수**(F1 score)를 다음과 같이 정의한다.  
![metrics][def3]  
  - 정확도 : 전체 예측 중에서 맞게 예측한 비율
  - 정밀도 : 양성으로 예측한 것들 중에서 맞게 예측한 비율
    - downside : 확실한 하나만 양성으로 예측하고 나머지는 모두 음성으로 예측하는 경우, 정밀도는 `1`이 됨
  - 재현율 : 실제 양성이었던 것들 중에서 맞게 예측한 비율
    - downside : 모든 것을 양성으로 예측하는 경우, 재현율은 `1`이 됨  
  - F1 점수 : 정밀도와 재현율의 조화평균

<br>

- 회귀(Regression)에서 사용하는 Metrics
  - 평균절대오차(MAE, Mean Absolute Error)  
  ![MAE][def5]  

  - 평균제곱오차(MSE, Mean Squared Error)  
  ![MSE][def6]  

  - 평균제곱근오차(RMSE, Root Mean Squared Error)   
  ![RMSE][def7]  

  - R<sup>2</sup> Score  
  ![R2 Score][def8]  

  <br>

### 2. 분류 알고리즘
- k-최근접 이웃(k-Nearest Neighbors, k-NN) 알고리즘  
![kNN][def9]  
  - 단계
    - (1) 숫자 `k`와 측정할 거리 기준을 선택
    - (2) 분류하고자 하는 새로운 샘플 데이터를 기준으로 `k`개의 최근접 이웃 데이터를 찾음
    - (3) 최근접한 `k`개의 데이터를 대상으로 다수결 투표하여 데이터가 더 많은 클래스를 분류하고자 하는 새로운 샘플 데이터에 할당

<br>

- 의사결정트리 (Decision Tree, DT) 알고리즘  
![DT][def10]  
  - 트리 모양의 그래프로, 각 노드에서는 데이터셋의 특성을 판단하고 노드에서 갈라져 나온 가지들은 판단 결과에 대응
  - 일종의 순서도(flow chart)로, 근거가 투명한 분류 과정이 필요한 데이터에 적합  

  - **정보 이득**(information gain, IG) 함수 `IG()`는 다음과 같이 정의한다.  
  ![IG][def11]  
    - 여기서 `D`<sub>`p`</sub>와 `D`<sub>`j`</sub>는 각각 부모노드와 `j`번째 자식노드의 데이터셋이고, `f`는 분할에 사용할 특성이다. 또한 `I()`는 불순도 지표이고, `N`<sub>`p`</sub>는 부모노드에 있는 전체 샘플 개수이며, `N`<sub>`j`</sub>는 `j`번째 자식노드에 있는 샘플 개수이다.  

  - 자주 사용하는 불순도 지표  
    - 지니 불순도(Gini impurity)  
    ![Gini][def12]  
    - 엔트로피(Entropy)  
    ![Entropy][def13]  

  <br>

- 랜덤 포레스트(Random Forest) 알고리즘
  - 분류 성능이 뛰어나고 쉽게 확장 가능
  - 의사결정트리 알고리즘의 앙상블
    - 앙상블(ensemble) : 여러 개의 분류기를 생성하고, 각 예측을 결합하여 정확한 예측을 도출

<br>

### 3. 회귀분석 알고리즘
- 선형 회귀(Linear Regression) : 하나 이상의 특성(독립변수)과 연속적인 목표(종속변수) 사이의 관계를 모델링하는 기법
  - 목표 변수가 1개인 회귀모델 : 단변량 선형 회귀(univariate linear regression)
  - 목표 변수가 2개 이상인 회귀모델 : 다변량 선형 회귀(multivariate linear regression)  

  <br>

- 단변량 선형회귀 중 하나의 특성 `x`와 연속적인 목표변수 `y` 사이의 관계를 모델링하는 선형 회귀 모델을 **단순선형회귀**(simple linear regression)라고 한다.  
- 따라서 특성이 하나인 단순선형회귀모델의 식은 다음과 같다.  
`y` = `w`<sub>`0`</sub> + `w`<sub>`1`</sub>`x`  

- 훈련 데이터 셋에 가장 잘 맞는 직선을 **회귀직선**(regression line)이라고 한다.  
![regression_line][def14]  
  - 회귀직선과 샘플 사이의 거리를 **오차**(error) 또는 **잔차**(residual)라고 한다.
  - 선형회귀직선에서 상수항에 해당하는 `w`<sub>`0`</sub>는 **편향**(bias)이라고 한다.  

  <br>

- **다중선형회귀**(multiple linear regression)는 연속적인 목표변수 `y`와 여러 개의 특성이 있는 경우로서 단순선형회귀모델을 일반화한 것이다.  
식은 다음과 같다.  
![multiple_linear_regression][def15]  

<br>

- **다항회귀**(polynomial regression)는 독립변수들과 종속변수가 이차함수 이상의 관계를 갖는 것이다.  
일반적인 곡선에 대해 다항회귀모델을 가정하면 다음과 같은 식으로 표현한다.  
`y` = `w`<sub>`0`</sub> + `w`<sub>`1`</sub>`x` + `w`<sub>`2`</sub>`x`<sup>`2`</sup> + ... + `w`<sub>`d`</sub>`x`<sup>`d`</sup>  
  - 다항회귀는 데이터가 Linear가 아닌 Non-Linear일 때 사용한다.  

<br>

### 4. 군집화와 주성분분석
- k-평균 군집화(k-means clustering) 알고리즘  
![k-means][def16]  
  - 비슷한 객체로 이루어진 그룹을 찾는 기법
  - 한 그룹 안에 있는 객체들은 다른 그룹에 있는 객체보다 더 관련이 있다고 볼 수 있다.
  - 즉, k-평균 군집화의 목표는 특성의 유사도에 기초하여 샘플을 그룹으로 모으는 것이다.  
  - 여기서 비슷한 객체로 이루어진 그룹을 **군집**(cluster)이라고 한다.  
    - 샘플을 군집으로 묶는 데 많이 사용하는 거리는 두 점 사이의 **유클리드 거리의 제곱**(squared Euclidean distance)이다.  
    ![squared_euclidean_distance][def17]  

    - k-평균 군집화는 군집 안에서 **오차제곱합**(sum of squared errors, SSE) 또는 **군집 관성**(cluster inertia)을 최소화하는 방법으로 다음과 같다.  
    ![sse][def18]  

<br>

- k-평균++ (k-means++) 알고리즘
  - k-평균 군집화 알고리즘의 초기 중심점 선택 방법을 개선한 알고리즘
  - k-개의 중심점 위치를 서로 멀리 떨어진 위치에 설정한 후 학습을 진행한다.

<br>

- **특성 선택** : 원본 데이터의 특성을 유지하며 데이터셋의 차원을 축소하는 방법
- **특성 추출** : 새로운 특성 공간으로 원본 데이터를 변환/투영하여 데이터셋의 차원을 축소하는 방법

- **주성분분석**(principal component analysis, PCA) : 특성 사이의 상관관계를 기반으로 데이터의 특성을 찾아, 고차원 데이터에서 분산이 가장 큰 방향을 찾고 원본 데이터보다 작거나 같은 수의 차원을 갖는 부분공간으로 원본 데이터를 투영.  
  - 비지도 선형 기법 중 하나로, 주로 차원 축소의 용도로 많이 사용.
  - 여기서 분산이 가장 큰 방향으로 해석되는 새로운 부분공간의 직교좌표를 **주성분**(principal component)이라고 한다.  

- 주성분분석을 사용하여 차원을 축소하기 위해서는 `n` x `k` 크기의 변환행렬 `W`를 만든다.  
  - 그리고 변환행렬 `W`를 사용하여 `n`개의 특성을 가진 샘플벡터 `x`를 새로운 `k`차원의 특성 부분공간으로 투영한다.
  - 이때 변환행렬 `W`를 **투영행렬**(projection matrix)이라고 한다.  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 이구연 교수님의 인공지능수학 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details>

[def]: https://i.imgur.com/eW8lgkF.png
[def2]: https://i.imgur.com/B0m0mdv.png
[def3]: https://i.imgur.com/PBcFg7r.png
[def4]: https://i.imgur.com/SGc4GGl.png
[def5]: https://i.imgur.com/1Ui2Fjh.png
[def6]: https://i.imgur.com/gRXCqVz.png
[def7]: https://i.imgur.com/FVPcUVw.png
[def8]: https://i.imgur.com/Qv9K3v4.png
[def9]: https://i.imgur.com/BcN03oz.png
[def10]: https://i.imgur.com/2wEjMlQ.png
[def11]: https://i.imgur.com/uhZW7jD.png
[def12]: https://i.imgur.com/LvDZcVG.png
[def13]: https://i.imgur.com/NDrRzo1.png
[def14]: https://i.imgur.com/o8gJNZs.png
[def15]: https://i.imgur.com/AgAyi0b.png
[def16]: https://i.imgur.com/7zNkv7l.png
[def17]: https://i.imgur.com/9eB7gQR.png
[def18]: https://i.imgur.com/qE82jnb.png