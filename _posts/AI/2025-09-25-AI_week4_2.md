---
layout: post
title: "[인공지능] 4주차 (2) - Monte-Carlo Methods"
excerpt: "Dynamic Programming vs. Reinforcement Learning, Monte-Carlo Methods, Off-Policy Monte-Carlo Methods"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-25
last_modified_at: 2025-09-29
---
## Monte-Carlo Methods
### 1. Dynamic Programming vs. Reinforcement Learning
- Dynamic Programming : Model-based  
`P(s', r | s, a) = p`<sub>`*`</sub>`(s', r | s, a)`  
  - 환경의 모델(= 현재 상태에서 행동을 수행했을 때, 다음 상태 및 보상을 받을 확률)을 알고 있음
  - 환경에 대한 완벽한 지식을 바탕으로 최적의 정책을 계획  

- Reinforcement Learning : Model-free  
`P(s', r | s, a) ≈ p`<sub>`*`</sub>`(s', r | s, a)`  
  - 환경의 모델을 모름
  - 환경에 대한 불완전한 지식을 바탕으로 최적의 정책을 학습

<br>

- Backup Diagram  
![backup_diagram][def]  
  - 마르코프 결정 프로세스에 대한 상태-행동-보상의 에피소드를 시각적으로 표현
  - 상태는 백색 원, 행동은 흑색 원으로 표현
  - 상태-행동 간에는 정책, 행동-상태 간에는 보상을 표현
  - Backup : 현재 상태에서 전이할 수 있는 다음 상태를 의미  

- Full Backup  
![full_backup][def2]  
  - Dynamic Programming에서 사용
  - **환경의 모델을 바탕으로 모든 다음 상태와 보상**을 고려하여 현재 상태에 대한 가치 함수를 업데이트
  - 모든 상태에 대해 가치 함수의 값을 계산하므로, 상태의 개수가 많다며녀 많은 연산이 필요  

- Sample Backup  
![sample_backup][def3]  
  - Reinforcement Learning에서 사용
  - 환경의 모델을 모르므로, 에이전트가 환경과 상호작용하여 **실제로 얻은 수익 (또는 추정치)**를 활용하여 가치 함수를 업데이트
  - 에피소드 내에서 경험한 상태에 대해서만 가치 함수의 값을 계산하므로, 연산량은 상태의 개수와는 독립적

<br>

### 2. Monte-Carlo Methods
- 반복 무작위 추출(Repeated Random Sampling)을 수행하여 문제를 해결하는 방법론
  - Example  
  ![monte_carlo_example][def4]  
    - (1) 주어진 입력 범위 (x = `[0, 1]`, y = `[0, 1]`) 내에서 반복 무작위 추출을 통해 점들을 생성
    - (2) 생성된 점들이 사분원호 내에 존재하는지 판단
    - (3) 사분원호 내에 존재하는 점들의 개수가 사분원 호의 넓이의 추정치
    - (4) 추출된 점들이 많을수록 사분원호의 참인 넓이, `π/4`에 가까워짐  

<br>

- Monte Carlo Methods for RL
  - 한 에피소드가 종료된 후, 에이전트가 실제로 경험한 행동, 상태, 보상으로 가치 함수를 추정하고 정책을 개선
  - 에피소드를 수 없이 반복하면? -> Generalized Policy Iteration (GPI)  
  - Policy Iteration과 마찬가지로, Monte Carlo Methods 또한 Policy Evaluation(**Monte Carlo Prediction**)과 Policy Improvement(**Monte Carlo Control**)를 반복하여 최적의 정책을 학습  

  <br>

- Monte Carlo Prediction
  - Episodes  
  ![mc_prediction_episodes][def5]  
    - 강화학습 에이전트가 주어진 정책에 기반하여 시작 시점 ~ 종료 시점까지 환경과 상호작용한 내역
    - 시간에 따라 발생한 상태, 행동, 보상을 나타내는 확률 변수들의 순차적 모임

  - Return  
  ![mc_prediction_return][def6]  
    - 만약, 한 에피소드에서 에이전트가 주어진 정책을 따라 환경과 상호작용한 내역을 알고 있다면, 특정 시점의 상태에서 해당 에피소드에 한해 얻을 수 있는 수익을 알 수 있음

  - Value Function  
  ![mc_prediction_value_function][def7]
    - 만약, 무한히 많은 에피소드에서 에이전트가 주어진 정책을 따라 환경과 상호작용한 내역을 알고 있다면, 특정 시점의 상태에서 일반적으로 얻을 수 있는 수익의 기대값(= 상태 가치 함수)를 추정할 수 있음

  - Incr. Imple.  
    - Monte Carlo Methods의 수익의 기대값은 Incremental Implementation으로 계산 가능  
    ![mc_prediction_incr_imple][def8]  
    - 또한, 서로 다른 에피소드에서 환경의 모델이 변할 수 있다고 가정한다면(Non-Stationary), 평균 대신 스텝 사이즈 `α`를 활용  
    ![mc_prediction_incr_imple_non_stationary][def9]  

  - Update Rule  
  ![mc_prediction_update_rule][def10]  

<br>

- Monte Carlo Control -> 환경의 모델 `P(s', r | s, a)`를 모르므로, 행동 가치 함수를 계산할 수 없음! 따라서 추정.  
  - Action-Value Function
    - 상태 가치 함수를 활용해 더 나은 정책으로 개선하기 위해서는 **환경의 모델에 대한 지식**이 필요
    - 강화 학습은 환경의 모델에 대한 지식이 없으므로, 상태 가치 함수로부터 행동 가치 함수를 계산하는 대신 직접 **행동 가치 함수**를 추정  
    ![mc_control_action_value_function][def11]  
    - 이후 Policy Improvement와 동일하게, 현재 정책에서 추정된 행동 가치 함수의 값이 최대가 되는 행동을 선택하는 더 나은 정책을 생성  
    ![mc_control_policy_improvement][def12]  

  - Convergence Issues
    - Policy Iteration: 환경의 모델을 정확하게 알고 있기 때문에, 주어진 정책에 대한 **가치 함수의 추정치가 수렴할 때까지 계산이 가능**  
    ![policy_iteration_convergence][def13]  
    - Monte Carlo Methods: 환경의 모델을 모르므로, 주어진 정책에 대한 **가치 함수의 추정치가 수렴할 때까지 에이전트가 정책을 따라 무한한 개수의 에피소드를 생성**해야 함  
    ![mc_methods_convergence][def14]  
    - Monte Carlo Methods에서는 주어진 정책에 대한 가치 함수의 값을 **정확히 추정하는 것을 포기**함
    - 대신, 매 에피소드마다 주어진 정책에 대한 가치 함수 추정 및 정책 개선이 이루어짐.
    - 다행히도, GPI는 현재 주어진 정책에 대한 가치 함수의 추정치가 부정확해도 궁극적으로는 최적의 정책으로 수렴됨을 보장  

<br>

- Pseudo Code  
![mc_control_pseudo_code][def15]  

<br>

- First-Visit vs. Every-Visit
  - 한 에피소드 내에서 동일한 상태를 여러 번 만난다면 ?  
  ![first_vs_every_visit][def16]  
    - First-Visit : 주어진 상태를 맨 처음으로 방문했던 시점을 대상으로 수익을 계산하고 가치 함수를 업데이트
    - Every-Visit : 주어진 상태를 방문했던 시점마다 수익을 계산하고 가치 함수를 업데이트  
![first_vs_every_visit_pseudo_code][def17]  
  - 두 방법 모두 궁극적으로 최적의 정책을 찾을 수 있음
  - First-Visit은 추가적인 연산이 필요하므로, 주로 구현이 간단한 Every-Visit이 선호됨  

<br>

- Greedy Policy가 Optimal Policy가 될 수 있을까?
  - Policy Iteration은 모든 상태-행동에 대한 수익의 기대값을 알고 있다. 따라서 Greedy Policy가 Optimal Policy가 될 수 있음  
  - Monte Carlo Methods
    - 환경의 모델에 대한 정보가 없으므로, **경험하지 않은 상태-행동에 대해서는 수익의 기대값을 추정할 수 없음**
    - **모든 상태-행동을 경험**하여 가치를 추정해야만, Greedy Policy를 통해 정책을 개선할 수 있음  

    <br>

- ε-Greedy Policy  
![epsilon_greedy_policy][def18]  
  - ε-Greedy Policy는 `ε`확률로 최적이지 않은 행동을 수행하므로, 궁극적으로는 모든 상태-행동을 경험할 수 있음
  - 하지만, Generalized Policy Iteration을 통해 최적의 정책을 찾을 수 있는가?  
  -> Yes! ε-Greedy Policy가 Policy Improvement Theorem을 만족하면 최적의 정책을 찾을 수 있음  
  ![policy_improvement_theorem][def19]  

- ε-Greedy Policy Becomes Optimal Policy  
![epsilon_greedy_policy_optimal][def20]  

<br>

- Monte Carlo Control w/ ε-Greedy Policy  
![mc_control_epsilon_greedy_policy][def21]  

- 여기서 궁금증이 하나 생긴다.
  - 결국 최적의 정책이 될 텐데, 왜 ε-Greedy Policy를 사용하여 엄한 행동을 수행하는가?  

<br>

### 3. Off-Policy Monte-Carlo Methods  
- Dliemma of Control
  - 최적의 행동에 대한 가치 함수를 학습해야 하지만, 이를 위해서는 최적이 아닌 행동을 탐험해야 함
  - On-Policy Methods : 최적의 행동에 대한 가치 함수를 학습하는 대신, 최적에 가까운(Near-Optimal) 행동에 대한 가치 함수를 학습
  - **Off-Policy Methods** : 학습하고자 하는 정책으로 생성한 데이터(에피소드)가 아닌 별도의 데이터로 학습
    - 학습 속도가 느린 대신 더 좋은 정책을 학습할 수 있음

- Off-Policy in Monte Carlo Methods
  - 최적의 행동에 대한 가치 함수를 학습하는 정책(Target Policy)과, 최적이 아닌 행동을 탐험하며 에피소드를 만들어내는 정책(Behavior Policy)을 활용
    - Target Policy : 주로 결정적 정책(Deterministic Policy)으로, 현재 학습한 가치 함수를 바탕으로 최적의 행동을 선택
    - Behavior Policy : 주로 확률적 정책(Stochastic Policy)으로, 주어진 상태에서 모든 행동들이 임의의 확률로 선택될 수 잇음
  - Behavior Policy로 생성한 에피소드에서, Target Policy에 대한 가치 함수를 추정하고 Target Policy를 개선 (Behavior Policy는 개선하지 않음)  

  <br>

- Behavior Policy to Target Policy
  - Behavior Policy `b`가 에피소드를 생성했을 때  
  ![behavior_to_target_policy][def22]  
  - Target Policy `π`에 대한 가치 함수는?  
  ![target_policy_value_function][def23]  
  -> 에피소드는 Behavior Policy `b`에 의해 생성되었으므로, 가치 함수 또한 Behavior Policy `b`에 대한 가치 함수로 추정해야 함  
  => Behavior Policy의 가치 함수를 Target Policy의 가치 함수로 변환 필요  
  ![importance_sampling_1][def24]  
  ![importance_sampling_2][def25]  
  ![importance_sampling_3][def26]  

- Importance Sampling: Monte Carlo Prediction  
  - Ordinary Importance Sampling Estimator  
  ![ordinary_importance_sampling_estimator][def27]  
    -> 그러나 IS Ratio가 크다면, 가치 함수의 값 또한 크게 변동할 수 있음  
  - Weighted Importance Sampling Estimator  
  ![weighted_importance_sampling_estimator][def28]  
    -> IS Ratio가 크더라도 가치 함수의 값이 커지지 않으므로, 안정적으로 가치 함수의 값을 추정 가능  

- Importance Sampling: Incr. Impl.  
  - 주어진 상태를 `i`번째로 방문한 시점을 `t`<sub>`i`</sub>라고 했을 때,  
  ![importance_sampling_incr_imple_1][def29]  
  ![importance_sampling_incr_imple_2][def30]  

- Importance Sampling: Update Rule  
  - 상태 가치 함수  
  ![importance_sampling_update_rule_state_value_function][def31]  
  - 행동 가치 함수  
  ![importance_sampling_update_rule_action_value_function][def32]  

- Importance Sampling: Monte Carlo Control
  - On-Policy Monte Carlo Control
    - 탐험 : ε-Greedy Policy
    - 정책 개선 : ε-Greedy Policy  
  - Off-Policy Monte Carlo Control
    - 탐험 : (Stochastic) Behavior Policy
    - 정책 개선 : (Deterministic) Target Policy -> Greedy Policy  
    ![greedy_policy][def33]  

- Importance Sampling: Pseudo Code  
![importance_sampling_pseudo_code_1][def34]  
![importance_sampling_pseudo_code_2][def35]  

<br>

- 만약 할인율(Discount Factor) `γ`가 굉장히 작다면? (e.g., `γ ≈ 0`)  
![small_discount_factor][def36]  
-> 현재 시점에 가까운 IS Ratio가 더 중요  
=> 할인율에 따라 고려해야하는 IS Ratio(+ 가치 함수의 추정치)가 달라져야 함  

- Discounting-Aware Importance Sampling  
  - Geometric Probability Distribution  
  ![geometric_probability_distribution][def37]  
    - 어떤 시행이 확률 `p`로 성공한다고 했을 때, `k-1`번쨰 시행까지 실패하고, `k`번째 시행에서 성공할 확률을 나타내는 확률 분포  
  - **할인율 `γ`를 매 타임 스텝에서 에피소드가 조기 종료되지 않는 확률로 간주**  
  ![geometric_probability_distribution_k-1][def38]  
    - 에피소드가 확률 `1-γ`로 조기 종료된다고 했을 때, `k-1`번째 타임 스텝까지는 조기 종료되지 않고, `k`번째 타임 스텝에서 조기 종료될 확률을 나타내는 확률 분포  

- Discounting-Aware Importance Sampling  
  - `γ`<sup>`0`</sup>`(1-γ)`의 확률로 `R`<sub>`t+1`</sub>의 수익 획득(=`t+1`에서 조기 종료되는 경우)  
  - `γ`<sup>`1`</sup>`(1-γ)`의 확률로 `R`<sub>`t+1`</sub> + `R`<sub>`t+2`</sub>의 수익 획득(=`t+2`에서 조기 종료되는 경우)  
  - `γ`<sup>`2`</sup>`(1-γ)`의 확률로 `R`<sub>`t+1`</sub> + `R`<sub>`t+2`</sub> + `R`<sub>`t+3`</sub>의 수익 획득(=`t+3`에서 조기 종료되는 경우)  
  ...
  - `γ`<sup>`T-2`</sup>`(1-γ)`의 확률로 수익 `R`<sub>`1`</sub> + `R`<sub>`2`</sub> + ... + `R`<sub>`T-1`</sub>의 수익 획득(=`T-1`에서 조기 종료되는 경우)  

- 할인되지 않은 수익(Flat Partial Return)과 일반적인 수익(Return)의 관계  
![flat_partial_return_vs_return][def39]  

- Discounting-Aware Importance Sampling  
  - Ordinary Importance Sampling Estimator  
  ![discounting_aware_ordinary_importance_sampling_estimator][def40]  
  - Weighted Importance Sampling Estimator  
  ![discounting_aware_weighted_importance_sampling_estimator][def41]  

<br>

- Per-Decision Importance Sampling  
  - 기본적인 Importance Sampling에서는 각 시점의 보상에 대해서 종료 시점까지 고려한 IS Ratio가 곱해짐  
  ![basic_importance_sampling][def42]  
    - 직관적으로 생각해보면 각 시점의 보상에 대해서는 해당 시점까지만 고려한 IS Ratio가 곱해져야 함  
  - Ordinary Importance Sampling Estimator  
  ![per_decision_ordinary_importance_sampling_estimator][def43]  
  - Weighted Importance Sampling Estimator  
    - 명확히 제안된 바 없음  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/Gxirqhv.png
[def2]: https://i.imgur.com/mTT5d22.png
[def3]: https://i.imgur.com/hSD7irj.png
[def4]: https://i.imgur.com/UknQmOw.png
[def5]: https://i.imgur.com/GjMotad.png
[def6]: https://i.imgur.com/b25g9mr.png
[def7]: https://i.imgur.com/yHGcHZ8.png
[def8]: https://i.imgur.com/4VXn70L.png
[def9]: https://i.imgur.com/pM5ghsy.png
[def10]: https://i.imgur.com/UGMziyW.png
[def11]: https://i.imgur.com/nwXgS3V.png
[def12]: https://i.imgur.com/qX3A31a.png
[def13]: https://i.imgur.com/pglw9li.png
[def14]: https://i.imgur.com/7cNW27Y.png
[def15]: https://i.imgur.com/vU6x2oE.png
[def16]: https://i.imgur.com/vq5GBmt.png
[def17]: https://i.imgur.com/SCgPRZa.png
[def18]: https://i.imgur.com/ZfnzRjZ.png
[def19]: https://i.imgur.com/SjDlZgt.png
[def20]: https://i.imgur.com/paRvPdz.png
[def21]: https://i.imgur.com/yezgnQQ.png
[def22]: https://i.imgur.com/BdmyeKX.png
[def23]: https://i.imgur.com/8mgVjfk.png
[def24]: https://i.imgur.com/9LVHmKH.png
[def25]: https://i.imgur.com/uVGGCyF.png
[def26]: https://i.imgur.com/KoylS3U.png
[def27]: https://i.imgur.com/CqiG6SG.png
[def28]: https://i.imgur.com/CtK2ejP.png
[def29]: https://i.imgur.com/HAp2odg.png
[def30]: https://i.imgur.com/cAycI2C.png
[def31]: https://i.imgur.com/YKjamzW.png
[def32]: https://i.imgur.com/kG6g0Bs.png
[def33]: https://i.imgur.com/qzaWzB7.png
[def34]: https://i.imgur.com/0zfg6h1.png
[def35]: https://i.imgur.com/NokRWpl.png
[def36]: https://i.imgur.com/hAjPtvV.png
[def37]: https://i.imgur.com/knFRyiB.png
[def38]: https://i.imgur.com/EdltnE4.png
[def39]: https://i.imgur.com/igsKOtr.png
[def40]: https://i.imgur.com/dxZeewM.png
[def41]: https://i.imgur.com/Pf40vxw.png
[def42]: https://i.imgur.com/n8VfxYo.png
[def43]: https://i.imgur.com/B38Rz7t.png