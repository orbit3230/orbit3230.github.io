---
layout: post
title: "[인공지능] 4주차 (2) - Monte-Carlo Methods"
excerpt: "Dynamic Programming vs. Reinforcement Learning, Monte-Carlo Methods, Off-Policy Monte-Carlo Methods"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-25
last_modified_at: 2025-09-25
---
## Monte-Carlo Methods
### 1. Dynamic Programming vs. Reinforcement Learning
- Dynamic Programming : Model-based  
`P(s', r | s, a) = p`<sub>`*`</sub>`(s', r | s, a)`  
  - 환경의 모델(= 현재 상태에서 행동을 수행했을 때, 다음 상태 및 보상을 받을 확률)을 알고 있음
  - 환경에 대한 완벽한 지식을 바탕으로 최적의 정책을 계획  

- Reinforcement Learning : Model-free  
`P(s', r | s, a) ≈ p`<sub>`*`</sub>`(s', r | s, a)`  
  - 환경의 모델을 모름
  - 환경에 대한 불완전한 지식을 바탕으로 최적의 정책을 학습

<br>

- Backup Diagram  
![backup_diagram](TODO)  
  - 마르코프 결정 프로세스에 대한 상태-행동-보상의 에피소드를 시각적으로 표현
  - 상태는 백색 원, 행동은 흑색 원으로 표현
  - 상태-행동 간에는 정책, 행동-상태 간에는 보상을 표현
  - Backup : 현재 상태에서 전이할 수 있는 다음 상태를 의미  

- Full Backup  
![full_backup](TODO)  
  - Dynamic Programming에서 사용
  - **환경의 모델을 바탕으로 모든 다음 상태와 보상**을 고려하여 현재 상태에 대한 가치 함수를 업데이트
  - 모든 상태에 대해 가치 함수의 값을 계산하므로, 상태의 개수가 많다며녀 많은 연산이 필요  

- Sample Backup  
![sample_backup](TODO)  
  - Reinforcement Learning에서 사용
  - 환경의 모델을 모르므로, 에이전트가 환경과 상호작용하여 **실제로 얻은 수익 (또는 추정치)**를 활용하여 가치 함수를 업데이트
  - 에피소드 내에서 경험한 상태에 대해서만 가치 함수의 값을 계산하므로, 연산량은 상태의 개수와는 독립적

<br>

### 2. Monte-Carlo Methods
- 반복 무작위 추출(Repeated Random Sampling)을 수행하여 문제를 해결하는 방법론
  - Example  
  ![monte_carlo_example](TODO)  
    - (1) 주어진 입력 범위 (x = `[0, 1]`, y = `[0, 1]`) 내에서 반복 무작위 추출을 통해 점들을 생성
    - (2) 생성된 점들이 사분원호 내에 존재하는지 판단
    - (3) 사분원호 내에 존재하는 점들의 개수가 사분원 호의 넓이의 추정치
    - (4) 추출된 점들이 많을수록 사분원호의 참인 넓이, `π/4`에 가까워짐  

<br>

- Monte Carlo Methods for RL
  - 한 에피소드가 종료된 후, 에이전트가 실제로 경험한 행동, 상태, 보상으로 가치 함수를 추정하고 정책을 개선
  - 에피소드를 수 없이 반복하면? -> Generalized Policy Iteration (GPI)  
  - Policy Iteration과 마찬가지로, Monte Carlo Methods 또한 Policy Evaluation(**Monte Carlo Prediction**)과 Policy Improvement(**Monte Carlo Control**)를 반복하여 최적의 정책을 학습  

  <br>

- Monte Carlo Prediction
  - Episodes  
  ![mc_prediction_episodes](TODO)  
    - 강화학습 에이전트가 주어진 정책에 기반하여 시작 시점 ~ 종료 시점까지 환경과 상호작용한 내역
    - 시간에 따라 발생한 상태, 행동, 보상을 나타내는 확률 변수들의 순차적 모임

  - Return  
  ![mc_prediction_return](TODO)  
    - 만약, 한 에피소드에서 에이전트가 주어진 정책을 따라 환경과 상호작용한 내역을 알고 있다면, 특정 시점의 상태에서 해당 에피소드에 한해 얻을 수 있는 수익을 알 수 있음

  - Value Function  
  ![mc_prediction_value_function](TODO)
    - 만약, 무한히 많은 에피소드에서 에이전트가 주어진 정책을 따라 환경과 상호작용한 내역을 알고 있다면, 특정 시점의 상태에서 일반적으로 얻을 수 있는 수익의 기대값(= 상태 가치 함수)를 추정할 수 있음

  - Incr. Imple.  
    - Monte Carlo Methods의 수익의 기대값은 Incremental Implementation으로 계산 가능  
    ![mc_prediction_incr_imple](TODO)  
    - 또한, 서로 다른 에피소드에서 환경의 모델이 변할 수 있다고 가정한다면(Non-Stationary), 평균 대신 스텝 사이즈 `α`를 활용  
    ![mc_prediction_incr_imple_non_stationary](TODO)  

  - Update Rule  
  ![mc_prediction_update_rule](TODO)  

<br>

- Monte Carlo Control -> 환경의 모델 `P(s', r | s, a)`를 모르므로, 행동 가치 함수를 계산할 수 없음! 따라서 추정.  
  - Action-Value Function
    - 상태 가치 함수를 활용해 더 나은 정책으로 개선하기 위해서는 **환경의 모델에 대한 지식**이 필요
    - 강화 학습은 환경의 모델에 대한 지식이 없으므로, 상태 가치 함수로부터 행동 가치 함수를 계산하는 대신 직접 **행동 가치 함수**를 추정  
    ![mc_control_action_value_function](TODO)  
    - 이후 Policy Improvement와 동일하게, 현재 정책에서 추정된 행동 가치 함수의 값이 최대가 되는 행동을 선택하는 더 나은 정책을 생성  
    ![mc_control_policy_improvement](TODO)  

  - Convergence Issues
    - Policy Iteration: 환경의 모델을 정확하게 알고 있기 때문에, 주어진 정책에 대한 **가치 함수의 추정치가 수렴할 때까지 계산이 가능**  
    ![policy_iteration_convergence](TODO)  
    - Monte Carlo Methods: 환경의 모델을 모르므로, 주어진 정책에 대한 **가치 함수의 추정치가 수렴할 때까지 에이전트가 정책을 따라 무한한 개수의 에피소드를 생성**해야 함  
    ![mc_methods_convergence](TODO)  
    - Monte Carlo Methods에서는 주어진 정책에 대한 가치 함수의 값을 **정확히 추정하는 것을 포기**함
    - 대신, 매 에피소드마다 주어진 정책에 대한 가치 함수 추정 및 정책 개선이 이루어짐.
    - 다행히도, GPI는 현재 주어진 정책에 대한 가치 함수의 추정치가 부정확해도 궁극적으로는 최적의 정책으로 수렴됨을 보장  

<br>

- Pseudo Code  
![mc_control_pseudo_code](TODO)  

<br>

- First-Visit vs. Every-Visit
  - 한 에피소드 내에서 동일한 상태를 여러 번 만난다면 ?  
  ![first_vs_every_visit](TODO)  
    - First-Visit : 주어진 상태를 맨 처음으로 방문했던 시점을 대상으로 수익을 계산하고 가치 함수를 업데이트
    - Every-Visit : 주어진 상태를 방문했던 시점마다 수익을 계산하고 가치 함수를 업데이트  
![first_vs_every_visit_pseudo_code](TODO)  
  - 두 방법 모두 궁극적으로 최적의 정책을 찾을 수 있음
  - First-Visit은 추가적인 연산이 필요하므로, 주로 구현이 간단한 Every-Visit이 선호됨  

<br>

- Greedy Policy가 Optimal Policy가 될 수 있을까?
  - Policy Iteration은 모든 상태-행동에 대한 수익의 기대값을 알고 있다. 따라서 Greedy Policy가 Optimal Policy가 될 수 있음  
  - Monte Carlo Methods
    - 환경의 모델에 대한 정보가 없으므로, **경험하지 않은 상태-행동에 대해서는 수익의 기대값을 추정할 수 없음**
    - **모든 상태-행동을 경험**하여 가치를 추정해야만, Greedy Policy를 통해 정책을 개선할 수 있음  

    <br>

- ε-Greedy Policy  
![epsilon_greedy_policy](TODO)  
  - ε-Greedy Policy는 `ε`확률로 최적이지 않은 행동을 수행하므로, 궁극적으로는 모든 상태-행동을 경험할 수 있음
  - 하지만, Generalized Policy Iteration을 통해 최적의 정책을 찾을 수 있는가?  
  -> Yes! ε-Greedy Policy가 Policy Improvement Theorem을 만족하면 최적의 정책을 찾을 수 있음  
  ![policy_improvement_theorem](TODO)  

- ε-Greedy Policy Becomes Optimal Policy  
![epsilon_greedy_policy_optimal](TODO)  

<br>

- Monte Carlo Control w/ ε-Greedy Policy  
![mc_control_epsilon_greedy_policy](TODO)  

- 여기서 궁금증이 하나 생긴다.
  - 결국 최적의 정책이 될 텐데, 왜 ε-Greedy Policy를 사용하여 엄한 행동을 수행하는가?  

<br>

### 3. Off-Policy Monte-Carlo Methods  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 