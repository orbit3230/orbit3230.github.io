---
layout: post
title: "[인공지능] 13주차 (2) - Advanced Topics (1)"
excerpt: "Distributed Reinforcement Learning, Off-Policy Policy Gradient Methods"

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-27
last_modified_at: 2025-11-27
---
## Advanced Topics
### 1. Distributed Reinforcement Learning
- 여러 개의 강화학습 에이전트를 독립적/병렬적으로 환경과 상호작용 시켜서 최적의 정책을 찾는 방법  
![distributed_RL][def]  

- Centralized Architecture
  - 다수의 작업자 에이전트(Worker Agent)가 병렬적으로 환경과 상호작용하여 경험 등을 생성
  - 작업자 에이전트가 생성한 경험 등으로 전역 에이전트의 매개변수를 업데이트하여 최적의 정책을 학습
  - e.g.,
    - Asynchronous Advantage Actor-Critic (A3C)
    - General Reinforcement Learning Architecture (Gorila)
    - Recurrent Experience Replay in Distributed Reinforcement Learning (R2D2)

- Decentralized Architecture
  - 전역 에이전트 없이, 각기 다른 강화학습 에이전트가 경험 등을 서로 공유하며 최적의 정책을 학습
  - e.g.,
    - Importance Weighted Actor-Learner Architecture (IMPALA)
    - rlpyt
    - Decentralized Distributed Proximal Policy Optimization (DD-PPO)

<br>

#### Asynchronous Advantage Actor-Critic (A3C)
- 중앙 집중식 구조를 가진 분산 강화 학습 기법
- 각 작업자 에이전트가 병렬적으로 환경과 상호작용하여 Actor-Critic Methods의 정책/손실 기울기를 계산하고, 이를 전역 에이전트를 훈련하는 데 활용

- Asynchronous "Advantage" Actor-Critic
  - 행동의 상대적인 중요도를 측정하는 함수  
  ![advantage_function][def2]  

  - (Recap) Valid Gradients  
  ![valid_gradients][def3]  

  - (Cont'd) 행동의 상대적인 중요도를 측정하는 함수  
  ![advantage_function_contd][def4]  
  -> (최대) n-step Return을 Target으로 활용  
  ![n_step_return][def5]  

<br>

- "Asynchronous" Advantage Actor-Critic
  - (1) Global Agent가 가진 초기 매개변수를 `K`개의 Worker Agent들에게 복제  
  ![a3c_step1][def6]  

  - (2) 각 Worker Agent들은 독립적으로 환경과 `n`회 상호작용하면서 정책/손실 기울기를 누적  
  ![a3c_step2_1][def7]  
  ![a3c_step2_2][def8]  

  - (3) 상호작용이 끝난 Worker Agent는 누적 정책/손실 기울기를 Global Agent에게 전달하여 Global Agent의 매개변수를 업데이트  
  ![a3c_step3_1][def9]  
  ![a3c_step3_2][def10]  

  - (4) Global Agent의 매개변수를 상호작용이 끝난 Worker Agent에 복제하고, 다시 환경과 상호작용을 재개  
  ![a3c_step4][def11]  

  - (5) 또 다른 Worker Agent 또한 일정 횟수동안 상호작용이 끝난 후, 누적 정책/손실 기울기를 Global Agent에 전달하여 Global Agent의 매개변수를 업데이트  
  ![a3c_step5][def12]  

  - (6) Global Agent의 매개변수를 상호작용이 끝난 Worker Agent에 복제하고, 다시 환경과 상호작용을 수행  
  ![a3c_step6][def13]  

<br>

- Pseudocode  
![a3c_pseudocode][def14]  

<br>

- Effect : 각 Worker Agent가 가지고 있는 매개변수의 값이 다름
  - 다양한 경험의 생성 -> 탐험의 강화
  - 경험들 간의 시간적 의존성 완화 -> 매개변수의 발산 완화

- Side-Effect : 각 Worker Agent가 가지고 있는 매개변수의 값이 Global Agent와 다름
  - Global Agent가 최적이 아닌 정책을 찾거나, 학습 과정이 불안정하고 매개변수의 수렴이 느릴 수 있음
  - 이를 완화하는 방법이 Synchronous Advantage Actor-Critic (A2C)

<br>

### 2. Off-Policy Policy Gradient Methods
- Off-Policy Actor-Critic
  - Behavior Policy `b`가 생성한 에피소드로부터 Target Policy `π`(= Actor)에 대한 정책 기울기를 계산하여 업데이트  
  - 상태 가치 함수를 추정하는 Critic을 사용

<br>

- Behavior Policy Grad. to Target Policy Grad.  
![off_policy_policy_gradient_1][def15]  
![off_policy_policy_gradient_2][def16]  

<br>

- Update Rule  
  - Actor  
  ![off_policy_actor_update][def17]  

  - Critic  
  ![off_policy_critic_update][def18]  

<br>

- Pseudocode  
![off_policy_actor_critic_pseudocode][def19]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/BaGkBTF.png
[def2]: https://i.imgur.com/4XUqMmD.png
[def3]: https://i.imgur.com/faLkcNP.png
[def4]: https://i.imgur.com/BiDSFWS.png
[def5]: https://i.imgur.com/S6jKLJw.png
[def6]: https://i.imgur.com/G52K9QT.png
[def7]: https://i.imgur.com/qTLrms1.png
[def8]: https://i.imgur.com/99w5MYd.png
[def9]: https://i.imgur.com/jWiuvmS.png
[def10]: https://i.imgur.com/L95S1Qb.png
[def11]: https://i.imgur.com/FvnjMTI.png
[def12]: https://i.imgur.com/NKmEfjT.png
[def13]: https://i.imgur.com/PnJqF2y.png
[def14]: https://i.imgur.com/ry4hmbd.png
[def15]: https://i.imgur.com/685kHuO.png
[def16]: https://i.imgur.com/3XEZuSz.png
[def17]: https://i.imgur.com/EIfYSmI.png
[def18]: https://i.imgur.com/xRseZBn.png
[def19]: https://i.imgur.com/4FASf9u.png