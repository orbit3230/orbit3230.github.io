---
layout: post
title: "[인공지능] 13주차 (2) - Advanced Topics (1)"
excerpt: "Distributed Reinforcement Learning, Off-Policy Policy Gradient Methods, Deterministic Policy Gradient Methods"

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-27
last_modified_at: 2025-12-01
---
## Advanced Topics
### 1. Distributed Reinforcement Learning
- 여러 개의 강화학습 에이전트를 독립적/병렬적으로 환경과 상호작용 시켜서 최적의 정책을 찾는 방법  
![distributed_RL][def]  

- Centralized Architecture
  - 다수의 작업자 에이전트(Worker Agent)가 병렬적으로 환경과 상호작용하여 경험 등을 생성
  - 작업자 에이전트가 생성한 경험 등으로 전역 에이전트의 매개변수를 업데이트하여 최적의 정책을 학습
  - e.g.,
    - Asynchronous Advantage Actor-Critic (A3C)
    - General Reinforcement Learning Architecture (Gorila)
    - Recurrent Experience Replay in Distributed Reinforcement Learning (R2D2)

- Decentralized Architecture
  - 전역 에이전트 없이, 각기 다른 강화학습 에이전트가 경험 등을 서로 공유하며 최적의 정책을 학습
  - e.g.,
    - Importance Weighted Actor-Learner Architecture (IMPALA)
    - rlpyt
    - Decentralized Distributed Proximal Policy Optimization (DD-PPO)

<br>

#### Asynchronous Advantage Actor-Critic (A3C)
- 중앙 집중식 구조를 가진 분산 강화 학습 기법
- 각 작업자 에이전트가 병렬적으로 환경과 상호작용하여 Actor-Critic Methods의 정책/손실 기울기를 계산하고, 이를 전역 에이전트를 훈련하는 데 활용

- Asynchronous "Advantage" Actor-Critic
  - 행동의 상대적인 중요도를 측정하는 함수  
  ![advantage_function][def2]  

  - (Recap) Valid Gradients  
  ![valid_gradients][def3]  

  - (Cont'd) 행동의 상대적인 중요도를 측정하는 함수  
  ![advantage_function_contd][def4]  
  -> (최대) n-step Return을 Target으로 활용  
  ![n_step_return][def5]  

<br>

- "Asynchronous" Advantage Actor-Critic
  - (1) Global Agent가 가진 초기 매개변수를 `K`개의 Worker Agent들에게 복제  
  ![a3c_step1][def6]  

  - (2) 각 Worker Agent들은 독립적으로 환경과 `n`회 상호작용하면서 정책/손실 기울기를 누적  
  ![a3c_step2_1][def7]  
  ![a3c_step2_2][def8]  

  - (3) 상호작용이 끝난 Worker Agent는 누적 정책/손실 기울기를 Global Agent에게 전달하여 Global Agent의 매개변수를 업데이트  
  ![a3c_step3_1][def9]  
  ![a3c_step3_2][def10]  

  - (4) Global Agent의 매개변수를 상호작용이 끝난 Worker Agent에 복제하고, 다시 환경과 상호작용을 재개  
  ![a3c_step4][def11]  

  - (5) 또 다른 Worker Agent 또한 일정 횟수동안 상호작용이 끝난 후, 누적 정책/손실 기울기를 Global Agent에 전달하여 Global Agent의 매개변수를 업데이트  
  ![a3c_step5][def12]  

  - (6) Global Agent의 매개변수를 상호작용이 끝난 Worker Agent에 복제하고, 다시 환경과 상호작용을 수행  
  ![a3c_step6][def13]  

<br>

- Pseudocode  
![a3c_pseudocode][def14]  

<br>

- Effect : 각 Worker Agent가 가지고 있는 매개변수의 값이 다름
  - 다양한 경험의 생성 -> 탐험의 강화
  - 경험들 간의 시간적 의존성 완화 -> 매개변수의 발산 완화

- Side-Effect : 각 Worker Agent가 가지고 있는 매개변수의 값이 Global Agent와 다름
  - Global Agent가 최적이 아닌 정책을 찾거나, 학습 과정이 불안정하고 매개변수의 수렴이 느릴 수 있음
  - 이를 완화하는 방법이 Synchronous Advantage Actor-Critic (A2C)

<br>

### 2. Off-Policy Policy Gradient Methods
- Off-Policy Actor-Critic
  - Behavior Policy `b`가 생성한 에피소드로부터 Target Policy `π`(= Actor)에 대한 정책 기울기를 계산하여 업데이트  
  - 상태 가치 함수를 추정하는 Critic을 사용

<br>

- Behavior Policy Grad. to Target Policy Grad.  
![off_policy_policy_gradient_1][def15]  
![off_policy_policy_gradient_2][def16]  

<br>

- Update Rule  
  - Actor  
  ![off_policy_actor_update][def17]  

  - Critic  
  ![off_policy_critic_update][def18]  

<br>

- Pseudocode  
![off_policy_actor_critic_pseudocode][def19]  

<br>

### 3. Deterministic Policy Gradient Methods  
- 연속적인 행동 공간에서 최적의 행동을 수행하는 "결정적 정책"을 학습
  - 주어진 상태에 대한 행동을 직접 출력  
  ![deterministic_policy](TODO)  

- 파라미터를 어떻게 학습하는가?  
  - (1) 매개변수 `θ`에 대해, 시작 상태에서 종료 상태까지 얻을 수 있는 (할인된) 보상의 총 합(= 수익)을 측정하는 목적 함수 `J(θ)` 정의  
  ![deterministic_policy_objective](TODO)  

  - (2) 목적 함수의 값을 최대화(= 보상의 총 합을 최대화) 하기 위해, 경사 상승법으로 정책 기울기를 현재 매개변수 `θ`<sup>`(k)`</sup>에 더하여 업데이트  

- Deterministic Policy Gradient를 계산하는 방법  
![deterministic_policy_gradient_1](TODO)  
![deterministic_policy_gradient_2](TODO)  
![deterministic_policy_gradient_3](TODO)  
![deterministic_policy_gradient_4](TODO)  
![deterministic_policy_gradient_5](TODO)  
![deterministic_policy_gradient_6](TODO)  
![deterministic_policy_gradient_7](TODO)  

<br>

- DPG -> Actor-Critic 구조로 확장  
![deterministic_policy_gradient_actor_critic](TODO)  
  - Actor  
  ![deterministic_policy_actor_update](TODO)  

  - Critic  
  ![deterministic_policy_critic_update](TODO)  

<br>

- 과연 결정적 정책이 확률적 정책보다 좋은가?  
  - ε-Greedy  
  ![epsilon_greedy](TODO)  
    - 정책을 학습하는 중에는 행동 가치 함수의 값이 가장 큰 행동을 무조건 선택하는 것이 아니라, `ε`의 확률로 무작위 행동을 선택
    - 현재 최적이라 여겨지지 않는 행동도 탐험  

  - (Stochastic) Policy Gradient for Continuous Action Spaces  
  ![stochastic_policy_gradient_continuous_action](TODO)  
    - 행동의 표준 편차가 크다면(= 목적 함수를 최대화하기 위한 행동이 아직 명확하게 정해지지 않았다면) 다양한 행동을 선택할 수 있음
    - 현재 최적이라 여겨지지 않는 행동(= 행동의 평균에서 먼 행동도 탐험)도 탐험  

  - Deterministic Policy Gradient  
  ![deterministic_policy_gradient_exploration](TODO)  
    - 정책을 학습하는 중에는 현재 최적이라 여겨지는 행동(= 목적 함수의 값을 최대화)을 선택
    - 탐험을 보장할 수 없으므로 결과적으로 최적의 정책을 찾을 수 없음

<br>

- Off-Policy Deterministic Policy Gradient  
  - Stochastic Behavior Policy `b`가 생성한 에피소드로부터 Deterministic Target Policy `π`에 대한 Policy Gradient를 계산하여 업데이트
  - Off-Policy Deterministic Policy Gradient Theorem  
  ![off_policy_deterministic_policy_gradient_theorem](TODO)  

- Off-Policy DPG Update Rule  
  - Actor  
  ![off_policy_dpg_actor_update](TODO)  

  - Critic  
  ![off_policy_dpg_critic_update](TODO)  

<br>

- Deterministic Policy with Added Noise  
  - 결정적 정책의 행동에 정규 분포에서 추출된 노이즈를 추가하여 탐험을 보장
  - Off-Policy DPG보다 단순하고 학습 속도가 빨라서 선호됨  
  ![deterministic_policy_with_noise](TODO)  

<br>

- Deep Deterministic Policy Gradient (DDPG)  
  - DPG + Experience Replay + Fixed Target Network + Polyak 
  
- DDPG: Experience Replay
  - 새로운 경험을 관측할 때마다 최근 `N`개의 경험만을 유지하는 Replay Buffer `D`에 저장하고, `N`개를 초과할 시 오래된 경험부터 삭제  
  ![ddpg_experience_replay](TODO)  

  - 매 `k`회 환경과 상호작용할 때마다, Replay Buffer `D`에서 무작위로 추출된 `B`개의 경험들로 매개변수를 업데이트  
  ![ddpg_experience_replay_update](TODO)  

- DDPG: Fixed Target Network
  - Critic의 업데이트에는 Target Actor/Critic Network를 활용  
  ![ddpg_fixed_target_network](TODO)  

- DDPG: Polyak Averaging
  - DQN : 일정 횟수 상호작용할 때마다 Behavior Network의 매개변수를 Target Network에 복제  

  - DDPG : Behavior Network가 업데이트 될 때마다 Polyak Averaging으로 Target Network를 업데이트  
  ![ddpg_polyak_averaging](TODO)  

- DDPG: Pseudocode  
  ![ddpg_pseudocode](TODO)  

<br>

- Twin Delayed DDPG (TD3)  
  - DDPG + Double Q-Learning + Target Policy Smoothing + Delayed Policy & Target Updates  

- TD3: Actor and Critic Networks  
![td3_actor_critic_networks](TODO)  

- TD3: Clipped Double Q-Learning  
![td3_clipped_double_q_learning](TODO)  
  - 행동 가치 함수를 추정하는 2개의 Behavior Critic 및 Target Critic을 활용
  - Behavior Critic의 업데이트에는 Target Critic이 추정한 행동 가치 함수의 값 중 작은 값을 Target으로 활용하여 행동 가치 함수가 과대평가되는 것을 방지  

- TD3: Target Policy Smoothing  
![td3_target_policy_smoothing](TODO)  
  - Target Actor Network의 출력 + [-c, c] 사이의 무작위 노이즈를 추가한 행동으로 Critic Target을 계산하여 너무 이르게 특정한 행동을 고평가하는 것을 방지  

- TD3: Delayed Policy & Target Updates  
![td3_delayed_policy_target_updates](TODO)  
  - Critic이 `P`회 이상 업데이트 된 후에 Behavior Actor는 경사 상승법으로, Target Actor/Critic은 Polyak Averaging으로 업데이트  

- TD3: Pseudocode  
![td3_pseudocode](TODO)  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/BaGkBTF.png
[def2]: https://i.imgur.com/4XUqMmD.png
[def3]: https://i.imgur.com/faLkcNP.png
[def4]: https://i.imgur.com/BiDSFWS.png
[def5]: https://i.imgur.com/S6jKLJw.png
[def6]: https://i.imgur.com/G52K9QT.png
[def7]: https://i.imgur.com/qTLrms1.png
[def8]: https://i.imgur.com/99w5MYd.png
[def9]: https://i.imgur.com/jWiuvmS.png
[def10]: https://i.imgur.com/L95S1Qb.png
[def11]: https://i.imgur.com/FvnjMTI.png
[def12]: https://i.imgur.com/NKmEfjT.png
[def13]: https://i.imgur.com/PnJqF2y.png
[def14]: https://i.imgur.com/ry4hmbd.png
[def15]: https://i.imgur.com/685kHuO.png
[def16]: https://i.imgur.com/3XEZuSz.png
[def17]: https://i.imgur.com/EIfYSmI.png
[def18]: https://i.imgur.com/xRseZBn.png
[def19]: https://i.imgur.com/4FASf9u.png