---
layout: post
title: "[인공지능] 13주차 (2) - Advanced Topics (1)"
excerpt: "Distributed Reinforcement Learning, Off-Policy Policy Gradient Methods, Deterministic Policy Gradient Methods, Entropy Regularization Method, Trust Region Constraint Method"

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-27
last_modified_at: 2025-12-04
---
## Advanced Topics
### 1. Distributed Reinforcement Learning
- 여러 개의 강화학습 에이전트를 독립적/병렬적으로 환경과 상호작용 시켜서 최적의 정책을 찾는 방법  
![distributed_RL][def]  

- Centralized Architecture
  - 다수의 작업자 에이전트(Worker Agent)가 병렬적으로 환경과 상호작용하여 경험 등을 생성
  - 작업자 에이전트가 생성한 경험 등으로 전역 에이전트의 매개변수를 업데이트하여 최적의 정책을 학습
  - e.g.,
    - Asynchronous Advantage Actor-Critic (A3C)
    - General Reinforcement Learning Architecture (Gorila)
    - Recurrent Experience Replay in Distributed Reinforcement Learning (R2D2)

- Decentralized Architecture
  - 전역 에이전트 없이, 각기 다른 강화학습 에이전트가 경험 등을 서로 공유하며 최적의 정책을 학습
  - e.g.,
    - Importance Weighted Actor-Learner Architecture (IMPALA)
    - rlpyt
    - Decentralized Distributed Proximal Policy Optimization (DD-PPO)

<br>

#### Asynchronous Advantage Actor-Critic (A3C)
- 중앙 집중식 구조를 가진 분산 강화 학습 기법
- 각 작업자 에이전트가 병렬적으로 환경과 상호작용하여 Actor-Critic Methods의 정책/손실 기울기를 계산하고, 이를 전역 에이전트를 훈련하는 데 활용

- Asynchronous "Advantage" Actor-Critic
  - 행동의 상대적인 중요도를 측정하는 함수  
  ![advantage_function][def2]  

  - (Recap) Valid Gradients  
  ![valid_gradients][def3]  

  - (Cont'd) 행동의 상대적인 중요도를 측정하는 함수  
  ![advantage_function_contd][def4]  
  -> (최대) n-step Return을 Target으로 활용  
  ![n_step_return][def5]  

<br>

- "Asynchronous" Advantage Actor-Critic
  - (1) Global Agent가 가진 초기 매개변수를 `K`개의 Worker Agent들에게 복제  
  ![a3c_step1][def6]  

  - (2) 각 Worker Agent들은 독립적으로 환경과 `n`회 상호작용하면서 정책/손실 기울기를 누적  
  ![a3c_step2_1][def7]  
  ![a3c_step2_2][def8]  

  - (3) 상호작용이 끝난 Worker Agent는 누적 정책/손실 기울기를 Global Agent에게 전달하여 Global Agent의 매개변수를 업데이트  
  ![a3c_step3_1][def9]  
  ![a3c_step3_2][def10]  

  - (4) Global Agent의 매개변수를 상호작용이 끝난 Worker Agent에 복제하고, 다시 환경과 상호작용을 재개  
  ![a3c_step4][def11]  

  - (5) 또 다른 Worker Agent 또한 일정 횟수동안 상호작용이 끝난 후, 누적 정책/손실 기울기를 Global Agent에 전달하여 Global Agent의 매개변수를 업데이트  
  ![a3c_step5][def12]  

  - (6) Global Agent의 매개변수를 상호작용이 끝난 Worker Agent에 복제하고, 다시 환경과 상호작용을 수행  
  ![a3c_step6][def13]  

<br>

- Pseudocode  
![a3c_pseudocode][def14]  

<br>

- Effect : 각 Worker Agent가 가지고 있는 매개변수의 값이 다름
  - 다양한 경험의 생성 -> 탐험의 강화
  - 경험들 간의 시간적 의존성 완화 -> 매개변수의 발산 완화

- Side-Effect : 각 Worker Agent가 가지고 있는 매개변수의 값이 Global Agent와 다름
  - Global Agent가 최적이 아닌 정책을 찾거나, 학습 과정이 불안정하고 매개변수의 수렴이 느릴 수 있음
  - 이를 완화하는 방법이 Synchronous Advantage Actor-Critic (A2C)

<br>

### 2. Off-Policy Policy Gradient Methods
- Off-Policy Actor-Critic
  - Behavior Policy `b`가 생성한 에피소드로부터 Target Policy `π`(= Actor)에 대한 정책 기울기를 계산하여 업데이트  
  - 상태 가치 함수를 추정하는 Critic을 사용

<br>

- Behavior Policy Grad. to Target Policy Grad.  
![off_policy_policy_gradient_1][def15]  
![off_policy_policy_gradient_2][def16]  

<br>

- Update Rule  
  - Actor  
  ![off_policy_actor_update][def17]  

  - Critic  
  ![off_policy_critic_update][def18]  

<br>

- Pseudocode  
![off_policy_actor_critic_pseudocode][def19]  

<br>

### 3. Deterministic Policy Gradient Methods  
- 연속적인 행동 공간에서 최적의 행동을 수행하는 "결정적 정책"을 학습
  - 주어진 상태에 대한 행동을 직접 출력  
  ![deterministic_policy][def20]  

- 파라미터를 어떻게 학습하는가?  
  - (1) 매개변수 `θ`에 대해, 시작 상태에서 종료 상태까지 얻을 수 있는 (할인된) 보상의 총 합(= 수익)을 측정하는 목적 함수 `J(θ)` 정의  
  ![deterministic_policy_objective][def21]  

  - (2) 목적 함수의 값을 최대화(= 보상의 총 합을 최대화) 하기 위해, 경사 상승법으로 정책 기울기를 현재 매개변수 `θ`<sup>`(k)`</sup>에 더하여 업데이트  
  ![deterministic_policy_update][def22]  

- Deterministic Policy Gradient를 계산하는 방법  
![deterministic_policy_gradient_1][def23]  
![deterministic_policy_gradient_2][def24]  
![deterministic_policy_gradient_3][def25]  
![deterministic_policy_gradient_4][def26]  
![deterministic_policy_gradient_5][def27]  
![deterministic_policy_gradient_6][def28]  
![deterministic_policy_gradient_7][def29]  

<br>

- DPG -> Actor-Critic 구조로 확장  
![deterministic_policy_gradient_actor_critic][def30]  
  - Actor  
  ![deterministic_policy_actor_update][def31]  

  - Critic  
  ![deterministic_policy_critic_update][def32]  

<br>

- 과연 결정적 정책이 확률적 정책보다 좋은가?  
  - ε-Greedy  
  ![epsilon_greedy][def33]  
    - 정책을 학습하는 중에는 행동 가치 함수의 값이 가장 큰 행동을 무조건 선택하는 것이 아니라, `ε`의 확률로 무작위 행동을 선택
    - 현재 최적이라 여겨지지 않는 행동도 탐험  

  - (Stochastic) Policy Gradient for Continuous Action Spaces  
  ![stochastic_policy_gradient_continuous_action][def34]  
    - 행동의 표준 편차가 크다면(= 목적 함수를 최대화하기 위한 행동이 아직 명확하게 정해지지 않았다면) 다양한 행동을 선택할 수 있음
    - 현재 최적이라 여겨지지 않는 행동(= 행동의 평균에서 먼 행동도 탐험)도 탐험  

  - Deterministic Policy Gradient  
  ![deterministic_policy_gradient_exploration][def35]  
    - 정책을 학습하는 중에는 현재 최적이라 여겨지는 행동(= 목적 함수의 값을 최대화)을 선택
    - 탐험을 보장할 수 없으므로 결과적으로 최적의 정책을 찾을 수 없음

<br>

- Off-Policy Deterministic Policy Gradient  
  - Stochastic Behavior Policy `b`가 생성한 에피소드로부터 Deterministic Target Policy `π`에 대한 Policy Gradient를 계산하여 업데이트
  - Off-Policy Deterministic Policy Gradient Theorem  
  ![off_policy_deterministic_policy_gradient_theorem][def36]  

- Off-Policy DPG Update Rule  
  - Actor  
  ![off_policy_dpg_actor_update][def37]  

  - Critic  
  ![off_policy_dpg_critic_update][def38]  

<br>

- Deterministic Policy with Added Noise  
  - 결정적 정책의 행동에 정규 분포에서 추출된 노이즈를 추가하여 탐험을 보장
  - Off-Policy DPG보다 단순하고 학습 속도가 빨라서 선호됨  
  ![deterministic_policy_with_noise][def39]  

<br>

- Deep Deterministic Policy Gradient (DDPG)  
  - DPG + Experience Replay + Fixed Target Network + Polyak 
  
- DDPG: Experience Replay
  - 새로운 경험을 관측할 때마다 최근 `N`개의 경험만을 유지하는 Replay Buffer `D`에 저장하고, `N`개를 초과할 시 오래된 경험부터 삭제  
  ![ddpg_experience_replay][def40]  

  - 매 `k`회 환경과 상호작용할 때마다, Replay Buffer `D`에서 무작위로 추출된 `B`개의 경험들로 매개변수를 업데이트  
  ![ddpg_experience_replay_update][def41]  

- DDPG: Fixed Target Network
  - Critic의 업데이트에는 Target Actor/Critic Network를 활용  
  ![ddpg_fixed_target_network][def42]  

- DDPG: Polyak Averaging
  - DQN : 일정 횟수 상호작용할 때마다 Behavior Network의 매개변수를 Target Network에 복제  

  - DDPG : Behavior Network가 업데이트 될 때마다 Polyak Averaging으로 Target Network를 업데이트  
  ![ddpg_polyak_averaging][def43]  

- DDPG: Pseudocode  
  ![ddpg_pseudocode][def44]  

<br>

- Twin Delayed DDPG (TD3)  
  - DDPG + Double Q-Learning + Target Policy Smoothing + Delayed Policy & Target Updates  

- TD3: Actor and Critic Networks  
![td3_actor_critic_networks][def45]  

- TD3: Clipped Double Q-Learning  
![td3_clipped_double_q_learning][def46]  
  - 행동 가치 함수를 추정하는 2개의 Behavior Critic 및 Target Critic을 활용
  - Behavior Critic의 업데이트에는 Target Critic이 추정한 행동 가치 함수의 값 중 작은 값을 Target으로 활용하여 행동 가치 함수가 과대평가되는 것을 방지  

- TD3: Target Policy Smoothing  
![td3_target_policy_smoothing][def47]  
  - Target Actor Network의 출력 + [-c, c] 사이의 무작위 노이즈를 추가한 행동으로 Critic Target을 계산하여 너무 이르게 특정한 행동을 고평가하는 것을 방지  

- TD3: Delayed Policy & Target Updates  
![td3_delayed_policy_target_updates][def48]  
  - Critic이 `P`회 이상 업데이트 된 후에 Behavior Actor는 경사 상승법으로, Target Actor/Critic은 Polyak Averaging으로 업데이트  

- TD3: Pseudocode  
![td3_pseudocode][def49]  

<br>

### 4. Entropy Regularization Method
- Entropy  
![entropy_definition](TODO)  
  - 확률 변수가 갖고 있는 정보의 양 또는 불확실성을 측정하는 수치

<br>

- Regularization
  - 매개변수의 값과 관련있는 규제항을 목적 함수에 추가하여, 매개변수가 여러가지 목적을 달성하도록 학습시키는 방법  
  - e.g., L1 Norm 규제항  
  ![l1_regularization](TODO)  
  -> 기본 목적함수(Mean Squared Error)에 매개변수의 크기만큼을 더함  
  -> 매개변수들의 값이 `0`이 아니라면 목적 함수의 값이 커짐  
  -> 목적 함수의 값을 최소화하기 위해 불필요한 매개변수를 제거 (`w` = `0`)  

<br>

- Entropy (of Policy) + Regularization  
  - 기존의 강화학습 : 수익을 최대화하는 매개변수를 찾는 것  
  ![reinforcement_learning_so_far](TODO)  
  - Entropy Regularization : 수익 + 정책의 엔트로피를 최대화하는 매개변수를 찾는 것  

- Why Policy of Entropy ?  
![why_policy_of_entropy](TODO)  
  - 주어진 상태에서 서로 다른 행동을 할 확률이 서로 비슷할수록 엔트로피가 높음
  - 주어진 사앹에서 수행할 행동을 아직 특정하지 못함
  - 주어진 상태에서 최적 또는 그에 준하는 정책을 아직 학습하지 ㅁ못하였음
  - 주어진 상태에 관측된 보상 또는 수익은 정확하지 않으며, 그보다 더 많을 수도 있음

<br>

- Rewrite Value Function with Entropy
  - State-Value Function  
  ![entropy_regularized_state_value_function](TODO)  

  - Action-Value Function  
  ![entropy_regularized_action_value_function](TODO)  

<br>

- Training Critic: Objective Function  
![entropy_regularized_critic_objective_function](TODO)  

- Training Critic: Loss Gradient & Update Rule  
![entropy_regularized_critic_loss_gradient](TODO)  

- Training Actor: Objective Function  
![entropy_regularized_actor_objective_function](TODO)  

- Training Actor: Policy Gradient & Update Rule  
![entropy_regularized_actor_policy_gradient](TODO)  

- Training Actor: Reparameterization Trick
  - 정규 분포  
  ![reparameterization_trick_normal](TODO)  

  - 표준 정규 분포 (`μ`=`0`, `σ`=`1`)  
  ![reparameterization_trick_standard_normal](TODO)  

  - 정규 분포의 확률 변수 `a`와 표준 정규 분포의 확률 변수 `z`의 관계  
  ![reparameterization_trick_relationship](TODO)  

  - 가능한 모든 행동의 확률을 고려하는 대신, 행동은 이미 선택되었고 그의 확률 분포는 표준 정규 분포를 따르도록 변경  
  ![reparameterization_trick_application](TODO)  
    - 더 이상 정책의 확률 분포에 의존적이지 않음  

- Training Actor: Updated Policy Gradient & Update Rule  
![entropy_regularized_actor_reparameterization_trick_1](TODO)  
![entropy_regularized_actor_reparameterization_trick_2](TODO)  

<br>

- Soft Actor-Critic : Entropy Regularization + Experience Replay + Fixed Target Network + Clipped Double Q-Learning  
  - Critic Update  
  ![soft_actor_critic_critic_update](TODO)  

  - Actor Update  
  ![soft_actor_critic_actor_update](TODO)

  - Target Network  
  ![soft_actor_critic_target_network](TODO)  

<br>

### 5. Trust Region Constraint Method  
- Relative Improvements of New Policy  
  - 기존 정책 `π`, 새로운 정책 `π'`이 주어졌을 때, 두 정책의 상대적인 차이는?  
  ![trust_region_relative_improvements_1](TODO)  
  ![trust_region_relative_improvements_2](TODO)  
  ![trust_region_relative_improvements_3](TODO)  
  ![trust_region_relative_improvements_4](TODO)  
  ![trust_region_relative_improvements_5](TODO)  

<br>

- Instability during Training
  - 지금까지 배운 어떠한 방법을 적용하더라도, 이론적으로는 최적의 정책을 학습할 수 있음.  
  - 하지만, 학습률(Learning Rate)에 따라 학습의 안정성이 크게 차이남
    - 큰 학습률 : 노이즈 등으로 인해 손실/정책 기울기가 갑자기 커진다면 매개변수의 값이 지나치게 튀어서 결과적으로 학습이 되지 않을 수 있음
    - 작은 학습률 : 학습 속도가 굉장히 느림

  - 적절한 학습률은 문제마다 다르기 때문에 시행 착오를거치는 수 밖에 없음

  - 매개변수가 업데이트 되는 크기 (= 정책의 변화하는 정도)를 제어할 수는 없을까 ?  

<br>

- Kullback-Leibler Divergence
  - 두 확률 분포의 차이를 측정하는 지표  
  ![kl_divergence_definition](TODO)  

  - 두 정책의 차이를 측정한다면?  
  ![kl_divergence_between_policies](TODO)  

<br>

- Trust Region Constraint
  - 기존 정책 `π`보다 상대적으로 수익을 더 많이 획득하면서도, 기존 정책과 크게 다르지 않은 (= Trust Region Constraint) 새로운 정책 `π'`을 학습  
  ![trust_region_constraint_1](TODO)  

  - 기존 정책의 매개변수 `θ`<sup>`(k)`</sup>보다 상대적으로 수익을 더 많이 획득하면서도, 기존 정책과 크게 다르지 않은 새로운 정책의 매개변수 `θ`<sup>`(k+1)`</sup>을 학습  
  ![trust_region_constraint_2](TODO)  

  <br>

- Trust Region Policy Optimization (TRPO)
  - 매개변수 업데이트 시 항상 수익의 기대값이 증가하도록 보장하면서, Trust Region Constraint를 통해 학습의 안정성을 확보  
  - 적절한 학습률에 대해 고민할 필요가 없음  

- TRPO: Taylor Series  
![trpo_taylor_series_1](TODO)  

- TRPO: Taylor Series of Relative Improvements  
![trpo_taylor_series_relative_improvements](TODO)  

- TRPO: Taylor Series of Constraint  
![trpo_taylor_series_constraint](TODO)  

<br>

- TRPO: Lagrange Multiplier
  - 제약사항이 있는 최적화 문제를 푸는 방법  
  ![trpo_lagrange_multiplier_1](TODO)  
    - 목적 함수와 제약 사항을 하나의 식으로 통합  
    ![trpo_lagrange_multiplier_2](TODO)  
    - 최적의 `x`와 `λ`를 찾는 문제(Dual Form)로 변경 (단, `f(x)`, `g(x)`는 볼록 함수)  
    ![trpo_lagrange_multiplier_3](TODO)  

<br>

- TRPO: Update Rule  
![trpo_update_rule](TODO)  

<br>

- TRPO: Pros and Cons
  - 장점
    - 이론적으로, 그리고 실제로도 TRPO는 매개변수의 업데이트마다 더 많은 수익을 얻을 수 있는 더 나은 정책을 안정적으로 학습

  - 단점
    - 복잡한 과정이 많이 있어서 구현이 까다로움
    - 한 번 미분하는 Gradient 대신 두 번 미분하는 Hessian 연산이 있어 연산량이 많고 메모리 사용량도 많음  

  <br>

- Proximal Policy Optimization (PPO)  
  - KL Divergence로 Trust Region Constraint를 적용하는 대신, 매개변수가 업데이트 되는 양이 일정 크기 이상 커지지 않도록 제한한 강화학습 알고리즘
  - TRPO보다 구현이 훨씬 간단하면서도 그와 유사한 성능을 냄  
  - 자세한 설명은 생략  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/BaGkBTF.png
[def2]: https://i.imgur.com/4XUqMmD.png
[def3]: https://i.imgur.com/faLkcNP.png
[def4]: https://i.imgur.com/BiDSFWS.png
[def5]: https://i.imgur.com/S6jKLJw.png
[def6]: https://i.imgur.com/G52K9QT.png
[def7]: https://i.imgur.com/qTLrms1.png
[def8]: https://i.imgur.com/99w5MYd.png
[def9]: https://i.imgur.com/jWiuvmS.png
[def10]: https://i.imgur.com/L95S1Qb.png
[def11]: https://i.imgur.com/FvnjMTI.png
[def12]: https://i.imgur.com/NKmEfjT.png
[def13]: https://i.imgur.com/PnJqF2y.png
[def14]: https://i.imgur.com/ry4hmbd.png
[def15]: https://i.imgur.com/685kHuO.png
[def16]: https://i.imgur.com/3XEZuSz.png
[def17]: https://i.imgur.com/EIfYSmI.png
[def18]: https://i.imgur.com/xRseZBn.png
[def19]: https://i.imgur.com/4FASf9u.png
[def20]: https://i.imgur.com/XIcRCSU.png
[def21]: https://i.imgur.com/MgoRI94.png
[def22]: https://i.imgur.com/xBvM5rm.png
[def23]: https://i.imgur.com/UTmw5Fr.png
[def24]: https://i.imgur.com/nKRhXrY.png
[def25]: https://i.imgur.com/RVcOyHH.png
[def26]: https://i.imgur.com/himc5e2.png
[def27]: https://i.imgur.com/BpddSfC.png
[def28]: https://i.imgur.com/yljeW4P.png
[def29]: https://i.imgur.com/fVvvSub.png
[def30]: https://i.imgur.com/CzuJ0D6.png
[def31]: https://i.imgur.com/ZGHKnjO.png
[def32]: https://i.imgur.com/oTT7ghy.png
[def33]: https://i.imgur.com/WtLmVEv.png
[def34]: https://i.imgur.com/Z2vaLT9.png
[def35]: https://i.imgur.com/f8tlfke.png
[def36]: https://i.imgur.com/aR1A8cD.png
[def37]: https://i.imgur.com/7QD7IXQ.png
[def38]: https://i.imgur.com/eLX9F5Q.png
[def39]: https://i.imgur.com/DYnPADb.png
[def40]: https://i.imgur.com/CMdoCVB.png
[def41]: https://i.imgur.com/zBvzRd8.png
[def42]: https://i.imgur.com/UxUTCTb.png
[def43]: https://i.imgur.com/dPglDrr.png
[def44]: https://i.imgur.com/9GgKDuB.png
[def45]: https://i.imgur.com/2ttlMf8.png
[def46]: https://i.imgur.com/0Jv0Tio.png
[def47]: https://i.imgur.com/c6iy2C5.png
[def48]: https://i.imgur.com/CyaoPhX.png
[def49]: https://i.imgur.com/C6VZq5h.png