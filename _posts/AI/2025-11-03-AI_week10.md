---
layout: post
title: "[인공지능] 11주차 - Nonlinear Function Approximation"
excerpt: "Artificial Neural Network, Deep Neural Network, Convolutional Neural Network"

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-03
last_modified_at: 2025-11-06
---
## Nonlinear Function Approximation
### 1. Artificial Neural Network
- Linear vs. Nonlinear Function  
![linear_vs_nonlinear][def]
  - Linear Function : 차수(Degree)가 1인 다항 함수로, 그래프가 직선  
  - Nonlinear Function : Linear Function이 아닌 모든 함수로, 그래프에 직선이 아닌 곡선이 포함되거나 일반적인 그래프 형태로 표현할 수 없음  

<br>

- Why Nonlinear & Artificial Neural Network ?
  - 선형 함수만을 이용해서는 복잡한 상태 공간 및 문제에 대한 가치 함수를 제대로 근사하기 어려움
    - e.g., 이미지로 제공되는 게임 화면을 상태로 받는 경우
  - 선형 함수로 근사하기 위해서는 좋은 특성 벡터를 찾아내는 것이 필수적이나, 직관, 도메인 지식, 운 등이 필요
  - 인공신경망(Artificial Neural Network)은 복잡한 상태 공간을 비선형적으로 근사할 수 있으며, 학습 과정에서 목적 함수를 최적화할 수 있는 특성 벡터를 암시적으로 찾아냄

<br>

- Biological -> Artificial Neural Network
  - 수없이 많은 신경 세포(Neuron)가 수없이 많은 연접(Synapse)을 통해 연결되어 있는 구조  
  => **신경 세포 -> 인공적인 신경 세포 (퍼셉트론, Perceptron)**  

  - 한 신경 세포로 들어온 전기적 신호는 그 강도가 임계치를 넘으면 해당 전기적 신호를 연결된 다른 신경 세포로 전달  
  => **입력은 활성 함수(Activation Function)를 통해 변환되어 다른 퍼셉트론으로 전달**  

  - 한 신경 세포가 다른 신경 세포와 강한 관련이 있으면 접합 강도가 증가하여, 해당 신경 세포로부터 전달받는 전기적 신호가 강해짐  
  => **퍼셉트론 간의 연결 강도는 가중치(Weight)로 표현**  

<br>

- Perceptron  
![perceptron][def2]  
  - 입력 `x` : 특성 벡터
  - 매개변수 `w` : 특성 벡터의 차원수와 같은 차원수의 가중치
  - 활성함수 `ɸ` : 가중 입력합 `z` = `x`<sup>`T`</sup>`w`을 입력으로 삼아 출력을 내는 함수
  - 출력 `h` = `ɸ(z)` : 스칼라 값  

<br>

- Activation Function  
![activation_functions][def3]  
  - 임계치에 대한 신경 세포의 반응을 흉내내는 함수
  - 대부분의 활성 함수는 연속이고, 단조 증가하며, 미분 가능함  

<br>

- Training Perceptron -> Gradient Descent  
- Training Perceptron: Example  
![training_perceptron_example_1][def4]  
![training_perceptron_example_2][def5]  

<br>

- Layer  
![layer][def6]  
  - `d`개의 퍼셉트론이 병렬적/독립적으로 연결된 구조  
  - `m`차원의 입력을 `d`차원의 출력으로 변환  

- Multi-Layered Perceptrons  
![multi_layered_perceptrons][def7]  
  - 입력, 잠재, 출력 레이어가 순차적으로 연결된 구조
  - 한 레이어의 출력이 다음 레이어에 존재하는 모든 퍼셉트론의 입력으로 사용됨(Fully Connected)
  - 출력 레이어엥 가까울 수록 깊다고 표현
  - 많은 레이어가 존재할수록 복잡하고 비선형적인 관계를 추론 가능  

- Training MLP ?
  - 각 레이어에 존재하는 퍼셉트론 내의 매개변수(= 가중치)에 대해서 각각 경사 하강법(Gradient Descent)을 적용하면 MLP를 학습시킬 수 있음
  - 그러나, MLP를 구성하는 퍼셉트론의 개수가 많아질수록, 훈련시켜야 하는 매개변수의 양은 기하급수적으로 증가.  

- Training MLP: Backpropagation  
![backpropagation][def8]  
  - 출력에 가까운 레이어(= 깊은 레이어)부터 입력에 가까운 레이어 방향으로 뒤로(backward) 이동하면서, 한 번에 하나의 레이어에 대해 손실 기울기를 계산  
  - 깊은 레이어에서 계산한 손실 기울기를 재사용함으로써 연산량을 감소  
  - Example  
    - Loss Gradient w.r.t `w`<sub>`5`</sub>  
    ![backpropagation_example_1][def9]  
    - Loss Gradient w.r.t `w`<sub>`7`</sub>  
    ![backpropagation_example_2][def10]  
    - Loss Gradient w.r.t `w`<sub>`1`</sub>   
    ![backpropagation_example_3][def11]  

<br>

- MLP as Nonlinear Function Approximation  
![mlp_as_nonlinear_function_approximation][def12]  
  - 상태(또는 특성 벡터)를 입력으로, 가치 함수의 추정치를 출력으로 하며, Target과의 차이를 최소화하도록 훈련된 Artificial Neural Network  

<br>

### 2. Deep Neural Network  
- Deep Neural Network(DNN)
  - Multi-Layered Perceptrons에ㅔ서 많은 수의 잠재 레이어(Hidden Layer)를 추가하여 깊게 만든 인공 신경망
  - 잠재 레이어의 수가 많을수록(= 깊이가 깊어질수록) 복잡한 문제를 잘 해결할 수 있음
  - 심층 신경망(Deep Neural Network) + 강화 학습(Reinforcement Learning) = 심층 강화 학습(Deep Reinforcement Learning)  

<br>

- Too Many Layers & Parameters !  
![too_many_layers_and_parameters][def13]  
  - 레이어의 수가 증가할수록 학습해야 하는 매개변수의 수는 엄청난게 증가
    - e.g., 2015년 이미지 인식 인공지능 ResNet : 152개의 레이어, 5830만개 매개변수
  - 모든 매개변수를 학습하는 데 많은 시간이 필요
  - 일부 레이어에 있는 매개변수는 학습이 잘 되지 않는 문제가 발생  

<br>

- Accelerate Training: Intuition  
![accelerate_training_intuition][def14]  

- Accelerate Training: Learining Rate Scheduling  
![learning_rate_scheduling][def15]  
  - 낮은 학습률 : 최적의 매개변수를 결국 찾을 수는 있으나, 학습 속도가 오래 걸림
  - 높은 학습률 : 최적의 매개변수로 빠르게 향하나, 그 주변에서 배회하는 경우가 발생  
  - Learning Rate Scheduling : 초기에는 높은 학습률, 후기에는 낮은 학습률을 사용한다면, 최적의 매개변수를 상대적으로 빠르게 찾을 수 있음  
  ![learning_rate_scheduling_example][def16]  

- Accelerate Training: Faster Gradient Descent  
![faster_gradient_descent][def17]  
  - 경사 하강법을 가속화하는 관성(Momentum)항을 추가함으로써 매개변수의 훈련 속도를 높임  
  - Performance Comparison (3이 가장 좋음)  
  ![momentum_performance_comparison][def18]  

<br>

- Gradient Problem
  - 기울기 소실(Vanishing Gradients) : 얕은 레이어의 손실 기울기가 점점 작아져서 더 이상 매개변수가 업데이트 되지 않는 현상
  - 기울기 폭주(Exploding Gradients) : 얕은 레이어의 손실 기울기가 점점 커져서 최적의 매개변수를 찾지 못하는 현상  
  - 기울기 불안정(Unstable Gradients) : 서로 다른 레이어의 손실 기울기가 다양해서, 각 레이어의 매개변수 학습 속도가 차이나는 현상  

- Gradient Problem: Why ?  
![gradient_problem_why][def19]  
  - 얕은 레이어의 손실 기울기는 깊은 레이어의 손실 기울기들의 곱으로 구성  
  -> 깊은 레이어의 손실 기울기가 한번 작아지기/커지기 시작하면, 얕은 레이어의 손실 기울기는 더욱더 작아지게/커지게 됨  

- Gradient Problem: Intuition  
  - 손실 기울기 값에 영향을 미치는 요소?
  - e.g., Activation Function:ReLU / Objective Function:Squared Loss  
  ![gradient_problem_intuition][def20]  
  -> 각 요소를 제어함으로써 기울기 문제를 해결할 수 있음  

- Gradient Problem: Activation Function  
  - 인공 신경망에서 가장 많이 사용되는 활성 함수 : ReLU(Rectified Linear Unit)  
  ![relu_activation_function_1][def21]  
  ![relu_activation_function_2][def22]  
    - 출력값은 `0` 또는 가중 입력합 `x`<sup>`T`</sup>`w`와 동일 -> 추가적인 지수/곱셈/덧셈 연산 등이 없으므로 리소스를 적게 사용  
    - 기울기 값은 `0` 또는 입력 `x`와 동일 -> 추가적인 지수/곱셈/덧셈 연산 등이 필요 없음  

  - Dying ReLU : ReLU를 활성함수로 사용하는 퍼셉트론은 훈련 도중 `0`의 출력만을 내는 경우가 자주 발생  
  ![dying_relu][def23]  
    - 우연히 매개변수의 값이 음수가 됨
    - ReLU의 입력인 가중 입력합 `x`<sup>`T`</sup>`w`또한 높은 확률로 음수가 됨
    - ReLU의 출력값이 `0`이 됨  
    - 기울기 소실  

  - 음의 입력에 대하여 `0`이 아닌 출력을 내는 ReLU의 변종들을 활용해서 Dying ReLU 문제를 방지  
  ![leaky_relu_activation_function][def24]  

  - 그렇지만 실제로는 Dying ReLU 문제가 심각하지 않음 (오히려 더 나은 성능을 내는 경우도 많음)  

- Gradient Problem: Weights  
![weight_initialization][def25]  
  - 훈련 초기에 활성 함수의 입력으로 사용되는 가중 입력합 `x`<sup>`T`</sup>`w`이 활성 함수의 기울기가 `0`에 가까운 지점에 위치  
  -> Saturating Gradient : 훈련 초반부터 매개변수의 학습이 이루어지지 않음  
  
  - Weight Initialization : 입력 데이터의 차원수 및 활성함수를 고려하여 가중치를 초기화하는 방법  
  ![weight_initialization_example][def26]  

- Gradient Problem: Input/Output/Loss Gradient  
![input_output_loss_gradient][def27]  
  - 목표로 하는 값(= 수익 혹은 보상+수익의 추정치)이 굉장히 크거나, 입력값이 굉장히 큰 경우에는 손실 기울기 값이 지나치게 커짐 -> 기울기 폭주
    - 보상 설계 시 적절히 작은 값(e.g., -1 ~ 1)을 사용
    - 입력 값을 특정한 범위 내에 제한
    - 손실 기울기 값을 특정한 범위 내에 제한  

<br>

### 3. Convolutional Neural Network  
- 어떤 함수 `g`를 `y`축을 기준으로 반전한 후, `x`축으로 이동시켜가면서 다른 함수 `f`와 곱하여 더하거나 적분한 것  
![convolution_definition][def28]  

- Convolution of Scalar Input  
![convolution_of_scalar_input_1][def29]  
![convolution_of_scalar_input_2][def30]  

- Convolution of Vector Input  
![convolution_of_vector_input][def31]  

<br>

- Why Convolution ?
  - 서로 인접한 입력값들이 하나의 출력을 생성  
  -> 서로 인접한 입력값들이 하나의 출력에 관련이 있음

  - 한 행씩 이동하면서 서로 인접한 입력값들에 대해 동일한 가중치를 활용  
  -> 서로 인접한 입력값들이 출력에 미치는 영향(= 가중치)은 전체 데이터에서 유사하게 발견됨  

  - 인접한 입력값들 사이에 존재하는 연관성을 추출
    - e.g., 시계열 데이터에서 타임 스텝 `t`=`1`의 데이터 `x`<sub>`1`</sub>과 타임 스텝 `t`=`2`의 데이터 `x`<sub>`2`</sub>가 출력에 미치는 영향(= 가중치, 매개변수)는 `x`<sub>`2`</sub>와 `x`<sub>`3`</sub>, `x`<sub>`3`</sub>과 `x`<sub>`4`</sub>에서 발견되는 영향과 유사할 것  

<br>

- Convolution Layer  
![convolution_layer][def32]  
  - Convolution Neural Network의 핵심으로, 입력(Receptive Field)과 가중치(Filter or Kernel)에 대한 Convolution 연산의 수행 결과(Feature Map)를 활성 함수에 통과시켜 최종 출력을 냄  

<br>

- 1D Conv w/ 1D Input & 1 Filter  
![1d_conv_w_1d_input_and_1_filter][def33]  

- 1D Conv w/ `m`-D Input & 1 Filter  
![1d_conv_w_m_d_input_and_1_filter][def34]  

- 1D Conv w/ `m`-D Input & `k` Filters  
![1d_conv_w_1d_input_and_k_filters][def35]  

<br>

- More than 1D Convolution  
  - 시계열 데이터의 경우 Receptive Field가 행 방향(= 시간축)으로 이동하는 1D Convolution 연산이 적절

  - 이미지 데이터라면 행 방향 뿐만 아니라 열 방향의 관계도 중요할 수 있음  
    - e.g., 아래 각 9개의 픽셀이 출력값에 미치는 영향(= 가중치, 매개변수)은 서로 유사함  
    ![2d_convolution_example][def36]  

  - 데이터의 특성과 가정에 따라 Receptive Field의 형태와 이동 방향을 다양하게 구성하여 Convolution 연산을 수행  
    - 2D Conv. Layer : Receptive Field가 `k`x`k` 형태이며 행 및 열 방향으로 이동  
    - 3D Conv. Layer : Receptive Field가 `k`x`k`x`k` 형태이며 행, 열, 깊이 방향으로 이동  

- 2D Conv w/ 1D Input & `k` Filters  
![2d_conv_w_1d_input_and_k_filters][def37] 

- 2D Conv w/ `m`-D Input & `k` Filters  
![2d_conv_w_m_d_input_and_k_filters][def38]  

<br>

- Conv. Layer vs. Dense Layer
  - Dense(or Fully Connected) Layer with 20 Perceptrons  
  ![dense_layer_with_20_perceptrons][def39]  
    - 입력 : 28 x 28 = 784 차원
    - 가중치 : 784 x 20 = 15,680개
    - 편차 : 1 x 20 = 20개
    - 총합 : 15,700개

  - 2D Conv. Layer with 10 x 10 Rec. Field & 20 Filters  
  ![conv_layer_with_10x10_rec_field_and_20_filters][def40]  
    - 입력 : 28 x 28 = 784 차원
    - 가중치 : 10 x 10 x 20 = 2,000개
    - 편차 : 1 x 20 = 20개
    - 총합 : 2,020개  

<br>

- Vanishing Outputs  
![vanishing_outputs][def41]  
  - Convolution Layer는 입력의 개수보다 적은 개수의 출력을 생성
  - 여러 개의 Convolution Layer가 연결되어 있다면 출력의 개수가 점점 작아져서 없어질 수도 있음  
  - Convolution Layer의 Receptive Field의 크기(= Kernel size)가 증가할 수록 생성하는 출력의 개수는 더욱 감소  
  - Stride를 큰 값으로 설정하면 일부 입력에 대해서는 Convolution 연산이 수행되지 않으며, 따라서 해당 입력에 대한 정보가 손실됨

- Zero-Padding  
![zero_padding][def42]  
  - 입력의 시작/끝 부분에 `0`을 덧대게 되면, Kernel size가 커지더라도 Convolution Layer가 생성하는 출력의 개수를 입력의 개수와 같도록 유지할 수 있음  

  - Zero-Padding 미적용 시 출력의 개수  
  ![zero_padding_not_applied][def43]  

  - Zero-Padding 적용 시 출력의 개수  
  ![zero_padding_applied][def44]  

  - 문제점
    - Stride가 `1`이고 Zero-Padding 이 적용된 Convolution Layer는 항상 입력의 개수와 같은 개수의 출력을 생성
    - 만약 고화질 이미지와 같이 크기가 큰 데이터라면?
      - Convolution Layer가 생성하는 출력의 크기가 크다
      - 다음 레이어가 사용하는 입력이 크다
      - Convolution 연산의 횟수가 많다
      - 훈련 시간과 컴퓨팅 리소스가 많이 필요하다  

<br>

- Pooling Layer  
![pooling_layer][def45]  
  - Convolution Neural Network의 또 다른 핵심으로, 중요한 정보를 유지한 채로 입력을 압축(Downsampling)  
  - 구성 요소
    - Pool : 서로 인접한 입력들의 집합. Convolution Layer의 Receptive Field와 같은 개념
    - Strides : Pool이 입력 공간을 움직이는 단위 거리
    - 요약 함수 `f` : Pool 내의 입력들을 요약하는 함수로, 최대(Max Pooling) 또는 평균(Average Pooling)을 활용
  - 별도로 훈련해야하는 매개변수는 없음  

<br>

- 1D Pool with `m`-D Input  
![1d_pool_with_m_d_input_1][def46]  
![1d_pool_with_m_d_input_2][def47]  

<br>

- Typical Structure  
![typical_structure][def48]  
  - (1) 적은 수의 큰 Filter로 된 Convolution Layer로 시작
  - (2) Pooling Layer를 통해 입력 크기를 감소
  - (3) 많은 수의 작은 Filter로 된 Convolution Layer를 연속적으로 연결
  - (4) 펼치기(Flattening)을 통해 한 행으로 변경
  - (5) Dense Layer를 통해 최종 출력 생성  

- Advanced Structure: Global Pooling Layer  
  - 이전의 구조에서는 Convolution Layer의 출력을 펼친 후, Dense Layer에 연결
  - 이로 인해, Convolution Layer가 구했던 인접한 입력들 간의 관계가 소실됨
  - 따라서 Flattening 대신, Convolution Layer가 출력한 각 Feature Map 마다 통계적 수치를 추출하는 Global Pooling을 수행  
  ![global_pooling_layer][def49]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/5cISjkU.png
[def2]: https://i.imgur.com/AJ4915F.png
[def3]: https://i.imgur.com/hUSCWVX.png
[def4]: https://i.imgur.com/2RfjrQo.png
[def5]: https://i.imgur.com/YItL9JX.png
[def6]: https://i.imgur.com/wkZGwhq.png
[def7]: https://i.imgur.com/YLHXllX.png
[def8]: https://i.imgur.com/uQJVqsD.png
[def9]: https://i.imgur.com/9YBD0ek.png
[def10]: https://i.imgur.com/ed9jHZR.png
[def11]: https://i.imgur.com/t6o5lxC.png
[def12]: https://i.imgur.com/RdKSHmi.png
[def13]: https://i.imgur.com/ZNnPgHv.png
[def14]: https://i.imgur.com/kb6RHty.png
[def15]: https://i.imgur.com/eeasQfI.png
[def16]: https://i.imgur.com/5C1sIgx.png
[def17]: https://i.imgur.com/cyWMOfK.png
[def18]: https://i.imgur.com/YIXKHLw.png
[def19]: https://i.imgur.com/wARDcUq.png
[def20]: https://i.imgur.com/onznpId.png
[def21]: https://i.imgur.com/YmIqRyA.png
[def22]: https://i.imgur.com/MC1DReR.png
[def23]: https://i.imgur.com/W2oXFWH.png
[def24]: https://i.imgur.com/eMD13hu.png
[def25]: https://i.imgur.com/iR3O0Ds.png
[def26]: https://i.imgur.com/HpFGwBu.png
[def27]: https://i.imgur.com/fNc4beb.png
[def28]: https://i.imgur.com/QSt5cuI.png
[def29]: https://i.imgur.com/sJTu8t6.png
[def30]: https://i.imgur.com/bdh9Yci.png
[def31]: https://i.imgur.com/US5ih8f.png
[def32]: https://i.imgur.com/jdKfSYR.png
[def33]: https://i.imgur.com/DQsN8lP.png
[def34]: https://i.imgur.com/riblhoJ.png
[def35]: https://i.imgur.com/oJd0Frq.png
[def36]: https://i.imgur.com/I8D5DU1.png
[def37]: https://i.imgur.com/uZh9IPF.png
[def38]: https://i.imgur.com/8RLCdiG.png
[def39]: https://i.imgur.com/jIQ1Qr8.png
[def40]: https://i.imgur.com/pa1kJY4.png
[def41]: https://i.imgur.com/FTRaivv.png
[def42]: https://i.imgur.com/1StWTE5.png
[def43]: https://i.imgur.com/eP74ENw.png
[def44]: https://i.imgur.com/yaW4ckT.png
[def45]: https://i.imgur.com/9tYP0BP.png
[def46]: https://i.imgur.com/Iu8fHOn.png
[def47]: https://i.imgur.com/8IfA5RU.png
[def48]: https://i.imgur.com/uqiYr0N.png
[def49]: https://i.imgur.com/YVWCjaE.png