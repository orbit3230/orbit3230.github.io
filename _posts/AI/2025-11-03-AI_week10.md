---
layout: post
title: "[인공지능] 11주차 - Nonlinear Function Approximation"
excerpt: "Artificial Neural Network, Deep Neural Network, Convolutional Neural Network"

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-03
last_modified_at: 2025-11-03
---
## Nonlinear Function Approximation
### 1. Artificial Neural Network
- Linear vs. Nonlinear Function  
![linear_vs_nonlinear](TODO)
  - Linear Function : 차수(Degree)가 1인 다항 함수로, 그래프가 직선  
  - Nonlinear Function : Linear Function이 아닌 모든 함수로, 그래프에 직선이 아닌 곡선이 포함되거나 일반적인 그래프 형태로 표현할 수 없음  

<br>

- Why Nonlinear & Artificial Neural Network ?
  - 선형 함수만을 이용해서는 복잡한 상태 공간 및 문제에 대한 가치 함수를 제대로 근사하기 어려움
    - e.g., 이미지로 제공되는 게임 화면을 상태로 받는 경우
  - 선형 함수로 근사하기 위해서는 좋은 특성 벡터를 찾아내는 것이 필수적이나, 직관, 도메인 지식, 운 등이 필요
  - 인공신경망(Artificial Neural Network)은 복잡한 상태 공간을 비선형적으로 근사할 수 있으며, 학습 과정에서 목적 함수를 최적화할 수 있는 특성 벡터를 암시적으로 찾아냄

<br>

- Biological -> Artificial Neural Network
  - 수없이 많은 신경 세포(Neuron)가 수없이 많은 연접(Synapse)을 통해 연결되어 있는 구조  
  => **신경 세포 -> 인공적인 신경 세포 (퍼셉트론, Perceptron)**  

  - 한 신경 세포로 들어온 전기적 신호는 그 강도가 임계치를 넘으면 해당 전기적 신호를 연결된 다른 신경 세포로 전달  
  => **입력은 활성 함수(Activation Function)를 통해 변환되어 다른 퍼셉트론으로 전달**  

  - 한 신경 세포가 다른 신경 세포와 강한 관련이 있으면 접합 강도가 증가하여, 해당 신경 세포로부터 전달받는 전기적 신호가 강해짐  
  => **퍼셉트론 간의 연결 강도는 가중치(Weight)로 표현**  

<br>

- Perceptron  
![perceptron](TODO)  
  - 입력 `x` : 특성 벡터
  - 매개변수 `w` : 특성 벡터의 차원수와 같은 차원수의 가중치
  - 활성함수 `ɸ` : 가중 입력합 `z` = `x`<sup>`T`</sup>`w`을 입력으로 삼아 출력을 내는 함수
  - 출력 `h` = `ɸ(z)` : 스칼라 값  

<br>

- Activation Function  
![activation_functions](TODO)  
  - 임계치에 대한 신경 세포의 반응을 흉내내는 함수
  - 대부분의 활성 함수는 연속이고, 단조 증가하며, 미분 가능함  

<br>

- Training Perceptron -> Gradient Descent  
- Training Perceptron: Example  
![training_perceptron_example_1](TODO)  
![training_perceptron_example_2](TODO)  

<br>

- Layer  
![layer](TODO)  
  - `d`개의 퍼셉트론이 병렬적/독립적으로 연결된 구조  
  - `m`차원의 입력을 `d`차원의 출력으로 변환  

- Multi-Layered Perceptrons  
![multi_layered_perceptrons](TODO)  
  - 입력, 잠재, 출력 레이어가 순차적으로 연결된 구조
  - 한 레이어의 출력이 다음 레이어에 존재하는 모든 퍼셉트론의 입력으로 사용됨(Fully Connected)
  - 출력 레이어엥 가까울 수록 깊다고 표현
  - 많은 레이어가 존재할수록 복잡하고 비선형적인 관계를 추론 가능  

- Training MLP ?
  - 각 레이어에 존재하는 퍼셉트론 내의 매개변수(= 가중치)에 대해서 각각 경사 하강법(Gradient Descent)을 적용하면 MLP를 학습시킬 수 있음
  - 그러나, MLP를 구성하는 퍼셉트론의 개수가 많아질수록, 훈련시켜야 하는 매개변수의 양은 기하급수적으로 증가.  

- Training MLP: Backpropagation  
![backpropagation](TODO)  
  - 출력에 가까운 레이어(= 깊은 레이어)부터 입력에 가까운 레이어 방향으로 뒤로(backward) 이동하면서, 한 번에 하나의 레이어에 대해 손실 기울기를 계산  
  - 깊은 레이어에서 계산한 손실 기울기를 재사용함으로써 연산량을 감소  
  - Example  
    - Loss Gradient w.r.t `w`<sub>`5`</sub>  
    ![backpropagation_example_1](TODO)  
    - Loss Gradient w.r.t `w`<sub>`7`</sub>  
    ![backpropagation_example_2](TODO)  
    - Loss Gradient w.r.t `w`<sub>`1`</sub>   
    ![backpropagation_example_3](TODO)  

<br>

- MLP as Nonlinear Function Approximation  
![mlp_as_nonlinear_function_approximation](TODO)  
  - 상태(또는 특성 벡터)를 입력으로, 가치 함수의 추정치를 출력으로 하며, Target과의 차이를 최소화하도록 훈련된 Artificial Neural Network  

<br>

### 2. Deep Neural Network  
- Deep Neural Network(DNN)
  - Multi-Layered Perceptrons에ㅔ서 많은 수의 잠재 레이어(Hidden Layer)를 추가하여 깊게 만든 인공 신경망
  - 잠재 레이어의 수가 많을수록(= 깊이가 깊어질수록) 복잡한 문제를 잘 해결할 수 있음
  - 심층 신경망(Deep Neural Network) + 강화 학습(Reinforcement Learning) = 심층 강화 학습(Deep Reinforcement Learning)  

<br>

- Too Many Layers & Parameters !  
![too_many_layers_and_parameters](TODO)  
  - 레이어의 수가 증가할수록 학습해야 하는 매개변수의 수는 엄청난게 증가
    - e.g., 2015년 이미지 인식 인공지능 ResNet : 152개의 레이어, 5830만개 매개변수
  - 모든 매개변수를 학습하는 데 많은 시간이 필요
  - 일부 레이어에 있는 매개변수는 학습이 잘 되지 않는 문제가 발생  

<br>

- Accelerate Training: Intuition  
![accelerate_training_intuition](TODO)  

- Accelerate Training: Learining Rate Scheduling  
![learning_rate_scheduling](TODO)  
  - 낮은 학습률 : 최적의 매개변수를 결국 찾을 수는 있으나, 학습 속도가 오래 걸림
  - 높은 학습률 : 최적의 매개변수로 빠르게 향하나, 그 주변에서 배회하는 경우가 발생  
  - Learning Rate Scheduling : 초기에는 높은 학습률, 후기에는 낮은 학습률을 사용한다면, 최적의 매개변수를 상대적으로 빠르게 찾을 수 있음  
  ![learning_rate_scheduling_example](TODO)  

- Accelerate Training: Faster Gradient Descent  
![faster_gradient_descent](TODO)  
  - 경사 하강법을 가속화하는 관성(Momentum)항을 추가함으로써 매개변수의 훈련 속도를 높임  
  - Performance Comparison (3이 가장 좋음)  
  ![momentum_performance_comparison](TODO)  

<br>

- Gradient Problem
  - 기울기 소실(Vanishing Gradients) : 얕은 레이어의 손실 기울기가 점점 작아져서 더 이상 매개변수가 업데이트 되지 않는 현상
  - 기울기 폭주(Exploding Gradients) : 얕은 레이어의 손실 기울기가 점점 커져서 최적의 매개변수를 찾지 못하는 현상  
  - 기울기 불안정(Unstable Gradients) : 서로 다른 레이어의 손실 기울기가 다양해서, 각 레이어의 매개변수 학습 속도가 차이나는 현상  

- Gradient Problem: Why ?  
![gradient_problem_why](TODO)  
  - 얕은 레이어의 손실 기울기는 깊은 레이어의 손실 기울기들의 곱으로 구성  
  -> 깊은 레이어의 손실 기울기가 한번 작아지기/커지기 시작하면, 얕은 레이어의 손실 기울기는 더욱더 작아지게/커지게 됨  

- Gradient Problem: Intuition  
  - 손실 기울기 값에 영향을 미치는 요소?
  - e.g., Activation Function:ReLU / Objective Function:Squared Loss  
  ![gradient_problem_intuition](TODO)  
  -> 각 요소를 제어함으로써 기울기 문제를 해결할 수 있음  

- Gradient Problem: Activation Function  
  - 인공 신경망에서 가장 많이 사용되는 활성 함수 : ReLU(Rectified Linear Unit)  
  ![relu_activation_function_1](TODO)  
  ![relu_activation_function_2](TODO)  
    - 출력값은 `0` 또는 가중 입력합 `x`<sup>`T`</sup>`w`와 동일 -> 추가적인 지수/곱셈/덧셈 연산 등이 없으므로 리소스를 적게 사용  
    - 기울기 값은 `0` 또는 입력 `x`와 동일 -> 추가적인 지수/곱셈/덧셈 연산 등이 필요 없음  

  - Dying ReLU : ReLU를 활성함수로 사용하는 퍼셉트론은 훈련 도중 `0`의 출력만을 내는 경우가 자주 발생  
  ![dying_relu](TODO)  
    - 우연히 매개변수의 값이 음수가 됨
    - ReLU의 입력인 가중 입력합 `x`<sup>`T`</sup>`w`또한 높은 확률로 음수가 됨
    - ReLU의 출력값이 `0`이 됨  
    - 기울기 소실  

  - 음의 입력에 대하여 `0`이 아닌 출력을 내는 ReLU의 변종들을 활용해서 Dying ReLU 문제를 방지  
  ![leaky_relu_activation_function](TODO)  

  - 그렇지만 실제로는 Dying ReLU 문제가 심각하지 않음 (오히려 더 나은 성능을 내는 경우도 많음)  

- Gradient Problem: Weights  
![weight_initialization](TODO)  
  - 훈련 초기에 활성 함수의 입력으로 사용되는 가중 입력합 `x`<sup>`T`</sup>`w`이 활성 함수의 기울기가 `0`에 가까운 지점에 위치  
  -> Saturating Gradient : 훈련 초반부터 매개변수의 학습이 이루어지지 않음  
  
  - Weight Initialization : 입력 데이터의 차원수 및 활성함수를 고려하여 가중치를 초기화하는 방법  
  ![weight_initialization_example](TODO)  

- Gradient Problem: Input/Output/Loss Gradient  
![input_output_loss_gradient](TODO)  
  - 목표로 하는 값(= 수익 혹은 보상+수익의 추정치)이 굉장히 크거나, 입력값이 굉장히 큰 경우에는 손실 기울기 값이 지나치게 커짐 -> 기울기 폭주
    - 보상 설계 시 적절히 작은 값(e.g., -1 ~ 1)을 사용
    - 입력 값을 특정한 범위 내에 제한
    - 손실 기울기 값을 특정한 범위 내에 제한  

<br>

### 3. Convolutional Neural Network  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 