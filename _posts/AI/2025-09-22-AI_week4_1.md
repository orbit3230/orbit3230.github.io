---
layout: post
title: "[인공지능] 4주차 (1) - Tutorial on Gymnasium"
excerpt: "Installation, Basic Usage, Example: Frozen Lake"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-22
last_modified_at: 2025-09-22
---
## Tutorial on Gymnasium
### 1. Installation
`%pip install gymnasium kymnasium matplotlib`  

<br>

### 2. Basic Usage
- `gymnasium.make` : 환경 인스턴스를 생성하는 함수  

```python
env = gym.make(
    id="environment-id",
    render_mode="human", # or "rgb_array"
)
```

- render modes
  - `human` : 화면에 환경을 렌더링
  - `rgb_array` : 백그라운드에서 불필요한 시간 소모 없이 빠르게 학습 (주로 훈련 시 사용)

<br>

- `gymnasium.Env` (환경 구조체) Key functions  
  - `Env.step(action)` : 환경에 행동을 수행한 후 행동에 대한 보상과 상태를 반환하는 함수. 아래와 같은 결과를 튜플 형태로 반환한다.  
    - `observation` : 행동을 수행한 후의 새로운 상태
    - `reward` : 행동에 대한 보상
    - `terminated` : 종료 상태 도달 여부 (True/False)
    - `truncated` : 시간 제한에 의한 종료 여부 (True/False)
    - `info` : 추가적인 정보 (디버깅 용도 등)

  - `Env.reset()` : 환경을 초기 상태로 되돌리는 함수. 아래와 같은 결과를 튜플 형태로 반환한다.  
    - `observation` : 초기 상태
    - `info` : 추가적인 정보 (디버깅 용도 등)
 
  - `Env.render()` : 환경의 상태를 NumPy 배열 이미지로 반환하는 함수.

  - `Env.close()` : 환경을 종료하는 함수.  

- `gymnasium.Env` (환경 구조체) Key attributes  
  - `Env.action_space` : 환경에서 가능한 행동공간(Action Space)을 반환
  - `Env.observation_space` : 환경에서 가능한 상태공간(State Space)을 반환  

<br>

### 3. Example: Frozen Lake
- 얼음 구멍을 피해서 도착 지점에 도달하도록 하는 것이 목표인 튜토리얼

- 기본 환경 생성 및 초기화  

```python
import gymnasium as gym


# 환경을 생성
env = gym.make(
    id='FrozenLake-v1',
    render_mode='rgb_array',
    map_name='4x4', # 기본적으로 탑재되어 있는 4x4 맵을 사용
    is_slippery=False, # 미끄러짐 (확률적으로 타일에서 미끄러져서 선택하지 않은 방향으로 이동) 여부 설정
)

# 환경과 상호작용하기 전 reset()을 호출하여 초기화
obs, info = env.reset()

# 환경에서 반환하는 초기 상태
print(f'# Observation: {obs}')

# 환경에서 반환하는 초기 정보
print(f'# Info: {info}')
```  

```
# Observation: 0
# Info: {'prob': 1}
```

- 시각적으로 확인  

```python
import matplotlib.pyplot as plt

image = env.render()

plt.imshow(image)
plt.axis('off')
plt.show()
```  

![frozen_lake_1][def]  

- 우측으로 1회 이동  

```python
# 0: 좌, 1: 하, 2: 우, 3: 상
obs, reward, terminated, truncated, info = env.step(2)

print(f'# Observation: {obs}')
print(f'# Reward: {reward}')
print(f'# Terminated: {terminated}')
print(f'# Truncated: {truncated}')
print(f'# Info: {info}')

image = env.render()

plt.imshow(image)
plt.axis('off')
plt.show()
```

![frozen_lake_2][def2]  

- 행동 공간과 상태 공간 확인  

```python
print('Action space: ', env.action_space)
print('State space: ', env.observation_space)
```

```
Action space:  Discrete(4)
State space:  Discrete(16)
```

- Custom map 생성

```python
import numpy as np


CUSTOM_MAP = [
    "SFFFFFFH",
    "FFHFFFFF",
    "FFFHFFFF",
    "FFFFFHFF",
    "FFFHFFFF",
    "FHHFFFHF",
    "FHFFHFHF",
    "FFFHFFFG",
]
CUSTOM_MAP = np.array(CUSTOM_MAP, dtype='c')
CUSTOM_MAP
```

- 환경 생성 시 `desc` 파라미터에 커스텀 맵을 전달  

```python
import gymnasium as gym


env = gym.make(
    id='FrozenLake-v1',
    render_mode='rgb_array',
    desc=CUSTOM_MAP, # 방금 제작한 맵을 사용
    is_slippery=False, # 타일에서 미끄러지는 여부를 결정
)

env.reset()
image = env.render()

plt.imshow(image)
plt.axis('off')
plt.show()
```

![frozen_lake_custom_map][def3]  

<br>

- 에이전트 생성. `kymnasium`에서 `Agent` 클래스를 상속받아 구현  

```python
import kymnasium as kym


class Agent(kym.Agent):
    def act(self, observation, info):
        pass

    def save(self, path):
        pass

    @classmethod
    def load(cls, path: str):
        pass
```

- `act(self, obsservation, info)` : 환경이 제공하는 `observation`과 `info`를 입력으로 받아 행동을 반환하는 함수  
- `save(self, path)` : 훈련한 에이전트를 저장하는 함수
- `load(cls, path)` : 저장된 에이전트를 불러오는 함수  

<br>

- 무작위로 움직이는 에이전트 구현  

```python
import random
import pickle
import kymnasium as kym


class RandomAgent(kym.Agent):
    ACTIONS = [0, 1, 2, 3]

    # 0, 1, 2, 3의 행동 중 무작위로 선택
    def act(self, obs, info):
        return random.choice(self.ACTIONS)

    # 파이썬의 Pickle 모듈로 에이전트를 저장
    def save(self, path):
        with open(path, mode='wb') as f:
            pickle.dump(self, f)

    # 파이썬의 Pickle 모듈로 에이전트를 로드
    @classmethod
    def load(cls, path: str):
        with open(path, mode='rb') as f:
            return pickle.load(f)
```

- 에이전트가 10회 행동하도록 시뮬레이션  

```python
agent = RandomAgent()
images = []

done = False
steps = 0

obs, info = env.reset()
images.append(env.render())

while not done and steps < 10:
    # 환경의 상태와 정보를 활용하여 행동을 선택
    action = agent.act(obs, info)

    # 선택된 행동으로 환경과 상호작용하고,
    # 그로 인해 변화된 상태와 보상 등을 획득
    obs, reward, terminated, truncated, info = env.step(action)
    images.append(env.render())

    steps += 1
    # 종료 여부를 확인
    done = terminated or truncated
```

- 시뮬레이션 결과 시각화  

```python
import matplotlib.pyplot as plt


fig, axes = plt.subplots(len(images), 1, figsize=(5, len(images) * 5))

for i, image in enumerate(images):
    axes[i].imshow(image)
    axes[i].axis('off')

plt.show()
```

<br>

- 최적의 정책을 학습한 에이전트 구현. 우선 정책을 클래스 생성 시 받아서 사용하는 에이전트 정의  

```python
import random
import pickle
import kymnasium as kym


class TrainedAgent(kym.Agent):
    # 클래스 생성 시 정책을 입력으로 받음
    def __init__(self, PI: list):
        self.PI = PI

    # 훈련된 정책에서 행동을 선택
    def act(self, obs, info):
        return self.PI[obs]

    # 파이썬의 Pickle 모듈로 에이전트를 저장
    def save(self, path):
        with open(path, mode='wb') as f:
            pickle.dump(self, f)

    # 파이썬의 Pickle 모듈로 에이전트를 로드
    @classmethod
    def load(cls, path: str):
        with open(path, mode='rb') as f:
            return pickle.load(f)
```

- 보상 설정  

```python
# 각 행동을 취했을 때 좌표의 변화를 기록
COORDS = {
    0: (0, -1), # 좌
    1: (1, 0), # 하
    2: (0, 1), # 우
    3: (-1, 0) # 상
}


def estimate_value(state, action, gamma, V):
    # 현재 상태로부터 행/열 번호를 추출
    row, col = state // 8, state % 8

    # 행동을 취했을 때 이동하는 곳의 좌표를 계산
    (d_row, d_col) = COORDS[action]
    new_row, new_col = row + d_row, col + d_col

    # 0 ~ 7 사이의 값으로 행/열번호 보정
    new_row = min(max(new_row, 0), 7)
    new_col = min(max(new_col, 0), 7)

    # 새로운 좌표로부터 새로운 상태 계산
    new_state = new_row * 8 + new_col

    # 이동된 좌표의 타일 정보를 추출
    tile = CUSTOM_MAP[new_row, new_col]

    # 얼음 구멍 타일의 경우 보상을 -1로 책정
    if tile == b'H':
        reward = -1
    # 도착 지점에 도착할 시 보상을 +1로 책정
    elif tile == b'G':
        reward = 1
    # 그 외 타일 (일반 타일, 시작 지점)은 보상을 0으로 책정
    else:
        reward = 0

    # 이 환경에서는 주어진 상태에서 같은 행동을 취하면
    # 항상 같은 보상 획득 및 상태 전이가 일어나므로,
    # 전이 확률 P(s',r|s, a)는 1과 같음
    # 벨만 방정식 계산
    v = reward + gamma * V[new_state]

    return v
```

- Value Iteration 구현 (가치 함수가 수렴할 때까지 반복)

```python
PHI = 1e-3 # 가치 함수의 수렴 여부를 확인하는 임계치
GAMMA = 0.99 # 할인율

# 상태 가치 함수를 초기화
V = [0 for _ in range(8 * 8)]


# 상태 가치 함수가 수렴할 때까지 업데이트
while True:
    delta = 0
    for state in range(8 * 8):
        prev_v = V[state]

        max_v = 0
        for action in [0, 1, 2, 3]:
            v = estimate_value(state, action, GAMMA, V)
            max_v = max(max_v, v)

        V[state] = max_v
        delta = max(delta, abs(V[state] - prev_v))

    if delta < PHI:
        break

PI = [-1 for _ in range(8 * 8)]

# 각 상태별로 최적의 행동을 저장
for state in range(8 * 8):
    max_v = -1e10
    opt_action = -1

    for action in [0, 1, 2, 3]:
        v = estimate_value(state, action, GAMMA, V)
        if max_v < v:
            max_v = v
            opt_action = action
    PI[state] = opt_action

agent = TrainedAgent(PI)
agent.save('./value_iter_agent.pkl')
```

- 학습된 에이전트로 시뮬레이션  

```python
import matplotlib.pyplot as plt


agent = TrainedAgent.load('./value_iter_agent.pkl')
images = []

done = False

obs, info = env.reset()
images.append(env.render())

while not done:
    # 환경의 상태와 정보를 활용하여 행동을 선택
    action = agent.act(obs, info)

    # 선택된 행동으로 환경과 상호작용하고,
    # 그로 인해 변화된 상태와 보상 등을 획득
    obs, reward, terminated, truncated, info = env.step(action)

    # 노트북에서는 백그라운드 렌더링밖에 안되기 때문에
    # 대신, 이미지를 따로 저장해서 추후 시각화
    images.append(env.render())

    # 종료 여부를 확인
    done = terminated or truncated

print(f'Game Clear w/ {len(images) - 1} Actions')

fig, axes = plt.subplots(len(images), 1, figsize=(5, len(images) * 5))

for i, image in enumerate(images):
    axes[i].imshow(image)
    axes[i].axis('off')

plt.show()
```

- Policy Iteration 구현 (정책이 수렴할 때까지 반복)

```python
import random
import copy


PHI = 1e-3 # 가치 함수의 수렴 여부를 확인하는 임계치
GAMMA = 0.99 # 할인율

# 상태 가치 함수를 초기화
V = [0 for _ in range(8 * 8)]

# 정책을 초기화
PI = [random.choice([0, 1, 2, 3]) for _ in range(8 * 8)]
PI_prime = [None for _ in range(8 * 8)]

# 정책이 수렴할 때까지 업데이트
while PI != PI_prime:
    # 현재 정책에 대한 상태 가치 함수를 초기화
    for state in range(8 * 8):
        V[state] = 0

    # Policy Evaluation: 현재 정책에 대한 상태 가치 함수를 추정
    while True:
        delta = 0

        for state in range(8 * 8):
            prev_v = V[state]
            V[state] = estimate_value(state, PI[state], GAMMA, V)
            delta = max(delta, abs(V[state] - prev_v))

        if delta < PHI:
            break

    PI_prime = list(PI)

    # Policy Improvement: 현재 정책을 더 나은 정책으로 개선
    for state in range(8 * 8):
        max_v = -1e10
        opt_action = -1

        for action in [0, 1, 2, 3]:
            v = estimate_value(state, action, GAMMA, V)
            if max_v < v:
                max_v = v
                opt_action = action

        PI[state] = opt_action

agent = TrainedAgent(PI)
agent.save('./policy_iter_agent.pkl')
```

- 학습된 에이전트로 시뮬레이션  

```python
import matplotlib.pyplot as plt


agent = TrainedAgent.load('./policy_iter_agent.pkl')
images = []

done = False

obs, info = env.reset()
images.append(env.render())

while not done:
    # 환경의 상태와 정보를 활용하여 행동을 선택
    action = agent.act(obs, info)

    # 선택된 행동으로 환경과 상호작용하고,
    # 그로 인해 변화된 상태와 보상 등을 획득
    obs, reward, terminated, truncated, info = env.step(action)
    images.append(env.render())

    # 종료 여부를 확인
    done = terminated or truncated

print(f'Game Clear w/ {len(images) - 1} Actions')

fig, axes = plt.subplots(len(images), 1, figsize=(5, len(images) * 5))

for i, image in enumerate(images):
    axes[i].imshow(image)
    axes[i].axis('off')

plt.show()
```

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/hv7SHMT.png
[def2]: https://i.imgur.com/JapdGRI.png
[def3]: https://i.imgur.com/foZccmr.png