---
layout: post
title: "[인공지능] 2주차 (2) - Multi-Armed Bandits"
excerpt: "Multi-Armed Bandits, Action-Value Function, Exploration Strategies"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-11
last_modified_at: 2025-09-11
---
## Multi-Armed Bandits
### 1. Multi-Armed Bandits
- 여러 대의 슬롯 머신 중 최대의 이익을 얻을 수 있는 슬롯 머신을 선택하는 문제  
=> 여러 행동(Action)들 중 최대의 보상(Reward)을 얻을 수 있는 행동을 선택하는 최적의 정책(Policy)을 학습하는 문제  

<br>

- Reinforcement Learning vs. Multi-Armed Bandits
![rl_vs_mab][def]  
  - 일반적인 강화 학습과 달리 시간에 따라 변화하는 상태를 고려하지 않음
  - 상태가 고정되어 있으므로, 현 시점부터 미래까지 받을 수 있는 보상의 누적 합인 수익(Return) 대신, **현 시점의 보상**만을 고려
  - 상태가 고려되지 않으므로, 보상의 양은 행동에만 의존적

<br>

- Optimal Policy?
  - 보상을 가장 많이 주는 행동을 선택
  - 따라서 각 행동에 대한 보상의 양을 알아야 함
  - 그러나 일반적으로 각 행동에 대한 보상의 양을 정확히 알 수 없음
  - 각 행동을 했을 때 받을 수 있는 **보상의 기대값**(평균값)을 추정

<br>

### 2. Action-Value Function
![action_value_function_1][def2]  
- 보상의 기대값으로 평가한 행동의 가치

![action_value_function_2][def3]
- 현재 타임 스텝 `t` 전까지 수행했던 행동으로 얻은 보상을 평균내어 보상의 기대값을 추정
- 행동에 대한 보상을 무한번 관측한다면 추정치가 실제 기대값에 수렴  
![action_value_function_3][def4]  

<br>

- Simple Bandit Algorithm  
![simple_bandit_algorithm][def5]  

<br>

- Incremental Implementation
  - 모든 행동 및 보상 정보를 저장하면 간단히 가치 함수의 값을 추정할 수 있으나, 메모리 사용량이 많아짐
  - Instead, 재귀 관계를 활용하여 행동 당 변수 두 개로 가치 함수의 값을 추정
  - 행동 하나에 대하여,
    - `R`<sub>`i`</sub> : 주어진 행동을 `i`번째 수행했을 때 얻은 보상의 양
    - `Q`<sub>`n`</sub> : 주어진 행동을 `n-1`번 수행했을 때 추정한 행동 가치 함수의 값  
    ![incremental_implementation_1][def6]  
    ![incremental_implementation_2][def7]  

    <br>

- Non-Stationary Problem
  - 지금까지는 보상의 확률 분포(기대값)가 고정되어 있다고 가정  
  ![non_stationary_problem_1][def8]  
  - 그러나 보상의 확률 분포(기대값)가 시간에 따라 달라질 수 있음  
  ![non_stationary_problem_2][def9]  
  - 그렇다면 과거에 관측된 보상이 상대적으로 부정확하다고 여기는 것이 합리적  

- Exponential Recency-Weighted Average
  - 과거에 관측된 보상에 더 적은 가중치를 부여  
  ![exponential_recency_weighted_average_1][def10]  
  ![exponential_recency_weighted_average_2][def11]  
  ![exponential_recency_weighted_average_3][def12]  
  - `α` 값이 클 수록 과거 시점에서 얻은 보상의 영향을 감소시킴  

- Update Rule  
![update_rule][def13]  

<br>

- General Bandit Algorithm  
![general_bandit_algorithm][def14]  

### 3. Exploration Strategies
- Greedy Policy  
![argmax][def15]  
  - 단순하게 생각해보면, 가치 함수의 값(보상의 기대값)이 가장 높은 행동을 선택하는 것이 보상을 최대화할 수 있는 최적의 정책  

- Greedy Policy Pseudo Code  
![pseudo_code][def16]  

- 10-Arm Testbed w/ Greedy Policy  
![10_arm_testbed_greedy_policy][def17]  
  - 2000회 행동 선택 결과  
  ![10_arm_testbed_greedy_policy_result][def18]  
    - `10`번 Action을 한 번도 선택하지 않았다. (No exploration)  

    <br>

- Exploitation vs. Exploration
  - 가치 함수의 값을 정확히 추정하기 위해서는, 모든 행동에 대한 보상을 충분히 관측해야 함
  - 현재 시점에서 최적이 아닌 행동 또한 수행하여 보상을 관측
  - 최적의 행동 만을 선택하는 것은 모든 행동에 대한 보상을 제대로 알지 못한 채 부정확하게 추정된 가치 함수의 값에 근거하는 정책
  - 착취와 탐험을 적절히 조화시켜야 함

<br>

- ε-Greedy Policy  
![epsilon_greedy_policy][def19]  
  - 확률 `ε`로 무작위 행동을 선택하여 탐험
  - 확률 `1-ε`로 가치 함수의 값이 가장 높은 행동을 선택하여 착취  

- ε-Greedy Policy Pseudo Code  
![epsilon_greedy_policy_pseudo_code][def20]  

<br>

- Explore-then-Commit Intuition
  - ε-Greedy는 시간이 지나도 무조건 일정한 확률로 Exploration을 함
  - 초반에만 일정 횟수 정도 탐험을 하고, 이후에는 착취만 하는 것이 더 나을 수도 있음  

- Explore-then-Commit Pseudo Code  
![explore_then_commit_pseudo_code][def21]  

<br>

- Optimistic Initial Value Intuition
  - 환경에 대한 사전 지식으로 가치 함수 값의 대략적인 최댓값을 알고 있다고 가정
  - 가치 함수의 추정치 `Q`의 초기 값을 `q`의 대략적인 최댓값보다 충분히 큰 낙관적인 값(Optimistic Initial Value)으로 설정?
    - 한 번 선택된 행동은 초기 값보다 작아짐
    - 따라서 선택되지 않은 행동이 다음 번에 선택될 확률이 높아짐  

- Optimistic Initial Value Pseudo Code  
![optimistic_initial_value_pseudo_code][def22]  

<br>

- Upper Confidence Bound (UCB) Intuition
  - Explore-then-Commit과 Optimistic Initial Value는 간단하지만, 휴리스틱과 환경에 대한 사전 지식이 필요
  - 또한 현재 추정 가치 함수와 상관 없이 모든 행동을 골고루 탐험하므로, 많은 탐험이 필요
  - 현재까지 추정한 가치 함수를 고려하면서 탐험 하려면?  
  ![upper_confidence_bound_intuition_1][def23]  
  - 가치 함수의 값과 더불어 행동을 수행한 횟수 `N(a)`도 고려  
  - `N(a)`가 작을 수록 불확실성이 높음 -> 실제 기대값과 차이가 클 수 있음
  - `N(a)`가 클 수록 불확실성이 낮음 -> 실제 기대값과 유사  

- Upper Confidence Bound (UCB) Pseudo Code  
![upper_confidence_bound_pseudo_code][def24]  

<br>

- Gradient Bandit Intuition  
![gradient_bandit_intuition_1][def25]  
  - 각 행동 별로 가치 함수 `Q(a)` 대신, 보상과는 상관없는 선호도 `H(a)`를 고려한 Softmax 확률 분포를 활용 가능  
    - 선호도가 높은 행동은 선택될 확률이 높음
    - 선호도가 낮은 행동은 선택될 확률이 낮음  

- Update Rule  
![update_rule_gradient_bandit][def26]  
  - 빨간 네모는 현재까지 얻은 보상들의 평균
  - 행동에 대한 보상이 높을 수록 선호도가 커지도록 선호도를 학습  
  - 얻은 보상 > 평균 보상
    - 선택된 행동의 선호도가 증가
    - 선택되지 않은 행동의 선호도는 감소
  - 얻은 보상 < 평균 보상
    - 선택된 행동의 선호도가 감소
    - 선택되지 않은 행동의 선호도는 증가  

- Gradient Bandit Pseudo Code  
![gradient_bandit_pseudo_code][def27]  

- Derivation  
  - 목적 : 보상의 기대값 `E[R]`를 최대화  
  ![derivation_1][def28]  
  - 목적 : 보상의 기대값 `E[R]`를 최대화할 수 있는 선호도 `H(a)`를 학습  
  ![derivation_2][def29]  

- Gradient Ascent/Descent  
![gradient_ascent_descent][def30]  
  - 매개변수 `w`에 대한 미부누가능한(Differentiable) 함수 `J(w)`의 값을 최대화/최소화하는 최적의 매개변수 `w*`를 찾는 최적화방법  
  ![gradient_ascent_descent_2][def31]  
    - (1) `k`번째 업데이트 때, 주어진 매개변수 `w`<sub>`k`</sub>에 대한 함수 `J(w)`의 기울기(Gradient)를 계산
    - (2) 구한 기울기만큼 `w`<sub>`k`</sub>를 감소/증가
    - (3) (1), (2)를 반복하여 최적의 매개변수 `w*`를 찾음  

- Derivation (Cont'd)  
  - 목적 : 보상의 기대값 `E[R]`를 최대화할 수 있는 선호도 `H(a)`를 학습  
  ![derivation_2][def32]  
  - `E[R]`은 `H(a)`에 대한 함수이고 미분 가능하므로, Gradient Ascent 적용 가능  
  ![derivation_3][def33]  

- Derivation (Cont'd)  
![derivation_4][def34]  
![derivation_5][def35]  
![derivation_6][def36]  
![derivation_7][def37]
  - 참인 보상의 기대값 `q*(a')`를 알 수 없으므로, 위 식을 계산할 수 없음
  - Instead, 현재 시점 `t`에서 실제로 행동을 하나 선택하여 수행한 결과로 얻은 실제 보상의 양 `R`<sub>`t`</sub>를 활용  
  - 행동이 실제로 선택되면 외부 기대값의 확률 분포 `a'` ~ `π(t)`또한 의미가 없음  
  ![derivation_8][def38]  

<br>

- Comparison of Exploration Strategies  
![comparison_of_exploration_strategies_1][def39]  
![comparison_of_exploration_strategies_2][def40]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/7v10Rug.png
[def2]: https://i.imgur.com/Eq0A9ff.png
[def3]: https://i.imgur.com/o7PUmHn.png
[def4]: https://i.imgur.com/hutVsi5.png
[def5]: https://i.imgur.com/NkYr6gz.png
[def6]: https://i.imgur.com/15mi4OH.png
[def7]: https://i.imgur.com/uCGt6aH.png
[def8]: https://i.imgur.com/pVwzY3k.png
[def9]: https://i.imgur.com/gUoLpbw.png
[def10]: https://i.imgur.com/J9RW4ES.png
[def11]: https://i.imgur.com/KJe0ECx.png
[def12]: https://i.imgur.com/FVMp2F9.png
[def13]: https://i.imgur.com/Z4FZZwb.png
[def14]: https://i.imgur.com/bSIDmNz.png
[def15]: https://i.imgur.com/Es140zD.png
[def16]: https://i.imgur.com/fCoQVl7.png
[def17]: https://i.imgur.com/cT0RT9f.png
[def18]: https://i.imgur.com/IV53TgV.png
[def19]: https://i.imgur.com/vp78TYN.png
[def20]: https://i.imgur.com/M0OOM2a.png
[def21]: https://i.imgur.com/hnDr5Lj.png
[def22]: https://i.imgur.com/OXaMFFM.png
[def23]: https://i.imgur.com/VnTnrKe.png
[def24]: https://i.imgur.com/Kzxk7rN.png
[def25]: https://i.imgur.com/0K8f1i7.png
[def26]: https://i.imgur.com/qiVvLtF.png
[def27]: https://i.imgur.com/h8b6vyz.png
[def28]: https://i.imgur.com/eFP8cI9.png
[def29]: https://i.imgur.com/J81h8QC.png
[def30]: https://i.imgur.com/CysMa8n.png
[def31]: https://i.imgur.com/iFRUviZ.png
[def32]: https://i.imgur.com/9fCYCyE.png
[def33]: https://i.imgur.com/MZwoy5a.png
[def34]: https://i.imgur.com/ygvrIrm.png
[def35]: https://i.imgur.com/wsKkf0g.png
[def36]: https://i.imgur.com/DPJWSJP.png
[def37]: https://i.imgur.com/QOqYOJH.png
[def38]: https://i.imgur.com/j3droZn.png
[def39]: https://i.imgur.com/sOMQExo.png
[def40]: https://i.imgur.com/5DOsQGl.png