---
layout: post
title: "[인공지능] 11주차 (2) - Deep Q-Network"
excerpt: "Deep Q-Network, Dealing with Divergence, Variants of Deep Q-Networks"

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-13
last_modified_at: 2025-11-13
---
## Deep Q-Network
### 1. Deep Q-Network
- Google DeepMind에서 개발한 가와 학습 알고리즘. Deep Neural Network를 Function Approximator로 활용하고, Q-Learning으로 정책을 학습

<br>

- Architecture  
![DQN_Architecture](TODO)  
  - (1) Raw Input : 210 x 160 픽셀, RGB 3채널 이미지
  - (2) Preprocessing : 84 x 84 흑백 이미지로 변환, 연속된 4 프레임의 이미지 활용
  - (3) 1st Convolutional Layer : 32개의 8 x 8 필터, 스트라이드 4, ReLU 활성화 함수. 출력은 20 x 20 x 32
  - (4) 2nd Convolutional Layer : 64개의 4 x 4 필터, 스트라이드 2, ReLU 활성화 함수. 출력은 9 x 9 x 64
  - (5) 3rd Convolutional Layer : 64개의 3 x 3 필터, 스트라이드 1, ReLU 활성화 함수. 출력은 7 x 7 x 64
  - (6) Flattening : 7 x 7 x 64 텐서를 3136차원의 벡터로 변환
  - (7) Fully Connected Layer : 512개의 퍼셉트론, ReLU 활성화 함수. 출력은 512
  - (8) Output Layer : 18개의 퍼셉트론으로 구성, Identity 활성화 함수. 각 퍼셉트론은 주어진 행동에 대한 행동 가치 함수를 출력
  - 총 파라미터 수 : 1,693,362 개

<br>

- Performance ?  
![DQN_Performance](TODO)
  - 49개의 Atari 2600 게임을 대상으로 실험
  - 43개의 게임에서 기존의 강화학습 에이전트를 능가
  - 29개의 게임에서 프로게이머를 능가 (?)  

<br>

### 2. Dealing with Divergence
- Function Approximation with On-Policy Controls  
![on_policy_function_approximation](TODO)  

- Off-Policy Temporal Difference Learning, Q-Learning  
![off_policy_td_learning](TODO)  
  - Q-Learning 또한 쉽게 Function Approximation 형태로 변환할 수 있으나, 매개변수는 그 값이 발산(divergence)하거나 수렴하지 않아서(Instability) 제대로 학습되지 않을 수 있음  

<br>

- 왜 발산하는가 ?
  - Tabular Method
    - 관측된 상태-행동에 대하여 행동 가치 함수를 업데이트  
    ![tabular_method](TODO)  
    - 관측되지 않은 상태 행동에 대해서는 행동 가치 함수의 값이 변하지 않음  
    ![tabular_method_unobserved](TODO)  

  - Function Approximation  
    - 관측된 상태-행동에 대하여 매개변수를 업데이트  
    ![function_approximation](TODO)  
    - 매개변수의 값이 업데이트 되면, 매개변수를 통해 가치 함수를 함수적으로 근사하므로, 관측되지 않은 상태-행동에 대한 가치 함수의 값 또한 변화할 수 있음  
    ![function_approximation_unobserved](TODO)  

  - Bootstrapping + Off-Policy (할인율 `γ` = `1`, 목적함수 `J(w`<sup>`(k)`</sup>`)` = Squared Loss)  
  ![bootstrapping_off_policy_1](TOOD)  
  ![bootstrapping_off_policy_2](TODO)  
  ![bootstrapping_off_policy_3](TODO)  
  ![bootstrapping_off_policy_4](TODO)  
    - 업데이트 할수록 매개변수의 값이 증가하여 수렴하지 않음  

  - Deadly Triad
    - (1) Bootstrapping : 가치함수의 참 값이 아닌 추정치를 Target으로 활용  
    - (2) Off-Policy Control : 현재 정책과 상관없는 값을 Target으로 활용  
    - (3) Function Approximation : 경험하지 않은 상태-행동에 대한 가치 함수의 값이 변화  

    - 참인 값도 아니고(1), 현재 정책과 상관도 없고(2), 심지어 지속적으로 변화하는(3) Target과 현재 정책에 대한 가치 함수의 차이를 최소화?
    - 세 조건을 모두 만족하는 경우, 매개변수의 수렴을 보장할 수 없음  

  - 발산 문제를 해결하는 Deep-Q Network의 핵심 아이디어 : Experience Replay + Fixed Target Network  

<br>

- Experience Replay
  - 일반적인 Q-Learning : 새로운 경험을 관측할 때마다 매개변수를 업데이트  
  ![q_learning_update](TODO)  

  - Deep Q-Network : 새로운 경험을 관측할 때마다 최근 N개의 경험만을 유지하는 Replay Buffer `D`에 저장하고, N개를 초과할 시 오래된 경험부터 삭제  
  ![experience_replay](TODO)  
    - 매 K회 환경과 상호작용할 때마다, Replay Buffer `D`에서 **무작위로 추출된** `B`개의 경험들(Mini-Batch)로 매개변수를 업데이트  
    ![experience_replay_update](TODO)  
    - 이를 통해 환경과 상호작용하는 순서와는 상관없이 매개변수가 업데이트  
    - 따라서 업데이트에 사용되는 경험들 간의 시간적 연관성(Temporal Correlation)을 완화  

<br>

- Function Approximation + Bootstrapping : 매개변수 업데이트마다 Target이 변화  
![function_approximation_bootstrapping](TODO)  
  - Unstable Target.  

- Fixed Target Network  
![fixed_target_network](TODO)
  - Behavior Network `Q` : 매개변수가 업데이트 되는 신경망
  - Target Network `Q`<sup>`-`</sup> : 매개변수가 업데이트 되지 않는 신경망으로 안정된 Target 반환  
  - 매 `C`회 환경과 상호작용 할 때마다 Behavior Network의 매개변수를 Target Network에 복사  

<br>

- Pseudocode  
![dqn_pseudocode](TODO)  

<br>

### 3. Variants of Deep Q-Networks
- Double Deep Q-Network  
![double_dqn](TODO)  
  - Target Network가 출력하는 행동 가치 함수의 최대값을 Target으로 활용하는 대신, Behavior Network가 선택한 행동에 대해 Target Network가 출력하는 행동 가치 함수의 값을 Target으로 사용  

<br>

- Priority of Experiences
  - Deep Q-Network는 Replay Buffer에서 이전 경험을 무작위로 경험을 추출하여 매개변수를 업데이트
  - 그러나, 어떤 경험은 다른 경험보다 더 중요하거나 중요하지 않을 수 있음
  - 경험의 중요도를 평가하는 방법은?

- Error as Priority of Experiences  
![error_as_priority_of_experiences](TODO)  
  - 가치함수는 목표값과 과거의 추정치 간의 차이를 최소화하는 것이 목적
  - 목표값과 과거의 추정치가 일치하면(손실 기울기가 `0`), 가치함수의 값은 업데이트 되지 않음  
  -> 해당 상태-행동은 업데이트의 중요도가 낮음
  - 반대로, 목표값과 과거의 추정치가 크게 차이난다면 해당 상태-행동에 대한 행동 가치 함수는 업데이트가 필요  
  -> 해당 상태-행동은 업데이트의 중요도가 높음  

- 만약 (1) 새로운 경험을 관측할 때 마다 Error를 계산하여 Replay Buffer에 저장하고,  
(2) Replay Buffer에서 Error가 높은 경험만을 추출하여 업데이트 한다면 ?  
  - 문제 1 : 과도한 연산량이 요구됨.  
    - Sol. 새로운 경험에 대해서는 Replay Buffer에 저장된 Error의 최대치를 우선순위로 활용하고, Replay Buffer에서 경험이 추출될 때만 Error를 새로 계산  
    ![prioritized_experience_replay](TODO)  

  - 문제 2 : 현재 추정된 Error가 낮은 경험이 무조건 불필요하다고 할 수는 없음  
    - Sol. Error가 높은 경험을 무조건 추출하는 대신, 높은 확률로 추출  
    ![stochastic_sampling](TODO)   

  - 문제 3 : Gradient Descent로 업데이트 되는 매개변수의 크기는 굉장히 작아서, Error가 극적으로 감소하지 않음. 따라서 Error가 크게 추정된 경험이 계속해서 우선적으로 추출될 수 있음  
    - Sol. 추출될 확률이 높은 경험이 매개변수 업데이트에 미치는 영향을 조정  
    ![importance_sampling_weight](TODO)  
    - e.g.,  
    ![importance_sampling_example](TODO)  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 