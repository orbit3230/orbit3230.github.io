---
layout: post
title: "[인공지능] 7주차 - n-Step Bootstrapping"
excerpt: "n-Step Temporal Difference Prediction,, n-Step Temporal Difference Control"

tags:
  - [인공지능, Python]

toc: true

date: 2025-10-16
last_modified_at: 2025-10-16
---
## n-Step Bootstrapping
### 1. n-Step Temporal Difference Prediction
- 표본 추출 (Sampling)  
![sampling][def]
  - **모집단**의 확률 분포를 추정하기 위하여 모집단의 **부분 집합인 표본**을 반복적으로 추출하는 것
  - e.g., 정규 분포를 따르는 모집단의 평균을 표본 추출을 통해 추정 (표본 평균의 평균 = 모평균)  
    - 모집단의 확률 분포 : `N(μ, σ²)`
    - 표본 평균의 확률 분포 : `N(μ, σ²/n)` (`n`은 표본 추출 횟수)  

- 재표집 (Resampling or Bootstrapping)
  - **표본 집단**의 확률 분포를 추정하기 위하여 **표본 집단의 재표본**을 반복적으로 추출

<br>

- Monte Carlo Methods : 현재 정책과 상태에 대한 참인 수익(= 모집단) 의 분포에서 **실제로 얻은 수익(= 표본)** 을 추출하여 참인 가치 함수를 추정

- Temporal Difference Learning : 현재 정책과 상태에 대한 참인 수익(= 모집단) 의 분포에서 **전이된 다음 상태에 대한 수익(= 표본) 의 추정치(= 재표본)** 를 추출하여 참인 가치 함수를 추정

<br>

- n-Step in Temporal Difference Learning
  - 현재 상태의 가치 함수를 업데이트 하기 위하여, 1회 행동으로 실제로 얻는 보상 + 1회 행동 후 전이된 다음 상태에 대한 가치 함수를 활용  
  ![1_step_td][def2]

  - More generally, 현재 상태의 가치 함수를 업데이트 하기 위하여, n회 행동으로 실제로 얻는 보상 + n회 행동 후 전이된 다음 상태에 대한 가치 함수를 활용

<br>

- n-Step Return  
![n_step_return][def3]  

<br>

- Update Rule of n-Step TD Prediction  
![n_step_td_update][def4]  
  - `t` 시점의 상태 `S`<sub>`t`</sub>의 가치 함수를 업데이트하기 위해, `t + n` 시점까지 관측한 보상 `R`<sub>`t+1`</sub>, `R`<sub>`t+2`</sub>, ..., `R`<sub>`t+n`</sub>과 `t + n` 시점에서 전이한 상태 `S`<sub>`t+n`</sub>의 가치 함수를 활용  

<br>

- Example of 3-Step TD Prediction  
![3_step_td_example_1][def5]  
![3_step_td_example_2][def6]  
![3_step_td_example_3][def7]  
![3_step_td_example_4][def8]  
![3_step_td_example_5][def9]  
![3_step_td_example_6][def10]  
![3_step_td_example_7][def11]  
![3_step_td_example_8][def12]  
![3_step_td_example_9][def13]  

<br>

- 구현 방법?
  - 시점 `t`의 상태 `S`<sub>`t`</sub>의 가치 함수 업데이트는 시점 `t + n` 까지 상호작용을 하여 보상 `R`<sub>`t+n`</sub>, 상태 `S`<sub>`t+n`</sub>을 관측한 이후에 이루어짐.  
    - e.g., 3-Step TD Prediction  
    ![3_step_td_example_implementation][def14]  

  - 반대로 시점 `t`의 상태 `S`<sub>`t`</sub>에서 행동을 수행한 후 보상 `R`<sub>`t+1`</sub>과 상태 `S`<sub>`t+1`</sub>을 관측했다면, 시점 `t - n + 1`의 상태 `S`<sub>`t-n+1`</sub>의 가치 함수를 업데이트할 수 있음.  
    - e.g., 3-Step TD Prediction  
    ![3_step_td_example_implementation_2][def15]  

  - 시점 `t`에서 업데이트 가능한 상태 `S`<sub>`t-n+1`</sub>(or `S`<sub>`τ`</sub>) 기준으로 n-Step TD Prediction의 Update Rule을 재정의  
  ![n_step_td_update_redefined][def16]  

<br>

### 2. n-Step Temporal Difference Control
- State-Value -> Action-Value
  - Monte Carlo Methods나 Temporal Difference Learning처럼 상태 가치 함수 대신 행동 가치 함수를 활용하고, n-Step Return을 Target으로 설정  
  ![n_step_td_control][def17]  

<br>

- n-Step SARSA  
![n_step_sarsa_update][def18]  
  - 1-Step Sarsa : `t + 1` 시점의 상태 `S`<sub>`t+1`</sub>에서 현재 정책으로 선택한 행동 `A`<sub>`t+1`</sub>에 대한 행동 가치 함수를 업데이트에 활용  
  - n-Step Sarsa : `t + n` 시점의 상태 `S`<sub>`t+n`</sub>에서 현재 정책으로 선택한 행동 `A`<sub>`t+n`</sub>에 대한 행동 가치 함수를 업데이트에 활용  

- n-Step Sarsa: Pseudocode  
![n_step_sarsa_pseudocode][def19]  

<br>

- Expected n-Step SARSA  
![expected_n_step_sarsa_update][def20]  
  - 1-Step Expected Sarsa : `t + 1` 시점에서 얻은 보상 `R`<sub>`t+1`</sub>과 상태 `S`<sub>`t+1`</sub>에서 가능한 모든 행동에 대한 행동 가치 함수의 기대값을 활용  
  - n-Step Expected Sarsa : `t + 1`, `t + 2` , ..., `t + n` 시점에서 얻은 보상들과 상태 `S`<sub>`t+n`</sub>에서 가능한 모든 행동에 대한 행동 가치 함수의 기대값을 활용  
    - 단, 1-Step Expected Sarsa와 다르게, 현재 정책을 따라 `n`회 행동을 하며 보상을 얻었으므로 완전한 Off-Policy Method 라고 하기 어려움  

<br>

- Off-Policy n-Step SARSA  
![off_policy_n_step_sarsa_update][def21]  
  - Target Policy `π`와 Behavior Policy `b`를 활용
  - `t + 1`, `t + 2` , ..., `t + n` 시점까지 Behavior Policy가 얻은 n-Step Return을 IS Ratio를 활용해 Target Policy에 대한 수익의 기대값으로 변환  

<br>

- Off-Policy n-Step Sarssa vs. Off-Policy MC  
  - Off-Policy Monte Carlo Control  
  ![off_policy_mc_update][def22]  
  - Off-Policy n-Step SARSA  
  ![off_policy_n_step_sarsa_update_redefined][def23]  
   - 종료 시점까지 고려하는 MC의 IS Ratio보다 `t + n` 시점까지만 고려하는 n-Step SARSA의 IS Ratio가 훨씬 작으므로 적당한 `α` 값으로도 IS Ratio가 지나치게 커지지 않도록 보정할 수 있음  

<br>

- Off-Policy n-Step SARSA: Pseudocode  
![off_policy_n_step_sarsa_pseudocode][def24]  

<br>

- Off-Policy n-Step Expected SARSA  
![off_policy_n_step_expected_sarsa_update][def25]  
  - `t + 1`, `t + 2` , ..., `t + n` 시점까지 Behavior Policy가 얻은 n-Step Return과 `t + n` 시점의 상태 `S`<sub>`t+n`</sub>에서 가능한 모든 행동에 대한 가치 함수의 기대값을 모두 활용  

<br>

- Backup Diagrams  
![backup_diagrams][def26]  

<br>

- Behavior Policy(= Importance Sampling) 없이 Off-Policy를 달성하는 방법은? -> n-Step Tree Backup  

- n-Step Tree Backup
  - 1-Step Expected Sarsa를 확장
  - Target Policy를 따라 행동을 선택하되, Target Policy로 선택되지 않은 행동에 대한 가치 함수의 기대값을 함께 활용  
  ![n_step_tree_backup_1][def27]  
  ![n_step_tree_backup_2][def28]  
  ![n_step_tree_backup_3][def29]  

- n-Step Tree Backup: Pseudocode  
![n_step_tree_backup_pseudocode][def30]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/S69mBIs.png
[def2]: https://i.imgur.com/xS2CK6s.png
[def3]: https://i.imgur.com/qQJVgwR.png
[def4]: https://i.imgur.com/KNAjSNY.png
[def5]: https://i.imgur.com/sxe4OGK.png
[def6]: https://i.imgur.com/lsOSrd4.png
[def7]: https://i.imgur.com/Io0Ei61.png
[def8]: https://i.imgur.com/O0JvbFj.png
[def9]: https://i.imgur.com/83XMBoC.png
[def10]: https://i.imgur.com/CLKADTc.png
[def11]: https://i.imgur.com/lGi62Ap.png
[def12]: https://i.imgur.com/Q5leUyM.png
[def13]: https://i.imgur.com/EElmM0I.png
[def14]: https://i.imgur.com/CbfpfWU.png
[def15]: https://i.imgur.com/Qte5aXZ.png
[def16]: https://i.imgur.com/wmFPiEY.png
[def17]: https://i.imgur.com/4CBbNgi.png
[def18]: https://i.imgur.com/j3XA2tj.png
[def19]: https://i.imgur.com/sMtkIWi.png
[def20]: https://i.imgur.com/9vRDod6.png
[def21]: https://i.imgur.com/9a55zf7.png
[def22]: https://i.imgur.com/8uMjfhG.png
[def23]: https://i.imgur.com/QirEW6E.png
[def24]: https://i.imgur.com/OKJFcrY.png
[def25]: https://i.imgur.com/Gfyqs8w.png
[def26]: https://i.imgur.com/UXwhqIB.png
[def27]: https://i.imgur.com/WZpHHUW.png
[def28]: https://i.imgur.com/LFNQ4WG.png
[def29]: https://i.imgur.com/muD8UqK.png
[def30]: https://i.imgur.com/PCcVbaM.png