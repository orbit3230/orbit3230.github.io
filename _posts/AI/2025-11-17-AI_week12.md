---
layout: post
title: "[인공지능] 12주차 - Policy Gradient Methods"
excerpt: ""

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-17
last_modified_at: 2025-11-17
---
## Policy Gradient Methods
### 1. Policy Gradient Methods
- 지금까지 배웠던 방법들
  - Tabular Q-Learning : 현재 상태에서 테이블에 저장된 행동 가치 함수를 참조하여 행동 선택, 수행 후 관측된 보상과 다음 상태에 따라 행동 가치 함수 업데이트
  - Sarsa with Function Approximation : 현재 상태 및 행동엥 대해 함수적으로 근사한 행동 가치 함수를 참조하여 행동 선택, 수행 후 관측된 보상과 다음 상태에 따라 행동 가치 함수의 매개변수 업데이트  
  - Deep Q-Network : 현재 상태 및 행동에 대해 함수적으로 근사한 행동 가치 함수를 참조하여 행동 선택, Replay Buffer에 저장된 경험을 추출하여 행동 가치 함수의 매개변수 업데이트

- 즉, 지금까지는 매개변수 `w`를 통해 행동 가치 함수를 근사  
![until_now_action_value_function](TODO)  
- 그리고 행동 가치 함수를 고려한 정책으로 행동 선택  
![until_now_policy](TODO)  

<br>

- Policy Gradient Methods
  - 매개변수 `θ`를 통해 정책을 직접 근사  
  ![policy_gradient_method](TODO)  

  - 정책으로 행동을 선택.  
  ![policy_action_selection](TODO)  
    - 확률적 정책을 매개변수를 통해 직접 학습하므로, `ε`-greedy가 불필요  

<br>

- How to Parameterize Policy?
  - Value Function Approximation
    - 주어진 상태에 대해 각 행동에 대한 행동 가치 함수를 출력
    - 행동 가치 함수를 바탕으로 행동을 선택  
    ![value_function_approximation](TODO)  

  - Policy Gradient Methods for Discrete Action Space  
    - 주어진 상태에 대해 각 행동을 할 확률을 (주로 softmax 함수로) 출력
    - e.g., 인공 신경망의 출력 레이어 활성 함수로 Softmax를 활용  
    ![policy_gradient_discrete_action_space](TODO)  

  - Policy Gradient Methods for Continuous Action Space
    - 주어진 상태에 대해 연속적인 행동 공간에 대한 확률 분포(또는 확률 분포를 구성하는 매개변수)를 출력  
    ![policy_gradient_continuous_action_space](TODO)  

<br>

- How to Learn Parameters?
  - Value Function Approximation
    - 매개변수 `w`에 대해 Target과 행동 가치 함수의 차이를 측정하는 목적함수 `J(w)`를 정의  
    ![value_function_objective_function](TODO)  
    - 목적함수의 값을 최소화하기 위해, 경사 하강법으로 손실 기울기를 현재 매개변수에 차감  
    ![value_function_gradient_descent](TODO)  

  - Policy Gradient Methods
    - 매개변수 `θ`에 대해, 시작 상태에서 종료 상태까지 얻을 수 있는 (할인된) 보상의 총 합(= 수익)을 측정하는 목적함수 `J(θ)`를 정의  
    ![policy_gradient_objective_function](TODO)  
    - 목적함수의 값을 최대화하기 위해, 경사 상승법으로 정책 기울기를 현재 매개변수 `θ`<sup>`(k)`</sup>에 더하여 업데이트  
    ![policy_gradient_ascent](TODO)  

<br>

- How to Calculate Policy Gradient?  
![policy_gradient_calculation_1](TODO)  
![policy_gradient_calculation_2](TODO)  
![policy_gradient_calculation_3](TODO)  
![policy_gradient_calculation_4](TODO)  
![policy_gradient_calculation_5](TODO)  

- Policy Gradient Theorem  
![policy_gradient_theorem](TODO)  

<br>

### 2. Policy Gradient
- Action Value Function -> Actual Returns  
![policy_gradient_action_value_function_to_actual_returns_1](TODO)  
![policy_gradient_action_value_function_to_actual_returns_2](TODO)  

<br>

- REINFORCE, Monte-Carlo Policy Gradient  
![REINFORCE_1](TODO)  
  - Policy Gradient Theorem의 행동 가치 함수 대신, 실제로 얻은 수익을 활용  
  - 에피소드가 종료되어야 수익을 알 수 있으므로, Monte Carlo Control  

- REINFORCE: Pseudocode  
![REINFORCE_pseudocode](TODO)  

<br>

- Speed & Direction in Update Rules  
![speed_and_direction_in_update_rules](TODO)  
  - 수익 `G`<sub>`t`</sub>는 에피소드마다 천차만별이며, 그 값 또한 굉장히 커지거나 작아질 수 있음
  - 매개변수의 업데이트가 불안정
  - 매개변수의 업데이트를 안정화시키면서도 Policy Gradient Theorem을 만족시키는 방법이 필요  

<br>

- Deducting State-Dependent Function  
![deducting_state_dependent_function](TODO)  

<br>

- REINFORCE with Baseline  
![REINFORCE_with_Baseline](TODO)  
  - 상태에만 의존적인 함수인 Baseline을 차감하여 Policy Gradient를 계산  
  - Policy Gradient Theorem을 만족하므로, 매개변수로 표현된 정책을 최적화할 수 있음
  - 수익에서 일정한 값을 차감하여 지나치게 커지거나 작아지지 않도록 보정했으므로, 매개변수의 업데이트가 안정화될 수 있음  

- REINFORCE with Baseline: Good Baseline  
![REINFORCE_with_Baseline_good_baseline](TODO)  
  - 수익의 값을 보정하여 매개변수의 업데이트를 안정화시키는 것이 목적
  - 직관적으로, 주어진 상태에서 실제로 얻어낸 수익을 평균낸 값이 좋은 Baseline이 될 수 있음
  - 상태에 의존적인 함수 + 수익의 평균 (즉, 기대값) = 상태 가치 함수  

<br>

### 3. Actor-Critic Methods
- Valid Gradients  
![valid_gradients](TODO)  

- Gradients with Value Function  
![gradients_with_value_function](TODO)  

- Policy Gradient + Value Function Approximation  
![policy_gradient_value_function_approximation](TODO)  

- Actor-Critic Methods  
![actor_critic_methods](TODO)  
  - 빨간색 부분 : 매개변수 `θ`로 표현된 정책으로 행동을 선택하는 Actor
  - 파란색 부분 : Actor가 가진 정책의 가치를 (매개변수 `w`를 통해 근사하여) 평가하는 Critic  

- Actor-Critic Methods: Update Rules  
  - Actor  
  ![actor_update_rule](TODO)  

  - Critic  
  ![critic_update_rule](TODO)  

<br>

- MC Actor-Critic (즉, REINFORCE with Baseline) Pseudocode  
![MC_Actor_Critic_pseudocode](TODO)  

- 1-Step Action-Value Actor-Critic Pseudocode  
![1_Step_Action_Value_Actor_Critic_pseudocode](TODO)  

- 1-Step State-Value Actor-Critic Pseudocode  
![1_Step_State_Value_Actor_Critic_pseudocode](TODO)  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 