---
layout: post
title: "[인공지능] 12주차 - Policy Gradient Methods"
excerpt: "Policy Gradient Methods, Policy Gradient, Actor-Critic Methods"

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-17
last_modified_at: 2025-11-17
---
## Policy Gradient Methods
### 1. Policy Gradient Methods
- 지금까지 배웠던 방법들
  - Tabular Q-Learning : 현재 상태에서 테이블에 저장된 행동 가치 함수를 참조하여 행동 선택, 수행 후 관측된 보상과 다음 상태에 따라 행동 가치 함수 업데이트
  - Sarsa with Function Approximation : 현재 상태 및 행동엥 대해 함수적으로 근사한 행동 가치 함수를 참조하여 행동 선택, 수행 후 관측된 보상과 다음 상태에 따라 행동 가치 함수의 매개변수 업데이트  
  - Deep Q-Network : 현재 상태 및 행동에 대해 함수적으로 근사한 행동 가치 함수를 참조하여 행동 선택, Replay Buffer에 저장된 경험을 추출하여 행동 가치 함수의 매개변수 업데이트

- 즉, 지금까지는 매개변수 `w`를 통해 행동 가치 함수를 근사  
![until_now_action_value_function][def]  
- 그리고 행동 가치 함수를 고려한 정책으로 행동 선택  
![until_now_policy][def2]  

<br>

- Policy Gradient Methods
  - 매개변수 `θ`를 통해 정책을 직접 근사  
  ![policy_gradient_method][def3]  

  - 정책으로 행동을 선택.  
  ![policy_action_selection][def4]  
    - 확률적 정책을 매개변수를 통해 직접 학습하므로, `ε`-greedy가 불필요  

<br>

- How to Parameterize Policy?
  - Value Function Approximation
    - 주어진 상태에 대해 각 행동에 대한 행동 가치 함수를 출력
    - 행동 가치 함수를 바탕으로 행동을 선택  
    ![value_function_approximation][def5]  

  - Policy Gradient Methods for Discrete Action Space  
    - 주어진 상태에 대해 각 행동을 할 확률을 (주로 softmax 함수로) 출력
    - e.g., 인공 신경망의 출력 레이어 활성 함수로 Softmax를 활용  
    ![policy_gradient_discrete_action_space][def6]  

  - Policy Gradient Methods for Continuous Action Space
    - 주어진 상태에 대해 연속적인 행동 공간에 대한 확률 분포(또는 확률 분포를 구성하는 매개변수)를 출력  
    ![policy_gradient_continuous_action_space][def7]  

<br>

- How to Learn Parameters?
  - Value Function Approximation
    - 매개변수 `w`에 대해 Target과 행동 가치 함수의 차이를 측정하는 목적함수 `J(w)`를 정의  
    ![value_function_objective_function][def8]  
    - 목적함수의 값을 최소화하기 위해, 경사 하강법으로 손실 기울기를 현재 매개변수에 차감  
    ![value_function_gradient_descent][def9]  

  - Policy Gradient Methods
    - 매개변수 `θ`에 대해, 시작 상태에서 종료 상태까지 얻을 수 있는 (할인된) 보상의 총 합(= 수익)을 측정하는 목적함수 `J(θ)`를 정의  
    ![policy_gradient_objective_function][def10]  
    - 목적함수의 값을 최대화하기 위해, 경사 상승법으로 정책 기울기를 현재 매개변수 `θ`<sup>`(k)`</sup>에 더하여 업데이트  
    ![policy_gradient_ascent][def11]  

<br>

- How to Calculate Policy Gradient?  
![policy_gradient_calculation_1][def12]  
![policy_gradient_calculation_2][def13]  
![policy_gradient_calculation_3][def14]  
![policy_gradient_calculation_4][def15]  
![policy_gradient_calculation_5][def16]  

- Policy Gradient Theorem  
![policy_gradient_theorem][def17]  

<br>

### 2. Policy Gradient
- Action Value Function -> Actual Returns  
![policy_gradient_action_value_function_to_actual_returns_1][def18]  
![policy_gradient_action_value_function_to_actual_returns_2][def19]  

<br>

- REINFORCE, Monte-Carlo Policy Gradient  
![REINFORCE_1][def20]  
  - Policy Gradient Theorem의 행동 가치 함수 대신, 실제로 얻은 수익을 활용  
  - 에피소드가 종료되어야 수익을 알 수 있으므로, Monte Carlo Control  

- REINFORCE: Pseudocode  
![REINFORCE_pseudocode][def21]  

<br>

- Speed & Direction in Update Rules  
![speed_and_direction_in_update_rules][def22]  
  - 수익 `G`<sub>`t`</sub>는 에피소드마다 천차만별이며, 그 값 또한 굉장히 커지거나 작아질 수 있음
  - 매개변수의 업데이트가 불안정
  - 매개변수의 업데이트를 안정화시키면서도 Policy Gradient Theorem을 만족시키는 방법이 필요  

<br>

- Deducting State-Dependent Function  
![deducting_state_dependent_function][def23]  

<br>

- REINFORCE with Baseline  
![REINFORCE_with_Baseline][def24]  
  - 상태에만 의존적인 함수인 Baseline을 차감하여 Policy Gradient를 계산  
  - Policy Gradient Theorem을 만족하므로, 매개변수로 표현된 정책을 최적화할 수 있음
  - 수익에서 일정한 값을 차감하여 지나치게 커지거나 작아지지 않도록 보정했으므로, 매개변수의 업데이트가 안정화될 수 있음  

- REINFORCE with Baseline: Good Baseline  
![REINFORCE_with_Baseline_good_baseline][def25]  
  - 수익의 값을 보정하여 매개변수의 업데이트를 안정화시키는 것이 목적
  - 직관적으로, 주어진 상태에서 실제로 얻어낸 수익을 평균낸 값이 좋은 Baseline이 될 수 있음
  - 상태에 의존적인 함수 + 수익의 평균 (즉, 기대값) = 상태 가치 함수  

<br>

### 3. Actor-Critic Methods
- Valid Gradients  
![valid_gradients][def26]  

- Gradients with Value Function  
![gradients_with_value_function][def27]  

- Policy Gradient + Value Function Approximation  
![policy_gradient_value_function_approximation][def28]  

- Actor-Critic Methods  
![actor_critic_methods][def29]  
  - 빨간색 부분 : 매개변수 `θ`로 표현된 정책으로 행동을 선택하는 Actor
  - 파란색 부분 : Actor가 가진 정책의 가치를 (매개변수 `w`를 통해 근사하여) 평가하는 Critic  

- Actor-Critic Methods: Update Rules  
  - Actor  
  ![actor_update_rule][def30]  

  - Critic  
  ![critic_update_rule][def31]  

<br>

- MC Actor-Critic (즉, REINFORCE with Baseline) Pseudocode  
![MC_Actor_Critic_pseudocode][def32]  

- 1-Step Action-Value Actor-Critic Pseudocode  
![1_Step_Action_Value_Actor_Critic_pseudocode][def33]  

- 1-Step State-Value Actor-Critic Pseudocode  
![1_Step_State_Value_Actor_Critic_pseudocode][def34]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/1RQwnOJ.png
[def2]: https://i.imgur.com/8tSI0Kh.png
[def3]: https://i.imgur.com/BhKh8Kn.png
[def4]: https://i.imgur.com/utIHmce.png
[def5]: https://i.imgur.com/lf7XK3K.png
[def6]: https://i.imgur.com/bFsjDA4.png
[def7]: https://i.imgur.com/5y3JBVc.png
[def8]: https://i.imgur.com/YAaGrig.png
[def9]: https://i.imgur.com/9aZRNqD.png
[def10]: https://i.imgur.com/wPL6Ejx.png
[def11]: https://i.imgur.com/SUX0uzZ.png
[def12]: https://i.imgur.com/CHn3VTz.png
[def13]: https://i.imgur.com/0oZ0CWf.png
[def14]: https://i.imgur.com/C4O4sSf.png
[def15]: https://i.imgur.com/OoBsebh.png
[def16]: https://i.imgur.com/idHdss8.png
[def17]: https://i.imgur.com/6ksaD2h.png
[def18]: https://i.imgur.com/PPnZDwD.png
[def19]: https://i.imgur.com/bJXeKmv.png
[def20]: https://i.imgur.com/VYLVc1i.png
[def21]: https://i.imgur.com/GpM8IWU.png
[def22]: https://i.imgur.com/11lJSSN.png
[def23]: https://i.imgur.com/P8ydjfQ.png
[def24]: https://i.imgur.com/36JQUzo.png
[def25]: https://i.imgur.com/QKLrDXq.png
[def26]: https://i.imgur.com/hwhaS81.png
[def27]: https://i.imgur.com/Do4ChBp.png
[def28]: https://i.imgur.com/5sDCdOe.png
[def29]: https://i.imgur.com/qngkg9K.png
[def30]: https://i.imgur.com/QPBmIDY.png
[def31]: https://i.imgur.com/5biJFyk.png
[def32]: https://i.imgur.com/aN67eqL.png
[def33]: https://i.imgur.com/zRUc5fm.png
[def34]: https://i.imgur.com/GqBgXxu.png