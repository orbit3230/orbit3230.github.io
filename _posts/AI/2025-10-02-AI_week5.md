---
layout: post
title: "[인공지능] 5주차 - Temporal Difference Learning"
excerpt: "Temporal Difference Prediction, Temporal Difference Control"

tags:
  - [인공지능, Python]

toc: true

date: 2025-10-02
last_modified_at: 2025-10-02
---
## Temporal Difference Learning
### 1. Temporal Difference Prediction
- Dynamic Programming의 Disadvantage  
![dynamic_programming](TODO)  
  - 환경의 모델에 대한 정확한 지식이 필요
  - 전이 가능한 모든 상태에 대해 가치함수를 업데이트 해야 함

- Monte Carlo Methods의 Disadvantage  
![monte_carlo](TODO)  
  - 에피소드가 종료되어야만 가치 함수를 업데이트 할 수 있음  
  -> 에피소드의 길이가 길다면 최적의 정책을 학습하는 데 많은 시간이 필요
  - 종료 상태(Terminal State)가 존재하지 않는 문제(Continuing Task)에서는 적용하기 어려움
  - Discounting-Aware IS나 Per-Decision IS는 Incremental Implementation이 어려우며, 따라서 에피소드에서 관측한 상태-행동-보상을 모두 저장해야 함  

<br>

- **Temporal Difference Learning**  
![temporal_difference](TODO)
  - Sample Backup
  - 환경의 모델을 모르므로, 에이전트가 환경과 상호작용하여 **실제로 얻은 보상과 다음 상태에 대한 수익의 추정치(= 가치 함수)**를 활용하여 가치 함수를 업데이트
  - 실제로 전이된 다음 상태만을 활용하여 가치 함수의 값을 계산하므로, 연산량은 상태의 개수와는 독립적  

<br>

- Revisit Value Function: Bellman Expectation Equation을 분해해보자  
![bellman_expectation](TODO)
  - 가치 함수의 기대값 : 수익 `G`  

<br>

- Update Rule
  - Monte Carlo Prediction  
  ![monte_carlo_update](TODO)  
    - 수익의 실제값과 수익의 추정치 간의 차이를 최소화 (실제 값으로 추정치를 참이 되게 학습)
    - 수익을 확인해야만 가치함수 업데이트 가능

  - Temporal Difference Prediction  
  ![temporal_difference_update](TODO)  
    - (보사의 실제값+할인된 수익의 추정치)와 수익의 추정치 간의 차이를 최소화 (추정치로 추정치를 참이 되게 학습)
    - 다음 상태를 확인하면(= 행동을 수행하면) 가치 함수 업데이트 가능

<br>

- Convergence Guarantees? : 추정치로부터 추정치를 학습(Learning a Guess from a Guess)하는 것이 참인 추정치로의 수렴을 보장할까?  
  - 다행히도, TD Prediction은 **스텝 사이즈 `a`가 충분히 작다면** 궁극적으로 참인 추정치에 수렴

- MC Prediction vs. TD Prediction
  - 수학적으로 엄밀히 증명된 바는 없으나, 많은 사례에서 **TD Prediction 쪽이 더 빨리 수렴**  

<br>

- Example  
![example_environment](TODO)  
  - Monte Carlo Prediction (`a` 대신 `1/n` 사용)  
  ![monte_carlo_example](TODO)    
    - 환경의 모델이 따르는 특정한 확률 분포 대신, 실제 데이터(= 에피소드)에 맞도록 학습
    - 현재 데이터에 맞는 가치 함수를 추정

  - Temporal Difference Prediction (`a` 대신 `1/n` 사용)  
  ![temporal_difference_example_1](TODO)  
  ![temporal_difference_example_2](TODO)  
    - 환경의 모델이 따를 수도 있는 확률 분포를 추정
    - 미래의 데이터에 맞는 가치 함수를 추정

<br>

### 2. Temporal Difference Control
- Monte Carlo Control 처럼 환경의 모델을 모르므로, 행동 가치 함수를 활용  
![td_control](TODO)  

- **SARSA**, On-Policy TD Control  
![sarsa_update](TODO)
  - 현재 상태 `s`, 현재 행동 `a`, 보상 `r`, 다음 상태 `s'`, 다음 행동 `a'`으로 행동 가치 함수를 업데이트
  - 다음 행동 : 전이된 다음 상태에 대해 현재 정책으로 선택하는 행동
  - On-Policy Methods : 현재 가치 함수를 업데이트하기 위해, 현재 정책으로 선택된 다음 상태, 다음 행동을 활용  

<br>

- SARSA: Pseudocode  
![sarsa_pseudocode](TODO)  

<br>

- Q-Learning, Off-Policy TD Control  
![q_learning_update](TODO)
  - SARSA와 달리, 전이된 다음 상태에서 가장 큰 행동 가치 함수를 활용하여 현재 상태-행동에 대한 행동 가치 함수를 업데이트
  - Off-Policy Methods : 현재 가치 함수를 업데이트하기 위해, 현재 정책으로는 선택되지 않을 수도 있는 다음 상태, 다음 행동을 활용

  <br>

- Q-Learning: Pseudocode  
![q_learning_pseudocode](TODO)  

<br>

- Expected SARSA  
![expected_sarsa_update](TODO)
  - 전이된 다음 상태에 대한 행동 가치 함수들의 기대값을 활용
  - 확률적 정책의 무작위성으로 인한 분산을 효과적으로 감소시킬 수 있음
  - 스템 사이즈 `a`가 크더라도 좋은 성능을 보장  
  - Off-Policy Methods : 현재 가치 함수를 업데이트하기 위해, 현재 정책으로는 선택되지 않을 수도 있는 **모든** 다음 상태, 다음 행동을 활용  

  <br>

- Backup Diagrams  
![backup_diagrams](TODO)  

<br>

- Maxmization Bias
  - 학습의 최종적인 목적 : 행동 가치 함수의 추정치 `Q`를 행동 가치 함수의 실제값 `q`에 가깝게 만드는 것
  - 하지만, 학습 도중에는 어떠한 행동에 대한 행동 가치 함수의 실제값 `q`보다, 행동 가치 함수의 추정치 `Q`가 크게 추정될 수 있음  
  - ε-Greedy Policy : `Q`의 값이 가장 큰 행동을 **높은 확률**로 선택
  - 학습 도중에는 가치 함수의 추정이 아직 부정확하여 실제로 최적이 아닌 행동이 자주 선택되는 경우가 발생  
  -> **Maximization Bias**  

<br>

- Example  
![maximization_bias_example_environment](TODO)  
  - 행동 가치 함수의 실제값 `q`<sub>`π`</sub>  
  ![maximization_bias_example_q_pi_1](TODO)  
  ![maximization_bias_example_q_pi_2](TODO)  
  - 하지만 실제값과는 달리 학습 도중에는 상태 `B`에서 확률적으로 양의 보상을 얻을 수 있음  
  ![maximization_bias_example_q_learning_1](TODO)  
  ![maximization_bias_example_q_learning_2](TODO)  
    - 따라서 최적이지 않은 행동 Left가 선택될 수 있음  

<br>

- Double Q-Learning  
![double_q_learning_update](TODO)  
  - 두 개의 가치 함수를 훈련(Double Learning)하는 방법
  - 하나의 가치 함수 업데이트를 위해 또 다른 가치 함수가 추정한 값을 활용
  - 행동 선택과 가치 함수 추정이 분리됨으로써 Maximization Bias를 해결
  - Q-Learning 뿐만 아니라 SARSA, Expected SARSA에도 적용 가능  

- Double Q-Learning: Pseudocode  
![double_q_learning_pseudocode](TODO)  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 