---
layout: post
title: "[인공지능] 1주차 - Basic Math"
excerpt: "Probability & Expectation"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-04
last_modified_at: 2025-09-04
---
## Basic Math
### 1. Probability
- 함수 (Function) - `f : x -> y`  
  - 집합 `x`(정의역, domain)의 각 원소를 집합 `y`(공역, codomain)의 유일한 원소에 연결하는 규칙  
    - 정의역의 같은 원소는 공역의 같은 원소와 연결됨
    - 정의역의 같은 원소가 공역의 다른 원소와 연결되지 않음

  - 공역의 각 원소는 정의역의 여러 개의 원소와 연결될 수 있음
    - 정의역의 서로 다른 원소는 같은 공역의 원소와 연결됨
    - 연결된 공역의 원소가 같다고 해서 정의역의 원소 또한 같지는 않음

- 함수 Example  
![function][def]  
  - 왼쪽은 함수이고, 오른쪽은 함수가 아님  

- 함수의 또 다른 표기법  
  - `y = f(x)`  
  - `x ∈ X`, `y ∈ Y`  

<br>

- 표본 공간 (Sample Space)
  - 무한히 반복되는 시행에서 얻을 수 있는 특정한 결과인 실현값(outcome)들의 집합  
  - Example
    - 시행 : 6면 주사위를 던지는 것
    - 실현값 : 1, 2, 3, 4, 5, 6
    - 표본 공간 : {1, 2, 3, 4, 5, 6}

<br>

- 확률 변수 (Random Variable)
  - `X : S -> x` or `x = X(s)` for `s ∈ S`, `x ∈ x`  
  - 표본 공간 `S`를 정의역으로, 특정한 확률로 발생하는 사건들의 집합 `x`를 공역으로 하는 함수  

<br>

- 확률 분포 함수 (Probability Distribution Function)
  - `P : x -> [0, 1]` or `p = P(x)` = `P(X(s) = x)` = `P(X = x)` for `s ∈ S`, `x ∈ x`, `p ∈ [0, 1]`  
  - 사건들의 집합 `x`를 정의역으로, `0` 이상 `1` 이하의 실수를 공역으로 하는 함수  

<br>

- Example 1 : 6면 주사위를 던져서 나온 각 눈의 값 -> `P(x) = 1/6` for `x = 1, 2, 3, 4, 5, 6`  
- Example 2 : 6면 주사위를 던져서 나온 눈의 값이 홀수일 때 `1`, 짝수일 때 `0` -> `P(x) = 1/2` for `x = 0, 1`

<br>

- 이산 확률 변수 (Discrete Random Variable)
  - 공역의 원소 수가 유한한 확률 변수  
  - Example
    - 동전 던지기
    - 주사위 던지기

- 확률 질량 함수 (Probability Mass Function, PMF)
  - 이산 확률 변수를 정의역으로 하는 확률 분포 함수
  - Example
    - 이산 균등 분포 (Discrete Uniform Distribution)  
    ![discrete_uniform_distribution][def2]  
    - 베르누이 분포 (Bernoulli Distribution)  
    ![bernoulli_distribution][def3]  

<br>

- 연속 확률 변수 (Continuous Random Variable)
  - 공역의 원소 수가 무한한 확률 변수  
  - Example  
    - 사람의 키
    - 주사위의 회전 수  

- 확률 밀도 함수 (Probability Density Function, PDF)  
  - 연속 확률 변수를 정의역으로 하는 확률 분포 함수  
  - Example  
    - 연속 균등 분포 (Continuous Uniform Distribution)  
    ![continuous_uniform_distribution][def4]  
    - 정규 분포/가우시안 분포 (Normal Distribution/Gaussian Distribution)  
    ![normal_distribution][def5]  

<br>

- Softmax
  - 이산 확률 변수 `X`의 값에 대한 가치를 나타내는 함수 `H`가 존재할 때, 가치가 높을수록 높은 확률을 내는 확률 분포 함수  
  ![softmax][def6]  

  - Example `H(x1) = -0.1`, `H(x2) = 0.2`, `H(x3) = -0.1`  
    - Softmax
      - `P(X = x1)` = `0.30`  
      - `P(X = x2)` = `0.40`  
      - `P(X = x3)` = `0.30`  

  - 특수한 경우 - 이산 확률 변수의 값이 2개일 때 : Sigmoid  
  ![sigmoid][def7]  

  <br>

- 표본 추출 (Sampling)  
  - 주어진 확률 분포에서 확률 변수의 값(또는 표본)을 추출하는 것  
  - 주어진 확률 분포에서 확률이 높은 확률 변수의 값이 높은 확률로 추출됨 (for sure)  
    - e.g., 균등 분퐆 : 모든 확률 변수의 값이 동일한 확률로 추출
    - e.g., 정규 분포 : 확률 변수의 값이 평균과 가까울수록 높은 확률로 추출  
    
    <br>

- 결합 확률 분포 (Joint Probability Distribution)  
  - `P(x, y)` = `P(X = x, Y = y)` = `P(X = x ∩ Y = y)`  
  - 두 개 이상의 서로 다른 확률 변수에 속한 사건들이 동시에 발생하는 확률 분포  
  - Example : 주사위 눈이 짝수면서(`X = 0`), 소수가 아닌(`Y = 0`) 사건의 확률  
  - **Warning : *같은* 표본 공간에 대해 확률 변수가 정의되면, 두 확률 변수는 독립적이지 않음**  
  `P(x, y) != P(x)P(y)`  

  <br>

- 조건부 확률 분포 (Conditional Probability Distribution)  
  - `P(x | y)` = `P(x, y)` / `P(y)`  
  - 한 확률 변수 `Y`의 값이 정해진 상태에서 다른 확률 변수 `X`에 속한 사건이 발생하는 확률 분포  
  - Example : 두 주사위 눈의 합이 `5` 이하인 사건에서(`Y = 1`) 첫 번째 주사위의 눈이 `2`인 사건(`X = 1`)이 발생할 확률  

<br>

- Properties
  - Law of Tatal Probability (LTP) : 확률 변수의 모든 값에 대한 확률 분포 함수의 값을 더하면 `1`  
    - 이산 확률 변수  
    ![LTP_discrete][def8]  
    - 연속 확률 변수  
    ![LTP_continuous][def9]  

  - Marginal Probability : 결합 확률 분포에서 한 확률 변수의 모든 값에 대한 확률 분포 함수의 값을 더하면, 다른 확률 변수에 대한 확률 분포 함수가 됨  
    - 이산 확률 변수  
    ![marginal_discrete][def10]  
    - 연속 확률 변수  
    ![marginal_continuous][def11]  

  - Bayes' Theorem : 사전 확률(Prior Probability)과 우도(Likelihood)를 바탕으로 사후 확률(Posterior Probability)을 계산  
  ![bayes_theorem][def12]  

<br>  

### 2. Expectation
- 기대값 (Expected Value)  
  - 모든 확률 변수의 값과 그 값에 대한 확률 분포를 곱하여 더한 것으로, 어떠한 시행을 무한히 반복했을 때 평균적으로 얻을 수 있는 확률 변수의 값  
    - 이산 확률 변수  
    ![expected_value_discrete][def13]  
    - 연속 확률 변수  
    ![expected_value_continuous][def14]  
    - 베르누이 분포를 따르는 확률 변수 `X`의 기대값  
    ![expected_value_bernoulli][def15]  
    - 정규 분포를 따르는 확률 변수 `X`의 기대값  
    ![expected_value_normal][def16]  

    <br>

- 조건부 기대값 (Conditional Expected Value)  
  - 조건부 확률 분포를 따르는 확률 변수의 기대값  
    - 이산 확률 변수  
    ![conditional_expected_value_discrete][def17]  
    - 연속 확률 변수  
    ![conditional_expected_value_continuous][def18]  

<br>

- Properties  
  - Linearity  
    - 일반적인 기대값의 경우  
    ![linearity_expected_value][def19]  
    - 조건부 기대값의 경우  
    ![linearity_conditional_expected_value][def20]  

  - Law of the Unconscious Statistician (LOTUS)  
    - `g`가 확률 변수 `X`에 대한 함수일 때  
      - 이산 확률 변수  
      ![LOTUS_discrete][def21]  
      - 연속 확률 변수  
      ![LOTUS_continuous][def22]  

  - Law of Total Expectation (LTE)  
    - 조건부 기대값의 기대값 = 조건이 없는 일반적인 기대값  
    ![conditional_LTE][def23]  
    - 결합 확률 분포를 조건부로 하는 경우  
    ![joint_LTE][def24]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/qIVvyOm.png
[def2]: https://i.imgur.com/KfT0pK0.png
[def3]: https://i.imgur.com/X5QRQvc.png
[def4]: https://i.imgur.com/o0pMZfK.png
[def5]: https://i.imgur.com/fVbrCnt.png
[def6]: https://i.imgur.com/NphOkGa.png
[def7]: https://i.imgur.com/SS5NTD2.png
[def8]: https://i.imgur.com/ylH6HQx.png
[def9]: https://i.imgur.com/vdCckHT.png
[def10]: https://i.imgur.com/CjLbnw0.png
[def11]: https://i.imgur.com/9URFtyH.png
[def12]: https://i.imgur.com/hZUgKNy.png
[def13]: https://i.imgur.com/zezjhYn.png
[def14]: https://i.imgur.com/Zh7eAnj.png
[def15]: https://i.imgur.com/LDGGVRs.png
[def16]: https://i.imgur.com/nHmJV39.png
[def17]: https://i.imgur.com/wuCSBRY.png
[def18]: https://i.imgur.com/fWZo5p4.png
[def19]: https://i.imgur.com/gxbHayi.png
[def20]: https://i.imgur.com/LH4j6mg.png
[def21]: https://i.imgur.com/ww6Lw9y.png
[def22]: https://i.imgur.com/iduKHWk.png
[def23]: https://i.imgur.com/SLdn0z6.png
[def24]: https://i.imgur.com/cv95Mlj.png