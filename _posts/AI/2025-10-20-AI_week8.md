---
layout: post
title: "[인공지능] 8주차 - Planning & Learning"
excerpt: "Model-Based Reinforcement Learning, Experence Sampling"

tags:
  - [인공지능, Python]

toc: true

date: 2025-10-20
last_modified_at: 2025-10-20
---
## Planning & Learning
### 1. Model-Based Reinforcement Learning
- Model-Based + Reinforcement Learning ?  
`P(s', r | s, a)` ≈ `p`<sub>`*`</sub>`(s', r | s, a)`
  - 에이전트가 실제로 경험한 상태-행동-보상을 바탕으로, 실제 모델을 따라하는 모델(Simulated Model)(= 주어진 상태-행동에 대해서 다음 상태 및 보상을 예측하는 함수)을 학습
  - 실제 경험(Real Experience) + 학습된 모델로부터 생성할 수 있는 모조 경험(Simulated Experience)을 모두 활용하여 최적의 정책을 학습  

<br>

- Planning vs. Learning
  - (Pure) Planning (e.g., Dynamic Programming)  
  ![pure_planning][def]  
  - Learning (e.g., MC & TD)  
  ![pure_learning][def2]  
  - Learning + Planning  
  ![learning_planning][def3]  

<br>

- Rough Categories of Model-Based RL
  - Experience Sampling  
  ![experience_sampling][def4]
    - 상태-행동-보상-다음 상태를 모델로부터 추출하여 가치 함수를 학습  
    - Dyna-Q, Deep Q Network 등

  - Trajectory Sampling  
  ![trajectory_sampling][def5]  
    - 임의의 상태에서 주어진 정책과 모델을 따라 모조 에피소드를 진행하면서 방문하는 상태-보상에 대해 가치 함수를 학습
    - Real-Time Dynamic Programming 등

  - Decision-Time Planning  
  ![decision_time_planning][def6]  
    - 현재 상태에서 추정된 모델을 바탕으로 모조 에피소드를 진행하여 얻은 수익으로 행동을 선택
    - Monte Carlo Tree Search(MCTS), Rollout Algorithm 등  
    - e.g., 바둑 AI 알파고(AlphaGo) : MCTS + Deep Neural Network

<br>

### 2. Experence Sampling
- Dyna-Q  
![dyna_q_algorithm][def7]  
  - Planning과 Learning을 통합한 강화학습 알고리즘
  - 환경으로부터 얻어지는 실제 경험은 정책의 학습 및 모델의 학습에 활용
  - 가치 함수의 학습 (+ 정책의 개선)은 실제 경험과 모델이 생성하는 모조 경험에 의해 이루어짐  

- Dyna-Q: Pseudocode  
![dyna_q_pseudocode][def8]  

<br>

- Q-Learning vs. Dyna-Q  
![q_learning_vs_dyna_q][def9]  
  - Q-Learning : 환경과 상호작용 할 때마다 **현재 상태 하나**에 대한 가치 함수를 업데이트
  - Dyna-Q : 환경과 상호작용 할 때마다 **현재 상태 하나 + `n`개의 상태**에 대한 가치 함수를 업데이트  
  => 환경과 적은 수의 상호작용으로도 가치 함수를 많이 업데이트 할 수 있음  

  <br>

- When Model is Wrong?
  - 비정상 문제(Non-Stationary Problem)에서는 환경의 모델이 시간이 지남에 따라 변할 수 있으므로, 추정된 모델은 부정확하며 학습한 정책 또한 최적이 아닐 수 있음  
  - 물론, ε-Greedy 정책으로 여러 상태-행동을 다시 충분히 탐험하면 이러한 문제는 어느정도 완화될 수 있음  
  ![dyna_q_non_stationary][def10]  
  - 하지만 환경이 이전보다 더 나은 보상을 얻을 수 있도록 변화된다면, ε-Greedy 정책이 가진 특성상 (= 현재 상태에서 상태 가치 함수가 가장 높은 행동을 선택), 새로운 최적의 정책을 학습하기는 어려움  
  - 따라서 ε-Greedy가 보장하는 탐험 외에도, 추가적인 탐험을 보장할 필요가 있음  
  ![dyna_q_plus_algorithm][def11]  

<br>

- Dyna-Q+
  - Planning 단계에서 오래전에 관측된 상태-행동에 대해서는 실제 관측된 보상 이외에도 추가적인 보상을 받을 수 있도록 설정  
    - 오래전에 관측된 상태-행동의 행동 가치 함수의 값이 증가
    - 오래전에 관측된 상태-행동이 선택될 확률이 증가
    - 오래전에 관측된 상태-행동에 대해서 행동 가치 함수가 업데이트 될 확률이 증가  

- Dyna-Q+: Pseudocode  
![dyna_q_plus_pseudocode][def12]  

<br>

- Priority of Experiences
  - Dyna-Q 및 Dyna-Q+는 Planning 단계에서 이전 경험(상태-행동)을 **무작위로 추출**하여 그에 대한 가치 함수를 업데이트
  - 하지만, 어떤 경험은 다른 경험보다 **중요하지 않을 수도** 있음  
  -> 경험의 중요도를 평가하는 방법은..?  

- Error as Priority of Experiences  
![priority_of_experiences][def13]  
  - 가치 함수는 목표 값과 과거의 추정치 간의 차이를 최소화하는 것이 목적
  - 목표값과 과거의 추정치(초록색 박스)가 일치하면, 가치 함수의 값은 업데이트 되지 않음
    - 업데이트 할 필요가 없으므로, 해당 상태-행동은 업데이트의 중요도가 낮음
  - 반대로, 목표값과 과거의 추정치가 크게 차이난다면, 해당 상태-행동에 대한 행동 가치 함수는 업데이트가 필요함
    - 업데이트가 필요하므로, 해당 상태-행동은 업데이트의 중요도가 높음  

- Prioritized Sweeping
  - Error를 우선순위로 하는 우선순위 큐(Priority Queue) 활용
  - Error가 높은 상태-행동부터 우선적으로 추출하여 그에 대한 행동 가치 함수를 업데이트  

- Prioritized Sweeping: Pseudocode  
![prioritized_sweeping_pseudocode][def14]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/cmSTq0E.png
[def2]: https://i.imgur.com/U8ZVMst.png
[def3]: https://i.imgur.com/SMJrI3i.png
[def4]: https://i.imgur.com/uiaFCQr.png
[def5]: https://i.imgur.com/xUg518t.png
[def6]: https://i.imgur.com/1KVexp2.png
[def7]: https://i.imgur.com/o31sVEp.png
[def8]: https://i.imgur.com/BtlGeTp.png
[def9]: https://i.imgur.com/ZDKTNp6.png
[def10]: https://i.imgur.com/q3QZQ8k.png
[def11]: https://i.imgur.com/spybynX.png
[def12]: https://i.imgur.com/ddlAwjh.png
[def13]: https://i.imgur.com/wRejTZO.png
[def14]: https://i.imgur.com/yXyLauP.png