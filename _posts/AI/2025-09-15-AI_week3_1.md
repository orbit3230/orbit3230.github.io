---
layout: post
title: "[인공지능] 3주차 (1) - Markov Process"
excerpt: "Markov Process, Bellman Equation"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-15
last_modified_at: 2025-09-15
---
## Markov Process
### 1. Markov Process
- Stochastic Process (확률 프로세스)  
  - 시간에 따라 발생한 확률 변수들의 순차적 모임  
  `X`<sub>`1`</sub>, `X`<sub>`2`</sub>, ... , `X`<sub>`t`</sub>
  - 다음 사건이 발생할 확률은 현재까지 발생했던 사건들이 발생할 결합 확률을 조건으로 하는 조건부 확률로 표현  
  `P(X`<sub>`t+1`</sub>` | X`<sub>`1`</sub>`, X`<sub>`2`</sub>`, ... , X`<sub>`t`</sub>`)`  
  -> 다음 사건이 발생할 확률을 알기 위해서는 이전에 발생한 모든 사건들을 기억해야 함.

<br>

- Markov Property (마르코프 성질)  
  - 확률 프로세스에서 다음 사건이 발생할 확률이 오직 **현재 사건에만 의존적**이라고 가정하는 성질  
  `P(X`<sub>`t+1`</sub>` | X`<sub>`1`</sub>`, X`<sub>`2`</sub>`, ... , X`<sub>`t`</sub>`) = P(X`<sub>`t+1`</sub>` | X`<sub>`t`</sub>`)`  
  `E(X`<sub>`t+1`</sub>` | X`<sub>`1`</sub>`, X`<sub>`2`</sub>`, ... , X`<sub>`t`</sub>`) = E(X`<sub>`t+1`</sub>` | X`<sub>`t`</sub>`)`  
  - 이전 사건들의 발생 확률을 기억할 필요 없이, 현재 사건에만 집중
  - 대다수의 강화 학습 문제는 마르코프 성질을 따르는 확률 프로세스로 가정  

  <br>

- Markov Chains (마르코프 연쇄)
  - 상태와 상태 전이 확률로 튜플로 정의되며, 마르코프 성질을 따르는 확률 프로세스  
  ![markov_chain](TODO)  
    - `S` : 상태의 집합  
    - `P` : 상태 전이 확률의 집합 (상태 전이 행렬)  
    ![state_transition_matrix](TODO)  
    - 에피소드의 표현 : `S`<sub>`0`</sub> → `S`<sub>`1`</sub> → `S`<sub>`2`</sub> → ... → `S`<sub>`t`</sub>
  
  - Example  
  ![markov_chain_example_1](TODO)  
  ![markov_chain_example_2](TODO)  

  <br>

- Markov Reward Process (MRP, 마르코프 보상 프로세스)
  - 상태, 보상, 상태 전이 확률, 할인율의 튜플로 정의되며, 마르코프 성질을 따르는 확률 프로세스  
  ![markov_reward_process](TODO)  
    - `S` : 상태의 집합  
    - `R` : 보상의 집합  
    - `P` : 상태 전이 확률의 집합 (상태 전이 행렬)  
    - `γ` : 할인율 (0 ≤ `γ` < 1)  
    - 에피소드의 표현 : `S`<sub>`0`</sub> → `R`<sub>`1`</sub> → `S`<sub>`1`</sub> → `R`<sub>`2`</sub> → `S`<sub>`2`</sub> → ... → `S`<sub>`t-1`</sub> → `R`<sub>`t`</sub> → `S`<sub>`t`</sub>  

<br>

- Markov Decision Process (MDP, 마르코프 결정 프로세스)  
  - 상태, 행동, 상태 전이 확률, 보상, 할인율의 튜플로 정의되며, 마르코프 성질을 따르는 확률 프로세스  
  ![markov_decision_process](TODO)  
    - `S` : 상태의 집합  
    - `A` : 행동의 집합  
    - `P` : 상태 전이 확률의 집합 (상태 전이 행렬)  
    - `R` : 보상의 집합  
    - `γ` : 할인율 (0 ≤ `γ` < 1)  
    - 에피소드의 표현 : `S`<sub>`0`</sub> → `A`<sub>`0`</sub> → `R`<sub>`1`</sub> → `S`<sub>`1`</sub> → `A`<sub>`1`</sub> → `R`<sub>`2`</sub> → `S`<sub>`2`</sub> → ... → `S`<sub>`t-1`</sub> → `A`<sub>`t-1`</sub> → `R`<sub>`t`</sub> → `S`<sub>`t`</sub>  

    <br>

- MDP & Reinforcement Learning
  - 강화학습을 통해 해결하려는 문제는 MDP로 정의  
  -> 강화 학습을 통해 문제를 해결하기 위해서는 상태, 행동, 상태 전이 확률, 보상, 할인율이 정의되어야 함
  -> 상태는 셀 수 있어야 함 (유한 상태 공간)  
  -> 행동은 셀 수 있어야 함 (유한 행동 공간)  
  -> Tabular

<br>

### 2. Bellman Equation
- State-Value Function vs. Action-Value Function  
  - 상태 가치 함수 : **현재 상태에서 가능한 행동들**에 대한 **행동 가치 함수**의 기대값  
  ![state_value_function](TODO)  
  - 행동 가치 함수 : **현재 상태에서 수행한 행동으로 전이 가능한 다음 상태 및 보상**에 대한 **보상**의 기대값 + **상태 가치 함수**의 기대값  
  ![action_value_function_1](TODO)  
  ![action_value_function_2](TODO)  

  <br>

- Bellman Expectation Equation (벨만 기대 방정식)  
  - 현재의 상태 가치 함수와 행동 가치 함수는 다음의 상태 가치 함수와 행동 가치 함수로 재귀적으로 정의 가능  
  ![bellman_expectation_equation_1](TODO)  
  -> 벨만 기대 방정식을 해결 = 현재 정책에 대한 가치 함수의 값을 정확하게 추정 (= 정책 평가)  

  <br>

- 더 나은 정책, `π` ≤ `π'`  
  - 어떤 두 정책 `π`, `π'`이 존재하고, 모든 상태에 대해서 상태 가치 함수가 다음의 조건을 마나족하면 정책 `π`보다 정책 `π'`이 더 나은 정책  
  `v`<sub>`π`</sub>`(s) ≤ v`<sub>`π'`</sub>`(s)` for `∀ s ∈ S`  
  -> 정책 `π`를 적용할 때보다 정책 `π'`를 적용할 때 상태 가치 함수의 값이 개선됨  
  - 직관적으로, 더 나은 정책에 대해 모든 상태 및 행동에 대한 행동 가치 함수 또한 다음의 조건을 만족함  
  `q`<sub>`π`</sub>`(s, a) ≤ q`<sub>`π'`</sub>`(s, a)` for `∀ s ∈ S, ∀ a ∈ A`  

<br>

- 최적의 정책, `π`<sub>`*`</sub>  
  - 가능한 모든 정책 중에서, 모든 상태에 대해 상태 가치 함수의 값을 최대화(= 최적 상태 가치 함수)할 수 있는 정책  
  `v`<sub>`*`</sub>`(s) = max`<sub>`π`</sub>` v`<sub>`π`</sub>`(s)` for `∀ s ∈ S`  
  - 직관적으로, 최적의 정책은 모든 상태 및 행동에 대하여, 행동 가치 함수의 값을 최대화(= 최적 행동 가치 함수)할 수 있음  
  `q`<sub>`*`</sub>`(s, a) = max`<sub>`π`</sub>` q`<sub>`π`</sub>`(s, a)` for `∀ s ∈ S, ∀ a ∈ A`  
  - 특징
    - 주어진 상태에서 가치 함수의 값을 최대화하는 최적의 행동이 명확하게 정해진 정책  
    - 즉, 더 이상 탐험이 필요 없으며, 동일한 상태에 대해 항상 동일한 행동이 선택되는 **결정적 정책**.  
  - Example
    - 확률적 정책 (ε-greedy)  
    ![stochastic_policy](TODO)  
    - 최적의 결정적 정책  
    ![optimal_deterministic_policy](TODO)  

    <br>

- Bellman Optimality Equation (벨만 최적 방정식)  
  - 벨만 기대 방정식으로부터, 현재의 최적 상태 가치 함수와 최적 행동 가치 함수는 다음의 최적 상태 가치 함수와 최적 행동 가치 함수로 재귀적으로 정의 가능  
  ![bellman_optimality_equation_1](TODO)  
  -> 벨만 최적 방정식을 해결 = 최적 가치 함수의 값을 찾음 = 최적의 정책을 알 수 있음  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 