---
layout: post
title: "[인공지능] 13주차 (1) - Practice on Policy Gradient Methods"
excerpt: "Prerequisite, REINFORCE Algorithm Implementation, REINFORCE with Baseline Implementation, Different Learning Rate, Normalization & More Update for Value Function, Continuous State-Action Space"

tags:
  - [인공지능, Python]

toc: true

date: 2025-11-24
last_modified_at: 2025-11-24
---
## Practice on Policy Gradient Methods
### 1. Prerequisite
`%pip install tensorflow-probability`  
`%pip install -q gymnasium[mujoco]`  

<br>

### 2. REINFORCE Algorithm Implementation
- 확률을 출력해야 하므로 마지막 레이어의 activation function으로 Softmax 사용  

```py
from tensorflow import keras

model = keras.models.Sequential([
    keras.layers.Input(
        shape=env.observation_space.shape,
    ),
    keras.layers.Dense(
        units=128,
        activation=keras.activations.relu,
        kernel_initializer=keras.initializers.HeNormal(seed=42),
    ),
    # 2개의 퍼셉트론이 있는 레이어를 연결한다.
    # 각 퍼셉트론이 좌, 우 행동에 대한 확률을 출력한다
    keras.layers.Dense(
        units=env.action_space.n,
        activation=keras.activations.softmax,
        kernel_initializer=keras.initializers.GlorotNormal(seed=42),
    )
])
model.summary()
```

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense (Dense)                   │ (None, 128)            │           640 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 2)              │           258 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
  Total params: 898 (3.51 KB)
  Trainable params: 898 (3.51 KB)
  Non-trainable params: 0 (0.00 B)
```

- 인공 신경망 출력값 확인  

```py
from tensorflow import keras

obs, _ = env.reset()
state = keras.ops.expand_dims(obs, axis=0)
model(state)
```

- 행동 선택

```py
import numpy as np


# (1, 2)로 행동의 확률을 받는다.
probs = model(state)

# (1, 2)를 (2, )로 바꾼다.
probs = np.squeeze(probs)

# 확률을 고려하여 행동을 선택한다
action = np.random.choice(env.action_space.n, p=probs)

print(action)
```

<br>

- Optimizer - Adam  

```py
from tensorflow import keras

optimizer = keras.optimizers.Adam(
    learning_rate=0.0003,
    clipnorm=1.0
)
```

<br>

- Training

```py
import numpy as np
from tensorflow import keras
import tensorflow as tf
from tqdm.auto import tqdm
import os

EPISODES_MAX = 1000
GAMMA = 0.99

# 최근 30개의 총 보상을 저장한다.
reward_per_episode = np.zeros(30)
history = []
random = np.random.default_rng(42)

pbar = tqdm(range(EPISODES_MAX), desc='Episode')

for episode in pbar:
    history.clear()
    episode_reward = 0

    done = False
    obs, _ = env.reset()

    while not done:
        state = keras.ops.expand_dims(obs, axis=0)

        # 위에서 했던대로 행동 하나를 선택한다.
        probs = model(state)
        action = random.choice(env.action_space.n, p=np.squeeze(probs))

        # CartPole 환경은 매 행동마다 +1의 보상을 제공한다.
        # 이걸 그대로 사용하겠다.
        obs, reward, terminated, truncated, _ = env.step(action)

        episode_reward += reward
        done = terminated or truncated

        history.append((state, action, reward))

    # 지난번과 좀 다른 방식으로 매개변수를 업데이트 할 것이다.
    # 지난번에는 에피소드의 경험을 하나씩 확인하면서
    # 손실 기울기를 계산했다.
    # 하지만, 경험 하나에 대한 출력값을 얻기 위해 매번 신경망을 호출하는 건
    # 비효율적이다. 대신에, 전체 경험들을 하나의 묶음으로 만들고
    # 한번에 출력을 얻어보겠다.

    # 먼저 상태, 행동, 보상을 각각 하나의 리스트로 만든다.
    batch_state = [exp[0] for exp in history]
    batch_action = [exp[1] for exp in history]
    batch_reward = [exp[2] for exp in history]

    # 수익을 계산한다.
    batch_return = []
    G = 0.0
    for reward in reversed(batch_reward):
        G = reward + GAMMA * G
        batch_return.insert(0, G)

    # 상태와 수익은 모델의 입출력 및 매개변수 업데이트에 활용되므로
    # Numpy 배열 혹은 Tensor로 바꿔준다.
    batch_state = keras.ops.concatenate(batch_state)
    batch_return = keras.ops.convert_to_tensor(batch_return)

    # 여기서부터 중요하다.
    with tf.GradientTape() as tape:
        # 에피소드 내에서 방문한 모든 상태에 대해 확률을 출력한다.
        probs = model(batch_state)

        # 0, 1의 행동을 2차원 이진 벡터로 One-Hot Encoding한다.
        action_onehot = keras.ops.one_hot(batch_action, env.action_space.n)

        # One-Hot Encoding된 벡터와 확률을 곱해서, 실제로 수행한 행동에 대한
        # 확률만을 남기고, 하지 않은 행동에 대한 확률은 0으로 바꾼다.
        action_onehot_probs = action_onehot * probs

        # 각 행을 하나로 합쳐서 확률값만 남긴다.
        action_probs = keras.ops.sum(action_onehot_probs, axis=1)

        # 로그 확률을 구한다.
        log_prob = keras.ops.log(action_probs)

        # 수익, 로그 확률, gamma^t을 곱해서 목적 함수의 값을 구한다.
        loss = log_prob * batch_return * (GAMMA ** np.arange(len(history)))

        # 각 상태별로 구한 목적 함수의 값을 합하여 최종적인 목적 함수의 값을 구한다.
        # 단, 목적 함수의 값을 최대화하는 경사 상승법을 사용하는 Policy Gradient와는 달리
        # Keras는 목적 함수의 값을 최소화하는 경사 하강법만 가능하므로
        # 음수로 바꿔줘야 한다.
        loss = -keras.ops.sum(loss)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    reward_per_episode[episode % len(reward_per_episode)] = episode_reward
    running_reward = np.mean(reward_per_episode)

    pbar.set_postfix(
        loss=f'{loss:.5f}',
        reward=f'{episode_reward:.5f}',
        running_reward=f'{running_reward:.5f}',
    )

    # 매 200 에피소드마다 훈련된 신경망을 저장하겠다.
    if (episode + 1) % 200 == 0:
        os.makedirs('./reinforce', exist_ok=True)
        keras.models.save_model(model, f'./reinforce/{episode}.keras')

    # 평균 보상이 300 이상이면 종료하겠다.
    if running_reward > 300.0:
        keras.models.save_model(model, f'./reinforce/{episode}.keras')
        break
```

- Agent Implementation  

```py
from tensorflow import keras
import kymnasium as kym
from typing import Any, Dict
import pickle

class CartPoleAgent(kym.Agent):
    def __init__(
            self,
            model: keras.models.Model,
            seed: int = None
    ):
        self.model = model
        self.seed = seed
        self.random = np.random.default_rng(self.seed)

    def save(self, path: str):
        config = {
            'seed': self.seed,
        }
        with open(f'{path}.config', 'wb') as f:
            pickle.dump(config, f)

        keras.models.save_model(
            model=self.model,
            filepath=f'{path}.keras'
        )

    @classmethod
    def load(cls, path: str):
        with open(f'{path}.config', 'rb') as f:
            config = pickle.load(f)

        model = keras.models.load_model(
            filepath=f'{path}.keras'
        )
        return CartPoleAgent(model=model, seed=config['seed'])

    def act(self, observation: Any, info: Dict):
        state = keras.ops.expand_dims(observation, axis=0)
        probs = self.model(state)

        # Value Function Approximation에서
        # 행동 가치 함수가 가장 큰 값을 선택한 것처럼
        # 확률이 가장 높은 행동을 선택하면 된다.
        action = self.random.choice(np.flatnonzero(probs == np.max(probs)))
        return action
```

- Evaluation  

```py
from tensorflow import keras
import gymnasium as gym

EPISODES = (199, 999)

for episode, stage in zip(EPISODES, ('Early', 'Later')):
    model = keras.models.load_model(f'./reinforce/{episode}.keras')
    agent = CartPoleAgent(model)
    recoder = gym.wrappers.RecordVideo(
        env=env,
        video_folder='./reinforce',
        name_prefix=stage
    )
    done = False
    obs, info = recoder.reset()

    while not done:
        action = agent.act(obs, info)
        obs, _, terminated, truncated, info = recoder.step(action)
        done = terminated or truncated

    recoder.close()
```

<br>

### 3. REINFORCE with Baseline Implementation
- Actor

```py
from tensorflow import keras

actor = keras.models.Sequential([
    keras.layers.Input(
        shape=env.observation_space.shape,
    ),
    keras.layers.Dense(
        units=128,
        activation=keras.activations.relu,
        kernel_initializer=keras.initializers.HeNormal(seed=42),
    ),
    keras.layers.Dense(
        units=env.action_space.n,
        activation=keras.activations.softmax,
        kernel_initializer=keras.initializers.GlorotNormal(seed=42),
    )
])

actor_optimizer = keras.optimizers.Adam(
    learning_rate=0.0003,
    clipnorm=1.0
)
```

- Critic : 마지막 출력 레이어의 퍼셉트론 수를 1개로, 활성화 함수는 Identity  

```py
from tensorflow import keras

critic = keras.models.Sequential([
    keras.layers.Input(
        shape=env.observation_space.shape,
    ),
    keras.layers.Dense(
        units=128,
        activation=keras.activations.relu,
        kernel_initializer=keras.initializers.HeNormal(seed=42),
    ),
    keras.layers.Dense(
        units=1,
        activation=keras.activations.linear,
        kernel_initializer=keras.initializers.GlorotNormal(seed=42),
    )
])

critic_optimizer = keras.optimizers.Adam(
    learning_rate=0.0003,
    clipnorm=1.0
)
```

<br>

- Training

```py
import numpy as np
from tensorflow import keras
import tensorflow as tf
from tqdm.auto import tqdm
import os

EPISODES_MAX = 1000
GAMMA = 0.99

reward_per_episode = np.zeros(30)
history = []
random = np.random.default_rng(42)
critic_objective = keras.losses.Huber()

pbar = tqdm(range(EPISODES_MAX), desc='Episode')

for episode in pbar:
    history.clear()
    episode_reward = 0

    done = False
    obs, _ = env.reset()

    while not done:
        state = keras.ops.expand_dims(obs, axis=0)

        # Actor의 행동 선택한다
        probs = actor(state)
        action = random.choice(env.action_space.n, p=np.squeeze(probs))

        # Critic으로 현재 상태에 대한 상태 가치 함수를 추정한다.
        value = np.squeeze(
            critic(state)
        )
        obs, reward, terminated, truncated, _ = env.step(action)

        episode_reward += reward
        done = terminated or truncated

        # 상태-행동-보상-가치 함수를 모두 저장한다.
        history.append((state, action, reward, value))

    batch_state = [exp[0] for exp in history]
    batch_action = [exp[1] for exp in history]
    batch_reward = [exp[2] for exp in history]
    batch_value = [exp[3] for exp in history]

    # 수익을 계산한다.
    batch_return = []
    G = 0.0
    for reward in reversed(batch_reward):
        G = reward + GAMMA * G
        batch_return.insert(0, G)

    # 상태, 수익, 가치 함수는 모델의 입출력 및 매개변수 업데이트에 활용되므로
    # Numpy 배열 혹은 Tensor로 바꿔준다.
    batch_state = keras.ops.concatenate(batch_state)
    batch_return = keras.ops.convert_to_tensor(batch_return)
    batch_value = keras.ops.convert_to_tensor(batch_value)

    # 먼저 Critic부터 업데이트 하자
    with tf.GradientTape() as tape:
        value = keras.ops.squeeze(
            critic(batch_state)
        )
        critic_loss = critic_objective(batch_return, value)
    critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)
    critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))

    with tf.GradientTape() as tape:
        probs = actor(batch_state)
        action_probs = keras.ops.sum(
            keras.ops.one_hot(batch_action, env.action_space.n) * probs,
            axis=1
        )
        log_prob = keras.ops.log(action_probs)

        # 수익에서 Baseline인 상태 가치 함수 (= Critic의 출력)을 빼줘야 한다.
        actor_loss = log_prob * (batch_return - batch_value) * (GAMMA ** np.arange(len(history)))
        actor_loss = -keras.ops.sum(actor_loss)
    actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)
    actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))

    reward_per_episode[episode % len(reward_per_episode)] = episode_reward
    running_reward = np.mean(reward_per_episode)

    pbar.set_postfix(
        actor_loss=f'{actor_loss:.5f}',
        critic_loss=f'{critic_loss:.5f}',
        reward=f'{episode_reward:.5f}',
        running_reward=f'{running_reward:.5f}',
    )

    if (episode + 1) % 200 == 0:
        os.makedirs('./baseline', exist_ok=True)
        keras.models.save_model(actor, f'./baseline/{episode}.keras')

    if running_reward > 300.0:
        keras.models.save_model(model, f'./baseline/{episode}.keras')
        break
```

- Evaluation  

```py
from tensorflow import keras
import gymnasium as gym
from IPython.display import HTML
from base64 import b64encode


EPISODES = (199, 999)

for episode, stage in zip(EPISODES, ('Early', 'Later')):
    model = keras.models.load_model(f'./baseline/{episode}.keras')
    agent = CartPoleAgent(model)
    recoder = gym.wrappers.RecordVideo(
        env=env,
        video_folder='./baseline',
        name_prefix=stage
    )
    done = False
    obs, info = recoder.reset()

    while not done:
        action = agent.act(obs, info)
        obs, _, terminated, truncated, info = recoder.step(action)
        done = terminated or truncated

    recoder.close()

urls = []
for stage in ('Early', 'Later'):
    video = open(f'./baseline/{stage}-episode-0.mp4','rb').read()
    url = f'data:video/mp4;base64,{b64encode(video).decode()}'
    urls.append(url)

HTML(f'''
<div>
    <video width=400 controls>
        <source src="{urls[0]}" type="video/mp4">
    </video>
    <video width=400 controls>
        <source src="{urls[1]}" type="video/mp4">
    </video>
</div>
''')
```

<br>

### 4. Different Learning Rate
- Loss를 줄이기 위한 방법 1 : Critic의 학습률을 Actor보다 크게 설정  

```py
from tensorflow import keras

actor_optimizer = keras.optimizers.Adam(
    learning_rate=0.00003,
    clipnorm=1.0
)

critic_optimizer = keras.optimizers.Adam(
    learning_rate=0.0001,
    clipnorm=1.0
)
```

<br>

### 5. Normalization & More Update for Value Function
- Loss를 줄이기 위한 방법 2 : 수익의 값을 정규화  
- Loss를 줄이기 위한 방법 3 : Critic의 업데이트를 여러 번 수행

```py
import numpy as np
from tensorflow import keras
import tensorflow as tf
from tqdm.auto import tqdm
import os

EPISODES_MAX = 1000
GAMMA = 0.99

reward_per_episode = np.zeros(30)
history = []
random = np.random.default_rng(42)
critic_objective = keras.losses.Huber()

pbar = tqdm(range(EPISODES_MAX), desc='Episode')

for episode in pbar:
    history.clear()
    episode_reward = 0

    done = False
    obs, _ = env.reset()

    while not done:
        state = keras.ops.expand_dims(obs, axis=0)

        probs = actor(state)
        action = random.choice(env.action_space.n, p=np.squeeze(probs))

        value = np.squeeze(
            critic(state)
        )
        obs, reward, terminated, truncated, _ = env.step(action)

        episode_reward += reward
        done = terminated or truncated

        history.append((state, action, reward, value))

    batch_state = [exp[0] for exp in history]
    batch_action = [exp[1] for exp in history]
    batch_reward = [exp[2] for exp in history]
    batch_value = [exp[3] for exp in history]

    batch_return = []
    G = 0.0
    for reward in reversed(batch_reward):
        G = reward + GAMMA * G
        batch_return.insert(0, G)

    batch_state = keras.ops.concatenate(batch_state)
    batch_return = keras.ops.convert_to_tensor(batch_return)
    batch_value = keras.ops.convert_to_tensor(batch_value)

    # 에피소드가 끝날 때마다 Critic을 10번 훈련시키겠다.
    for _ in range(10):
        with tf.GradientTape() as tape:
            value = keras.ops.squeeze(
                critic(batch_state)
            )
            critic_loss = critic_objective(batch_return, value)
        critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)
        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))

    with tf.GradientTape() as tape:
        probs = actor(batch_state)
        action_probs = keras.ops.sum(
            keras.ops.one_hot(batch_action, env.action_space.n) * probs,
            axis=1
        )
        log_prob = keras.ops.log(action_probs)

        # G_t - V_(S_t)의 값을 표준화 하겠다.
        target = batch_return - batch_value
        target = (target - keras.ops.mean(target)) / (keras.ops.std(target) + 1e-8)

        actor_loss = log_prob * target * (GAMMA ** np.arange(len(history)))
        actor_loss = -keras.ops.sum(actor_loss)
    actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)
    actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))

    reward_per_episode[episode % len(reward_per_episode)] = episode_reward
    running_reward = np.mean(reward_per_episode)

    pbar.set_postfix(
        actor_loss=f'{actor_loss:.5f}',
        critic_loss=f'{critic_loss:.5f}',
        reward=f'{episode_reward:.5f}',
        running_reward=f'{running_reward:.5f}',
    )

    if (episode + 1) % 200 == 0:
        os.makedirs('./norm-more', exist_ok=True)
        keras.models.save_model(actor, f'./norm-more/{episode}.keras')

    if running_reward > 300.0:
        keras.models.save_model(actor, f'./norm-more/{episode}.keras')
        break
```

<br>

### 6. Continuous State-Action Space
- 연속 행동 공간에서는 Softmax 함수로는 행동을 선택할 수 없다.

- 대신, 행동의 평균과 (로그)표준편차를 각각 따로 출력하여 가우시안 정규 분포를 구성한 후, 거기서 행동을 추출해야 한다.  

- Functional API를 사용하여 인공 신경망을 구성.  

- Actor

```py
from tensorflow import keras

input = keras.layers.Input(
    shape=env.observation_space.shape,
)

layer = keras.layers.Dense(
    units=256,
    activation=keras.activations.relu,
    kernel_initializer=keras.initializers.HeNormal(seed=42)
)(input)

layer = keras.layers.Dense(
    units=256,
    activation=keras.activations.relu,
    kernel_initializer=keras.initializers.HeNormal(seed=42)
)(layer)

# 평균을 출력한다.
# Inverted Pendulum의 경우 행동 공간이 [-3, 3]이므로,
# [-1, 1] 사이의 출력을 내는 Hyperbolic Tanget를 활성 함수로 사용하고
# 여기에 2을 곱해주겠다.
mu = keras.layers.Dense(
    units=1,
    activation=keras.activations.tanh,
    kernel_initializer=keras.initializers.GlorotNormal(seed=42)
)(layer) * 3.0

# 로그 표준편차를 출력한다.
# 표준편차는 당연히 양수인데, 그렇다면 활성함수는
# Sigmoid, Softplus, ReLU 등만 가능하다.
# 하지만 로그 표준편차로 간주하게 되면 음수가 나오는 활성 함수도 사용할 수 있다!
# 물론, 표준편차가 너무 크게 되면 확률 범위도 굉장히 넓어지므로 범위를 적당히 조정해주겠다.
# 이번 실습에서는 로그 표준편차가 [-5, 1], 표준편차로는 [0.006, 2.71] 정도가 나오도록 만들었다.
log_sigma = keras.layers.Dense(
    units=1,
    activation=keras.activations.tanh,
    kernel_initializer=keras.initializers.GlorotNormal(seed=42)
)(layer) * 3.0 - 2.0

# 위에서 만든 평균 및 로그 표준편차 레이어를
# 출력으로 활용한다.
actor = keras.models.Model(
    inputs=input,
    outputs=[mu, log_sigma]
)

actor.summary()
```

- Actor 행동 선택  

```py
from tensorflow import keras
from tensorflow_probability import distributions as tfd
import numpy as np

obs, _ = env.reset()
state = keras.ops.expand_dims(obs, axis=0)

# Actor에서 평균과 로그 표준편차를 받는다.
mu, log_sigma = actor(state)
mu, log_sigma = keras.ops.squeeze(mu), keras.ops.squeeze(log_sigma)

# 로그 표준편차를 표준편차로 만들어준다.
sigma = keras.ops.exp(log_sigma)

# tensorflow-probability를 활용하여 위에서 얻은 평균과 표준 편차로 정규 분포를 생성한다.
dist = tfd.Normal(loc=mu, scale=sigma)

# 정규 분포에서 행동을 추출한다.
action = dist.sample()

# tensorflow.Tensor로 된 값을 일반적인 숫자로 바꿔준다.
action = action.numpy()

# 행동 공간이 [-3, 3] 이므로, 그 안의 값으로 잘라준다.
action = np.clip(action, -3, 3)

print(action)
```

- Training  

```py
import numpy as np
from tensorflow import keras
from tensorflow_probability import distributions as tfd
import tensorflow as tf
from tqdm.auto import tqdm
import os

EPISODES_MAX = 1000
GAMMA = 0.99

reward_per_episode = np.zeros(30)
history = []
critic_objective = keras.losses.Huber()

pbar = tqdm(range(EPISODES_MAX), desc='Episode')

for episode in pbar:
    history.clear()
    episode_reward = 0

    done = False
    obs, _ = env.reset()

    while not done:
        state = keras.ops.expand_dims(obs, axis=0)

        # 위에서 봤던대로 행동을 추출한다.
        mu, log_sigma = actor(state)
        mu, log_sigma = keras.ops.squeeze(mu), keras.ops.squeeze(log_sigma)
        sigma = keras.ops.exp(log_sigma)
        action = tfd.Normal(loc=mu, scale=sigma).sample().numpy()
        action = np.clip(action, -3.0, 3.0)

        value = np.squeeze(
            critic(state)
        )
        obs, reward, terminated, truncated, _ = env.step((action,))

        episode_reward += reward
        done = terminated or truncated

        history.append((state, action, reward, value))

    batch_state = [exp[0] for exp in history]
    batch_action = [exp[1] for exp in history]
    batch_reward = [exp[2] for exp in history]
    batch_value = [exp[3] for exp in history]

    batch_return = []
    G = 0.0
    for reward in reversed(batch_reward):
        G = reward + GAMMA * G
        batch_return.insert(0, G)

    batch_state = keras.ops.concatenate(batch_state)
    batch_return = keras.ops.convert_to_tensor(batch_return, dtype='float32')
    batch_value = keras.ops.convert_to_tensor(batch_value)

    for _ in range(10):
        with tf.GradientTape() as tape:
            value = keras.ops.squeeze(
                critic(batch_state)
            )
            critic_loss = critic_objective(batch_return, value)
        critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)
        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))

    with tf.GradientTape() as tape:
        # 위에서 봤던대로 선택한 행동에 대한 로그 확률을 출력한다.
        mu, log_sigma = actor(batch_state)
        mu, log_sigma = keras.ops.squeeze(mu), keras.ops.squeeze(log_sigma)
        sigma = keras.ops.exp(log_sigma)
        log_prob = tfd.Normal(loc=mu, scale=sigma).log_prob(batch_action)

        target = batch_return - batch_value
        target = (target - keras.ops.mean(target)) / (keras.ops.std(target) + 1e-8)

        actor_loss = log_prob * target * (GAMMA ** np.arange(len(history)))
        actor_loss = -keras.ops.sum(actor_loss)
    actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)
    actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))

    reward_per_episode[episode % len(reward_per_episode)] = episode_reward
    running_reward = np.mean(reward_per_episode)

    pbar.set_postfix(
        actor_loss=f'{actor_loss:.5f}',
        critic_loss=f'{critic_loss:.5f}',
        reward=f'{episode_reward:.5f}',
        running_reward=f'{running_reward:.5f}',
    )

    if (episode + 1) % 200 == 0:
        os.makedirs('./pend', exist_ok=True)
        keras.models.save_model(actor, f'./pend/{episode}.keras')
```

<br>

- Evaluation  

```py
from tensorflow import keras
import kymnasium as kym
from typing import Any, Dict
import pickle

class InvertedPendulumAgent(kym.Agent):
    def __init__(
            self,
            model: keras.models.Model,
    ):
        self.model = model

    def save(self, path: str):
        keras.models.save_model(
            model=self.model,
            filepath=f'{path}.keras'
        )

    @classmethod
    def load(cls, path: str):
        model = keras.models.load_model(
            filepath=f'{path}.keras'
        )
        return InvertedPendulumAgent(model=model)

    def act(self, observation: Any, info: Dict):
        state = keras.ops.expand_dims(observation, axis=0)

        # 바로 전에는 행동을 확률 분포에서 추출했지만,
        # 학습이 완료되었다면 확률적 정책 대신 결정적 정책을 써야하므로
        # 평균을 그대로 행동으로 사용하겠다.
        mu, _ = self.model(state)
        mu = keras.ops.squeeze(mu)

        action = keras.ops.clip(mu, -2.0, 2.0)
        action = action.numpy()

        return (action, )
```

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 