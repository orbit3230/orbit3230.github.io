---
layout: post
title: "[인공지능] 2주차 (1) - Introduction to Reinforcement Learning"
excerpt: "Reinforcement Learning, Foundational Concepts"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-08
last_modified_at: 2025-09-08
---
## Introduction to Reinforcement Learning
### 1. Reinforcement Learning
- Law of Effect (Thorndike, 1911)  
  - 어떤 행동이 긍정적인 결과를 가져오면 그 행동을 반복하려는 경향이 생기고,  
    부정적인 결과를 가져오면 그 행동을 피하려는 경향이 생긴다.  

<br>

- 정의
  - Goal-directed 학습과 의사 결정의 이해와 자동화에 대한 계산적인 접근법
  - 환경과의 반복적인 상호작용에 기반함으로써 다른 학습 패러다임과 구별됨
  - 에이전트는 환경에 대한 완벽한 이해가 필요 없음

<br>

- 특징
  - (1) Optimization : 최대 보상을 얻을 수 있는 최적의 정책을 학습하는 것이 목표
    - 현재 선택한 행동에 대한 가치가 보상으로써 명시적으로 표현된다.

  - (2) Sequential Decision : 행동 선택은 순차적으로 여러 번 발생하며, 이전 시점에서 선택된 행동이 현재 행동 선택에 영향을 미친다.

  - (3) Delayed Consequencees : 현재 시점에서 선택한 행동은 현재 시점의 보상 뿐만 아니라 먼 미래의 보상에서도 영향을 줄 수 있다.

  - (4) Exploration : 최적의 정책을 학습하기 위해서 여러 행동들의 결과를 탐험(exploration) 해야 한다.

<br>

### 2. Foundational Concepts
- Basic Workflow
  - (1) 강화 학습 에이전트는 환경의 상태를 인지  
  ![basic_workflow_1)][def]  

  - (2) 정책을 바탕으로 인지된 상태에 대한 행동을 선택  
  ![basic_workflow_2)][def2]  

  - (3) 환경은 선택된 행동에 대해 보상을 제공하고, 새로운 상태로 변경  
  ![basic_workflow_3)][def3]  

  - (4) 상태-행동-보상을 참고하여, 현재 정책의 가치를 평가하고 더 나은 정책을 학습  
  ![basic_workflow_4)][def4]  

  <br>

- Agent
  - 환경과 상호작용하면서 주어진 목적을 달성할 수 있는 최적의 정책을 학습
  - 학습된 정책을 바탕으로 주어진 상태에 대한 행동을 선택  
  ![agent][def5]  

<br>

- Environment
  - 에이전트와 상호작용하되, 에이전트에 포함되지 않은 모든 것
  - 에이전트가 취하는 행동의 결과인 보상을 제공하며, 자신의 상태를 에이전트에게 드러냄  
  ![environment][def6]  

  <br>

- State
  - 에이전트가 관측할 수 있는 정보를 바탕으로 인지된 환경의 상태
  - 실제 환경의 상태와 에이전트에 의해 인지된 상태는 보통 같지 않지만, 많은 경우 같다고 가정 (Full Observability)  
  ![state][def7]  

<br>

- Action
  - 학습된 정책에 기반하여, 주어진 상태에서 선택한 의사 결정
  - 행동은 이산적(discrete)일 수도, 연속적(continuous)일 수도 있음  
  ![action][def8]  

  <br>

- Reward
  - 에이전트가 내린 행동의 결과로, 환경이 Scalar 값으로 제공하는 피드백
  - 환경이 명확한 보상을 지급하지 않는 경우도 많으므로, 적절한 보상의 설계가 RL의 주요 도전 과제 중 하나  
  ![reward][def9]  

<br>

- Policy
  - 상태를 입력으로, 출력을 행동으로 하는 함수로, 주어진 상태에 대한 에이전트의 행동을 결정  
  ![policy][def10]  
    - 결정론적 정책 (Deterministic Policy) : `a = π(s)`  
      - 주어진 상태에서 할 행동을 내는 정책. 동일한 상태에 대해 항상 동일한 행동을 선택
    - 확률론적 정책 (Stochastic Policy) : `π(a|s) = P[a|s]`  
      - 주어진 상태에서 행동을 선택할 확률을 내는 정책. 동일한 상태에 대해 다른 행동을 선택할 수 있음  

      <br>

- Return  
![return_evaluation][def11]  
  - 현재 시점에서 미래에 받을 수 있는 보상의 누적 합
  - 현재 시점에 가까울 수록 보상의 가치가 큼
  - 수익을 최대화 하는 것이 에이전트의 목적  
  ![return][def12]  

<br>

- Value Function
  - 정책의 가치를 평가하는 함수  
  ![value_function][def13]  
    - 상태 가치 함수(State-Value Function) : 정책이 주어졌을 때, 주어진 상태에서 얻을 수 있는 수익의 기대값  
    ![state_value_function][def14]  
    - 행동 가치 함수(Action-Value Function) : 정책이 주어졌을 때, 주어진 상태에서 특정 행동을 했을 때 얻을 수 있는 수익의 기대값  
    ![action_value_function][def15]  

<br>

- Model  
![model_evaluation][def16]  
  - 주어진 상태에서 에이전트가 내린 행동에 대해, 환경의 다음 반응을 예측하는 함수
  - 환경의 실제 모델은 알 수 없는 경우가 많으며, 오직 관측된 정보를 바탕으로 추정
  - 단, 모델은 강화 학습 에이전트를 학습시키는 데 필수적이지는 않음  
  ![model][def17]  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/SGyk7yd.png
[def2]: https://i.imgur.com/iNN0aKh.png
[def3]: https://i.imgur.com/8aXi3vH.png
[def4]: https://i.imgur.com/Bh8oGru.png
[def5]: https://i.imgur.com/pEFPc5p.png
[def6]: https://i.imgur.com/NnMFl4v.png
[def7]: https://i.imgur.com/bKUNoRm.png
[def8]: https://i.imgur.com/2eMy1az.png
[def9]: https://i.imgur.com/oe8y9IX.png
[def10]: https://i.imgur.com/DvXDXBK.png
[def11]: https://i.imgur.com/pdZE2F4.png
[def12]: https://i.imgur.com/By6Wjqx.png
[def13]: https://i.imgur.com/ew4TT1k.png
[def14]: https://i.imgur.com/vVWJywV.png
[def15]: https://i.imgur.com/AlEUcFy.png
[def16]: https://i.imgur.com/Ww4hbGm.png
[def17]: https://i.imgur.com/0br95nH.png