---
layout: post
title: "[인공지능] 3주차 (2) - Dynamic Programming"
excerpt: "Dynamic Programming, Value Iteration, Policy Iteration"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-18
last_modified_at: 2025-09-18
---
## Dynamic Programming
### 1. Dynamic Programming?
- Characteristic
  - Substructure : 큰 문제를 작은 문제로 분할
  - Table Structure : 작은 문제를 해결한 결과를 테이블 또는 변수 등에 저장
  - Bottom-Up Computation : 저장된 작은 문제에 대한 해결책을 합쳐서 큰 문제에 대한 해결책을 생성  

<br>

- Dynamic Programming을 사용하는 때
  - 작은 문제에 대한 최적의 해결책을 사용해서 큰 문제에 대한 최적의 해결책을 찾을 수 있거나(Optimal Substructure),  
  작은 문제에 대한 해결책이 반복적으로 재사용(Overlapping Subproblems)되는 경우  
  - Example : Floyd-Warshall Algorithm  
  ![floyd_warshall][def]  

<br>

- 벨만 방정식은 재귀적으로 정의되므로, 이전에 계산한 가치 함수의 값을 **저장하고 재사용**해야 함  
![bellman_equation][def2]  

<br>

### 2. Value Iteration
- 벨만 최적 방정식의 최적 상태 가치 함수를 반복적으로 계산하여 푸는 방법  
![value_iteration][def3]  
  - `V`<sub>`*`</sub><sup>`(k+1)`</sup>`(s)` : `k+1` 번째 반복에서의 최적 상태 가치 함수  

<br>

- 최적 상태 가치 함수의 추정이 완료되면, 최적 행동 가치 함수의 값이 최대가 되는 행동을 선택하는 **최적의 결정적 정책**을 생성할 수 있음  
![optimal_policy][def4]  

<br>

- Example  
![value_iteration_example_1][def5]  
![value_iteration_example_2][def6]  
  - 상태 가치 함수의 값은 `0`으로 초기화
  - 종료 상태에서는 얻을 수 있는 보상 및 수익이 없으므로, 상태 가치 함수의 값은 `0`으로 유지  
![value_iteration_example_3][def7]  
  - 계산의 편의를 위해 할인율 `γ`를 `1`로 설정  
![value_iteration_example_4][def8]  
![value_iteration_example_5][def9]  
![value_iteration_example_6][def10]  
![value_iteration_example_7][def11]  
![value_iteration_example_8][def12]  
![value_iteration_example_9][def13]  
![value_iteration_example_10][def14]  
![value_iteration_example_11][def15]  
![value_iteration_example_12][def16]  
![value_iteration_example_13][def17]  
![value_iteration_example_14][def18]  

- 따라서 Optimal Policy는 다음과 같음  
![optimal_policy_example][def19]  
  - 상태가 Normal 일 때는 항상 Overaccel
  - 상태가 Heated 일 때는 항상 Accel  

- Pseudo Code  
![value_iteration_pseudo_code][def20]  

- Synchronous vs. Asynchronous Backup  
![synchronous_asynchronous_backup][def21]  
  - Synchronous Backup : 매 단계가 끝나고 난 후 모든 상태에 대해 업데이트. 항상 이전 단계의 가치 함수를 활용  
  ![synchronous_backup][def22]  
  - Asynchronous Backup : 매 상태마다 가치 함수의 값을 업데이트. 항상 최신의 가치 함수를 활용  
  ![asynchronous_backup][def23]  

  <br>

### 3. Policy Iteration
- 현재 정책에 대한 벨만 기대 방정식의 상태 가치 함수를 추정  
![policy_evaluation][def24]  

<br>

- Policy Improvement
  - 현재 정책에 대한 상태 가치 함수의 추정이 완료되면, 현재 정책에서 추정된 행동 가치 함수의 값이 최대가 되는 행동을 선택(Greedy Policy)하는 **더 나은 정책**을 생성할 수 있음  
  ![policy_improvement][def25]  

- Policy Iteration = [Evaluation + Improvement]의 반복  
![policy_iteration][def26]  
  - Until, 업데이트 된 정책에 변화가 없어질 때 까지.  
  - 정책에 변화가 없음 = 더 나은 정책이 없음 = 최적의 정책을 찾았음
  - 이러한 강화 학습 기법을 **Generalized Policy Iteration (GPI)** 라고 함  
  ![gpi][def27]  

- Pseudo Code  
![policy_iteration_pseudo_code][def28]  

<br>

- 단순히 가치 함수의 값이 큰 행동을 선택했을 때, **이전 정책보다 더 나은 정책**이 되는가?  
![question][def29]  

- Policy Improvement Theorem  
![policy_improvement_theorem][def30]  
  - 두 정책 `π`와 `π'`이 위 조건을 만족하면, `π`≤`π'`가 성립  

- 따라서 이전 정책에 대한 행동 가치 함수의 값이 큰 행동을 선택하는 정책은, Policy Improvement Theorem을 만족하므로 이전 정책 보다 더 나은 정책이 됨  
![better_policy][def31]  

<br>

- Value Iteration vs. Policy Iteration  
  - Value Iteration : 최적 상태 가치 함수가 수렴할 때까지 업데이트를 반복
  - Policy Iteration : 정책이 수렴할 때까지 업데이트를 반복  
  - 대다수의 강화 학습 문제에서는 최적 상태 가치 함수가 수렴하기 전에 이미 최적의 정책을 찾는 경우가 많음  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 

[def]: https://i.imgur.com/cV8FuKP.png
[def2]: https://i.imgur.com/bJI4YC7.png
[def3]: https://i.imgur.com/epcNhz1.png
[def4]: https://i.imgur.com/UFmrdqg.png
[def5]: https://i.imgur.com/IIeCNyY.png
[def6]: https://i.imgur.com/CRb4dGo.png
[def7]: https://i.imgur.com/OMUrjVp.png
[def8]: https://i.imgur.com/JscxMwB.png
[def9]: https://i.imgur.com/JCICFSC.png
[def10]: https://i.imgur.com/hIdOMXo.png
[def11]: https://i.imgur.com/ck1VQG0.png
[def12]: https://i.imgur.com/m5BZQgD.png
[def13]: https://i.imgur.com/aRoF7in.png
[def14]: https://i.imgur.com/ccX7CLW.png
[def15]: https://i.imgur.com/V8NVQOP.png
[def16]: https://i.imgur.com/ERvC66x.png
[def17]: https://i.imgur.com/E2KhSbi.png
[def18]: https://i.imgur.com/wjN9xnA.png
[def19]: https://i.imgur.com/634bzaC.png
[def20]: https://i.imgur.com/Ou6nYdw.png
[def21]: https://i.imgur.com/0VqM3Zx.png
[def22]: https://i.imgur.com/vMQ8fLS.png
[def23]: https://i.imgur.com/2FX3xYX.png
[def24]: https://i.imgur.com/tosB8dk.png
[def25]: https://i.imgur.com/2yNpAYe.png
[def26]: https://i.imgur.com/EbBSGhH.png
[def27]: https://i.imgur.com/EKc36il.png
[def28]: https://i.imgur.com/RN3q9KC.png
[def29]: https://i.imgur.com/JapUtes.png
[def30]: https://i.imgur.com/Sw5fPql.png
[def31]: https://i.imgur.com/fJbBLV3.png