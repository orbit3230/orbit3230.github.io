---
layout: post
title: "[인공지능] 3주차 (2) - Dynamic Programming"
excerpt: "Dynamic Programming, Value Iteration, Policy Iteration"

tags:
  - [인공지능, Python]

toc: true

date: 2025-09-18
last_modified_at: 2025-09-18
---
## Dynamic Programming
### 1. Dynamic Programming?
- Characteristic
  - Substructure : 큰 문제를 작은 문제로 분할
  - Table Structure : 작은 문제를 해결한 결과를 테이블 또는 변수 등에 저장
  - Bottom-Up Computation : 저장된 작은 문제에 대한 해결책을 합쳐서 큰 문제에 대한 해결책을 생성  

<br>

- Dynamic Programming을 사용하는 때
  - 작은 문제에 대한 최적의 해결책을 사용해서 큰 문제에 대한 최적의 해결책을 찾을 수 있거나(Optimal Substructure),  
  작은 문제에 대한 해결책이 반복적으로 재사용(Overlapping Subproblems)되는 경우  
  - Example : Floyd-Warshall Algorithm  
  ![floyd_warshall](TODO)  

<br>

- 벨만 방정식은 재귀적으로 정의되므로, 이전에 계산한 가치 함수의 값을 **저장하고 재사용**해야 함  
![bellman_equation](TODO)  

<br>

### 2. Value Iteration
- 벨만 최적 방정식의 최적 상태 가치 함수를 반복적으로 계산하여 푸는 방법  
![value_iteration](TODO)  
  - `V`<sub>`*`</sub><sup>`(k+1)`</sup>`(s)` : `k+1` 번째 반복에서의 최적 상태 가치 함수  

<br>

- 최적 상태 가치 함수의 추정이 완료되면, 최적 행동 가치 함수의 값이 최대가 되는 행동을 선택하는 **최적의 결정적 정책**을 생성할 수 있음  
![optimal_policy](TODO)  

<br>

- Example  
![value_iteration_example_1](TODO)  
![value_iteration_example_2](TODO)  
  - 상태 가치 함수의 값은 `0`으로 초기화
  - 종료 상태에서는 얻을 수 있는 보상 및 수익이 없으므로, 상태 가치 함수의 값은 `0`으로 유지  
![value_iteration_example_3](TODO)  
  - 계산의 편의를 위해 할인율 `γ`를 `1`로 설정  
![value_iteration_example_4](TODO)  
![value_iteration_example_5](TODO)  
![value_iteration_example_6](TODO)  
![value_iteration_example_7](TODO)  
![value_iteration_example_8](TODO)  
![value_iteration_example_9](TODO)  
![value_iteration_example_10](TODO)  
![value_iteration_example_11](TODO)  
![value_iteration_example_12](TODO)  
![value_iteration_example_13](TODO)  
![value_iteration_example_14](TODO)  
![value_iteration_example_15](TODO)  

- 따라서 Optimal Policy는 다음과 같음  
![optimal_policy_example](TODO)  
  - 상태가 Normal 일 때는 항상 Overaccel
  - 상태가 Heated 일 때는 항상 Accel  

- Pseudo Code  
![value_iteration_pseudo_code](TODO)  

- Synchronous vs. Asynchronous Backup  
![synchronous_asynchronous_backup](TODO)  
  - Synchronous Backup : 매 단계가 끝나고 난 후 모든 상태에 대해 업데이트. 항상 이전 단계의 가치 함수를 활용  
  ![synchronous_backup](TODO)  
  - Asynchronous Backup : 매 상태마다 가치 함수의 값을 업데이트. 항상 최신의 가치 함수를 활용  
  ![asynchronous_backup](TODO)  

  <br>

### 3. Policy Iteration
- 현재 정책에 대한 벨만 기대 방정식의 상태 가치 함수를 추정  
![policy_evaluation](TODO)  

<br>

- Policy Improvement
  - 현재 정책에 대한 상태 가치 함수의 추정이 완료되면, 현재 정책에서 추정된 행동 가치 함수의 값이 최대가 되는 행동을 선택(Greedy Policy)하는 **더 나은 정책**을 생성할 수 있음  
  ![policy_improvement](TODO)  

- Policy Iteration = [Evaluation + Improvement]의 반복  
![policy_iteration](TODO)  
  - Until, 업데이트 된 정책에 변화가 없어질 때 까지.  
  - 정책에 변화가 없음 = 더 나은 정책이 없음 = 최적의 정책을 찾았음
  - 이러한 강화 학습 기법을 **Generalized Policy Iteration (GPI)** 라고 함  
  ![gpi](TODO)  

- Pseudo Code  
![policy_iteration_pseudo_code](TODO)  

<br>

- 단순히 가치 함수의 값이 큰 행동을 선택했을 때, **이전 정책보다 더 나은 정책**이 되는가?  
![question](TODO)  

- Policy Improvement Theorem  
![policy_improvement_theorem](TODO)  
  - 두 정책 `π`와 `π'`이 위 조건을 만족하면, `π`≤`π'`가 성립  

- 따라서 이전 정책에 대한 행동 가치 함수의 값이 큰 행동을 선택하는 정책은, Policy Improvement Theorem을 만족하므로 이전 정책 보다 더 나은 정책이 됨  
![better_policy](TODO)  

<br>

- Value Iteration vs. Policy Iteration  
  - Value Iteration : 최적 상태 가치 함수가 수렴할 때까지 업데이트를 반복
  - Policy Iteration : 정책이 수렴할 때까지 업데이트를 반복  
  - 대다수의 강화 학습 문제에서는 최적 상태 가치 함수가 수렴하기 전에 이미 최적의 정책을 찾는 경우가 많음  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown=   "1">

이 포스팅은 강원대학교 최우혁 교수님의 인공지능 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details> 