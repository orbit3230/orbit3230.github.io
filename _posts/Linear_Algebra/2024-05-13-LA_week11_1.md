---
layout: post
title: "[선형대수학] 11주차 (1) - QR factorization"
excerpt: "Matrix Power, Paths in a directed graph, Matrices with orthonormal columns, QR factorization"

tags:
  - [선형대수학, Math]

toc: true

date: 2024-05-13
last_modified_at: 2024-05-13
---
## Matrix Power
- 말 그대로 **Matrix의 제곱**이다.  

### 1. Matrix Power
- 조건
  - matrix `A`에 자기 자신을 multiply 하는 형태인 `AA`가 성립하려면,  
  `A`는 **반드시 square matrix** 여야만 하겠다.  

- `AA`를 우리는 `A`<sup>`2`</sup> 라고 부르기로 했고,  
Matrix의 제곱이다.  

- 만약 `k`와 `l`이 positive integer이고, `A`가 sqaure라면   
이와 같은 제곱의 성질도 만족한다.  

  - (1) `A`<sup>`k`</sup>`A`<sup>`l`</sup>`= A`<sup>`k+l`</sup>
  - (2) `(A`<sup>`k`</sup>`)`<sup>`l`</sup>`= A`<sup>`kl`</sup>

<br>

- 위와 같은 제곱의 성질들이 성립하려면, 하나의 **convention**을 정립해야한다.  

  - `A`<sup>`0`</sup>`= I` (`I`dentity matrix)  

  <br>

### 2. Paths in a directed graph
- **directed graph**에 대한 `n x n` dimension **adjacency matrix** `A`가 있다.  

![graph_and_adjacency_matrix](https://i.imgur.com/7mY687U.png)

- 그런데 이 **adjacency matrix를 제곱** 시켜보자.  
![adjacency_matrix_square][def]  
  - 각각의 element들은 무엇을 의미할까?  

- Square of adjacency matrix `A`의 각 `ij` 번째 element는 식으로 이렇게 표현된다.  
![square_of_adjacency_matrix_elements][def2]  
  - 즉, `i` -> `k`(어느 한 노드) -> `j` 로 ***두 단계***를 거쳐  
  결론적으로 `i` -> `j`로 **이동하는 경우의 수**를 나타내게 된다.  

- 이를 더 **일반화** 해보자. (with **mathematical induction**)  
  - adjacency matrix의 `l+1` 제곱은, 이렇게 표현된다.  
  ![l+1_power_of_adjacency_matrix][def3]  
    - 즉, 각 element는 **`l+1` 번** 만에 `i` -> `j`로 이동하는 경우의 수이며,  
    이는 곧 **`l`번**만에 `j` -> `k`로 이동한 후  
    마지막 **한 번**의 이동으로 `k` -> `i`로 이동하는 것과 같으므로  
    귀납적으로 증명이 된다.  

  - 따라서, adjacency matrix의 `l` 제곱은 곧,  
  **`l` 번의 이동**으로 `i` -> `j`로 **이동하는 경우의 수**를 나타낸다.  

<br>

## QR factorization
### 1. Matrices with orthonormal columns  
- **Gram matrix `A`<sup>`T`</sup>`A = I`인 특수한 경우**에,  
우리는 `A`의 **column**들인 `a`<sub>`1`</sub>`, ..., a`<sub>`k`</sub>는 **orthonormal** 하다고 말할 수 있었다.  

  - 여기서 더 나아가,  
  `A`<sup>`T`</sup>`A = I` 이면서 `A`가 **square matrix**라면,  
  `A`의 column들인 `a`<sub>`1`</sub>`, ..., a`<sub>`k`</sub>는 ***orthonormal basis*** 라고도 말할 수 있다.  

  <br>

- **Properties**
  - 이렇게 matrix `A`의 **column**들이 **orthonormal** 한 경우,  
  vector와의 multiplication에서 굉장히 특이한 **property**들이 발현된다.  

  - `f : R`<sup>`n`</sup>`-> R`<sup>`m`</sup> 은 vector `z`를 `Az`로 mapping 시키는 function 일 때,  

    - (1) `∥Ax∥ = ∥x∥` (`f` is *norm preserving*)  
    - (2) `(Ax)`<sup>`T`</sup>`(Ay) = x`<sup>`T`</sup>`y` (`f` *preserves the inner product between vectors*)  
    - (3) `∠(Ax, Ay) = ∠(x, y)` (`f` *preserves angles between vectors*)  
    <br>
    - 이들에 대한 증명은,  
    [1] **`A`<sup>`T`</sup>`A = I` 였던 성질**과  
    [2] **`I` multiply 'vector'** is **vector itself** 임을 이용하면 되겠다.  

    <br>

### 2. QR factorization
- 이제 우리는 **Gram-Schmidt Algorithm의 결과**를 **matrix들을 이용**한 compact form으로 표현할 수 있다.  

- `A`는 `n x k` matrix이며, column들인 `a`<sub>`1`</sub>`, ..., a`<sub>`k`</sub>는 **linearly independent** 하다.  
  - **independence-dimension inequality**에 따라, `A`는 반드시 **tall** or **square**(basis) 여야만 할 것이다. (obviously)  

- 자, 만약 Gram-Schmidt가 `A`의 columns `a`<sub>`1`</sub>`, ..., a`<sub>`k`</sub>에 적용되면,  
**orthonormal vectors** `q`<sub>`1`</sub>`, ..., q`<sub>`k`</sub>가 결과로 나오게 될 것이다.  
  - `n x k` **matrix `Q`**를 `q`<sub>`1`</sub>`, ..., q`<sub>`k`</sub> column들을 가지는 matrix로 정의하자.  
  - `Q`<sup>`T`</sup>`Q = I`  

  <br>

- **Gram-Schmidt algorithm** 으로부터, `a`<sub>`i`</sub>를 식으로 도출해낸다.  
![a_i_from_gram_schmidt][def4]
  - 복잡한 coefficient 대신, **`R`로 치환**한다.  
  - Gram-Schmidt 의 [1] Orthogonalization & [3] Normalization 으로부터 파생된 식이다.  
  ![gram_schmidt_steps][def5]  
  <br>
  - 이렇게 되면 결국 `R`<sub>`ij`</sub>`= q`<sub>`i`</sub><sup>`T`</sup>`a`<sub>`j`</sub> 이며, (for `i < j` -> positive diagonal entries)  
  `R`<sub>`ii`</sub>`= ∥q_tilde∥` (for diagonal position)  
  나머지(for `i > j`)는 모두 `0`인, **upper triangular matrix `R`**임을 알 수 있다.  

- **최종적으로 `A = QR` 로 작성**되며, 이를 `A`의 ***QR factorization*** 이라고 부른다.  

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 김도형 교수님의 선형대수학 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details>

[def]: https://i.imgur.com/D6T30e9.png
[def2]: https://i.imgur.com/N9PS123.png
[def3]: https://i.imgur.com/XKMkzbo.png
[def4]: https://i.imgur.com/1GlgKi0.png
[def5]: https://i.imgur.com/P5y2zC0.png