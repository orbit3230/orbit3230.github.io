---
layout: post
title: "[선형대수학] 2주차 (3) - Norm and Distance"
excerpt: "Norm, RMS value, Chebyshev inequality, Distance, Standard deviation, Cauchy-shuwarz inequality"

tags:
  - [선형대수학, Math]

toc: true

date: 2024-03-14
last_modified_at: 2024-03-14
---
## Norm
### 1. Norm 이란
- **Norm**은 어떠한 `n`-vector `x`에 대한 규모(크기)를 측정하는 것이다.  

  - `n`-vector `x`에 대한 **norm**  값은, `∥x∥` 라고 표현한다.  
  이 `∥x∥` 값은 모든 원소의 제곱의 합의 제곱근이다.  
  ![norm][def]

    - 만약 `n=1` 이면, 해당 vector의 **norm** 값은 원소의 절댓값일 것이다. (당연히도)  
    ![norm_n_is_1](https://i.imgur.com/BqI2bOk.png)  

    <br>

- 따라서 vector의 **norm** 값은 해당 vector의 크기를 나타내므로,  
Vector 간 대/소비교를 할 때는 **norm** 값을 기준으로 따진다.  
**norm** 값이 큰 vector가 더 큰 것이다.  

<br>

- **norm** 에도 여러 성질들이 적용된다.  

  - (1) Nonnegative homogeneity  
  ![nonnegative_homogeneity][def2]

  - (2) Triangle inequality  
  ![triangle inequality][def3]
    - 이 성질에 대한 증명은 이후 코시-슈바르츠 부등식과 응용하여 [등장한다.][def33]

  - (3) Nonnegativity  
  ![nonnegativity][def4]

  - (4) Definiteness  
  ![definitness][def5]

<br>

### 2. RMS value
- 하지만 **norm** 값을 기준으로 두 Vector의 대/소를 판별하기에는  
다소 `n` 값의 영향이 크다. 단순히 원소의 개수가 압도적으로 차이난다면,  
norm 값이 클 수밖에 없다. 이는 불공평하다.  

  - 따라서, norm 값을 `n` 값에 따라 표준화 시킨 값이, ***RMS value*** 이다.  
  (RMS : Root-Mean-Square)  

- `n`-vector `x`에 대한 *Mean-Square value*
![mean-square_value][def6]

- `n`-vector `x`에 대한 ***<u> Root-Mean-Square value(RMS value) </u>***
![root-mean-square_value][def7]

- 따라서, `rms(x)`는 원소들의 절댓값에 대한 ***Typical*** 한 값을 제공한다.  
- **RMS value**는 서로 dimension(`n`)이 다른 vector의 norm 값 간 비교에 유용한다.  
(왜? 각각의 `n`으로 나누기 때문에)

<br>

### 3. Norm of a Sum
- 두 Vector `x`와 `y`의 합에 대한 norm 값은, 
![norm_of_a_sum][def8]
  - 왜냐하면 vector의 제곱은 알다시피 같은 vector 두 개를 **inner product** 취한 것과 같기 때문이다.  
  ![norm_of_a_sum_proof][def9]  

  <br>

### 4. Norm of Block vectors
- 만약 `a`, `b`, `c`가 vector라면, `d = (a, b, c)`는 **block vecter** 라고 말한다고 했다.  

  - 그렇다면 `∥d∥`<sup>`2`</sup>은 어떻게 표현할 수 있을까?  
    ![dtd](https://i.imgur.com/BKGldzV.png)
    - 이처럼 `d`<sup>`T`</sup>`d`는 `a`<sup>`T`</sup>`a`, `b`<sup>`T`</sup>`b`, `c`<sup>`T`</sup>`c`의 합으로 구성된다.  
  - 따라서, 다시말해 **block vector** `d`의 norm 값은,  
  **요소** vector **각각의 norm 값에 대한 norm 값**이다.  
  ![norm_of_blockvector][def10]

<br>

### 5. Chebyshev inequlity
- 가정해보자.
  - `x`는 `n`-vector 이다.
  - `k`는 `|x`<sub>`i`</sub>`| >= a`를 만족하는 원소 `x`<sub>`i`</sub> 의 **개수**이다.

- 그렇다면 우리는, 어떠한 조건을 만족하는 원소의 개수인 `k`는  
일단 `0` 이상 `n` 이하라고 둘 수 있을 것이다.  
  - 이러한 우리의 추측에서, 추측가능한 범위를 줄여주는 부등식이 바로 Chebyshev 부등식이다.  
  ![chebyshev_inequality][def11]

<br>

- **만약** 조건 `a`가 `∥x∥`(vector의 norm 값) 보다 크다면, (`a > ∥x∥`)
![chebyshev_k_is_0](https://i.imgur.com/Z0cXnCp.png)
  - 이 되어버린다. (분모가 분자보다 더 크기 때문)  
- 따라서 `k`는 무조건 정수(integer)이기 때문에, `k = 0` 일 수밖에 없다.  

<br>

- **RMS value**의 관점에서 보자면, Chebyshev inequality를 이렇게도 표현할 수 있다.  
![rms_chebyshev][def12]  
  - 왜냐하면, 
  ![rms_value][def7]  
  이기 때문이다.

- 그런데, 여기서 `k/n`은 다시 말해 전체 원소의 개수(`n`) 중 조건을 만족하는 원소의 개수 `k`의 ***비율(Proportion)*** 이라고도 말할 수 있다.  

  - 그러므로 예를 들어 `a = 5 rms(x)` 라면,  
  조건을 만족하는 **원소 개수의 비율은 4%(1/25) 를 넘을 수가 없겠다!**

  <br>

## Distance
### 1. Distance 란
- 두 `n`-vector `a`와 `b`가 있을 때, 두 vector 간 Eudlidean **distance** 는 다음과 같다.  
![distance][def13]

  - 두 `n`-vector `a`와 `b`간 *RMS 편차*는 다음과 같을 것이다.
  ![rms_deviation][def14]

- 만약 `n`-vector `a`와 `b`, `c`가 존재할 때,  
`a`가 `b` 또는 `c` 중에서 어느 것과 더 가까운지(closer) 비교하려면 **distance**를 계산하면 된다.  

  - 만약 `∥a - b∥`가 `∥a - c∥` 보다 작다면, vector `a`는 `c`보다 `b`와 더 가까운 셈이 되겠다.  

<br>

### 2. Triangular inequality
- [위에서 본][def31] norm의 성질 중 하나인 **Triangular inequality**를,  
![triangular_inequality][def3]  
**distance**의 개념으로 보면 더 직관적으로 알 수 있다.  
![triangular_distance][def15]
우리가 초등학생 때 보던 삼각형의 성질, 바로 그것이다.  

<br>

### 3. Feature distance and Nearest neighbors
- `x`와 `y`가 두 개체에 대한 **feature vectors** 일 때,  
`∥x - y∥`는 featured distance 이다.  
가까울수록(distance가 작을 수록), 더 similarity한 feature를 가진다고 볼 수 있다.  

<br>

- 만약 vector의 list `z`<sub>`1`</sub>`, z`<sub>`2`</sub>`, ...., z`<sub>`m`</sub> 이 존재할 때,  
만약 다음과 같은 부등식이 참이라면 `z`<sub>`j`</sub>가 `x`와 가장 가까운 것이다.  
![nearest_neighbor][def16]

<br>

### 4. Document dissimilarity(유사성 검사)
- **Distance**를 활용하는 예를 알아보자.  

- 여러 article들에 대하여, article들이 서로 얼마나 관련이 있는 내용을 갖는지를 알고싶다.  

  - (1) articles에 대한 **Word count histogram**을 vector로 표현한다.  
  - (2) 그리고 vector들(단어 출현횟수 데이터)에 대한 서로의 **Distance**를 비교하였을 때,  
  **Distance**가 작을 수록 두 article의 내용은 similarity를 갖는다고 볼 수 있다.
  ![document_dissimilarity][def17]

  <br>

## Standard deviation(표준편차)
### 1. Average와 Standard deviation
- `n`-vector `x`에 대하여, `avg(x)`는 이렇게 나타낸다.
![avg][def18]
  - **Ones Vector**를 **inner product** 하는 이유는, 각각의 **원소들의 합을 Scalar 값으로** 얻어내기 위해서이다.  

<br>

- 따라서 `x`의 Standard deviation(표준 편차)는 다음과 같이 정의된다.
![standard deviation][def19]

<br>

- **De-meaned vecter x&#771;** 는, 이렇게 정의된다.  
![de-meaned_vecter][def20]  
  - De-meaned vecter에 대한 평균 avg(x&#771;)는, 당연하게도 `0`이다.  
  편차들의 평균이므로 `0`이 될 수밖에 없다.

  - 그러므로, 결국 De-meaned vecter의 Norm 값을 `n`의 제곱근을 로 나눈 것이  
  곧 표준편차(`std(x)`)이고,  
  곧 x&#771;의 RMS value 일 것이다.
  ![rms_value_of_de-meaned_vector][def21]

  - 다시말해, `std(x)`는 각각의 원소들이 `avg(x)`로 부터 얼마나 떨어져 있는 지에 대한  
  ***Typical*** 한 값을 제공한다. - 이는 위에서 봤듯이 **RMS value**를 뜻하는 문장이다.

- `std(x) = 0` 이라면, 모든 원소들의 Scalar 값이 `α`로 똑같은,  
`x = α1`과 같은 상수함수일 것이다.  

<br>

- 주로 그리스 문자 `μ`, `σ` 가 각각 mean(평균)과 standard deviation(표준편차)를 나타내는데 주로 사용된다.

<br>

### 2. Basic formula
- 잘 따라오고 있다면, `n`-vector `x`에 대한 `avg(x)`와 `std(x)`이, `rms(x)`와 유사한 점을 가지고 있다는 점을 느꼈을 것이다.  

- 이러한 공식이 성립한다.  
![rms_avg_std][def22]
  - 이에 대한 증명이다.
  ![rms_avg_std_proof][def23]
    - 참고로 두 번째 줄 -> 세 번째 줄로 넘어갈 때 마지막 항의 `n`이 어디서 나온거냐면,  
    `1`<sup>`T`</sup>`1` 은 `n`이기 때문에 생겨난 것이다.

    <br>

### 3. Mean return and risk
- 평균과 표준편차를 다루는 예시로서, 수익(**return**)과 위험도(**risk**)를 다루어 볼 수 있겠다.  

- 어떠한 투자나 자산에 대한 일정 기간동안의 일련의 return(수익)값을,  
`x`라고 하자.  
  - `avg(x)`는, 해당 기간동안의 return의 평균이며, 보통 이 것을 주로 **return** 이라고 부른다.  
  - `std(x)`는, 해당 기간 동안의 returnd이 얼마나 요동치는지.  
  즉, **risk** 라고 부른다.  

- 다양한 투자 방법들이, 이러한 **return**과 **risk**의 관점에서 비교되어 지곤 한다.
- 종종 risk-return 점 그래프로 표현되어진다.  
![risk-return-plot][def24]
  - 당신은 어떠한 investment를 선택할 것인가?

<br>

### 4. Chebyshev inequality for standard deviation
- `x`는 평균으로 `avg(x)`를, 표준편차로 `std(x)`를 가지는 `n`-vector 이다.

- [앞에서][def32] Chebyshev inequality는, 특정한 조건(`|x`<sub>`i`</sub>`| >= a`)을 만족하는 원소의 개수의 **비율**이  
`(rms(x)/a)`<sup>`2`</sup>를 넘을 수 없다고 하였다.  

  - 이를 조금만 변형하여, ![chebyshev_expression][def25] 조건을 충족하는 원소의 개수의 **비율**은,  
  ![proportion_no_more_than][def26]  
  을 넘을 수 없음을 이끌어 낼 수 있다.  

    - `rms(x - avg(x)1) / std(x)`이 어떻게 `1`로 약분되나요?  
    ![rms(x-avg(x)1/std(x)_is_1)][def27]

  - 따라서 만약 `μ = 8`, `σ = 3`인 흐름이 있을 때,  
  `x`<sub>`i`</sub>`<= 0`, 즉 손실(risk)이 나려면  
  `α <= 3/8` 이어야 하기 때문에,  
  손실이 날 수 있는 경우의 개수 **비율**은 전체의 `(3/8)`<sup>`2`</sup>, 즉 `14.1%`을 넘을 수 없다.  

  <br>

## Angle
### 1. Cauchy-Schwarz ineqaulity
- 우리가 고등학교 때 배우던 그 코시-슈바르츠 부등식 맞다.

- 두 `n`-vector `a`와 `b`에 대하여, 다음 부등식이 항상 성립한다.  
![cauchy-schwarz][def28]

  - 이를 이용하여, 드디어 [triangle inequality][def31]를 증명할 수 있게 되었다.  
  ![cauchy-schwarz_triangle_inequlity_proof][def29]
    - 두 번째 줄에 등호가 아니라 부등호이다. 이게 핵심이다.  

<br>

- **Cauchy-Schwarz inequality**를 유도해보자.  

  - `α = ∥a∥`, `β = ∥b∥`라고 가정하자. 다음과 같은 식을 작성하면,  
  ![derivation][def30]
    - 마지막을 `2∥a∥∥b∥`으로 나누어,  
    `a`<sup>`T`</sup>`b ≤ ∥a∥∥b∥`을 결론적으로 이끌어 낼 수 있다.  
    - 결국 `a`또는`b` 중 하나에 음수를 inner product 하여 `-a`<sup>`T`</sup>`b`가 되어도,  
    `-a`<sup>`T`</sup>`b ≤ ∥a∥∥b∥`또한 참이다.  
    따라서,  
    ![cauchy-schwarz][def28]
  
[다음 주에 계속됩니다.](https://orbit3230.github.io/2024/03/18/LA_week3_1/)

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 김도형 교수님의 선형대수학 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details>

[def]: https://i.imgur.com/yp5BAUh.png
[def2]: https://i.imgur.com/YFJYt0L.png
[def3]: https://i.imgur.com/iIZGzuS.png
[def4]: https://i.imgur.com/KhOq0bN.png
[def5]: https://i.imgur.com/NmGiin7.png
[def6]: https://i.imgur.com/ExFDVWV.png
[def7]: https://i.imgur.com/kAPp1cg.png
[def8]: https://i.imgur.com/Q8FV6wK.png
[def9]: https://i.imgur.com/iMtVdRH.png
[def10]: https://i.imgur.com/GZffJBI.png
[def11]: https://i.imgur.com/N312usu.png
[def12]: https://i.imgur.com/sKTfvKN.png
[def13]: https://i.imgur.com/fgn5RWL.png
[def14]: https://i.imgur.com/B50uG6Z.png
[def15]: https://i.imgur.com/fHQgC0G.png
[def16]: https://i.imgur.com/fHQgC0G.png
[def17]: https://i.imgur.com/4H22JWt.png
[def18]: https://i.imgur.com/ELoeAf9.png
[def19]: https://i.imgur.com/eZ589FL.png
[def20]: https://i.imgur.com/3OMeL3V.png
[def21]: https://i.imgur.com/1TRWKoh.png
[def22]: https://i.imgur.com/PNtY6dl.png
[def23]: https://i.imgur.com/twz8KXF.png
[def24]: https://i.imgur.com/JLNxwfo.png
[def25]: https://i.imgur.com/14fi6uF.png
[def26]: https://i.imgur.com/g1brfLy.png
[def27]: https://i.imgur.com/WiZruo4.png
[def28]: https://i.imgur.com/G6MtlW4.png
[def29]: https://i.imgur.com/xn53H40.png
[def30]: https://i.imgur.com/hTH20lP.png
[def31]: https://orbit3230.github.io/2024/03/14/LA_week2_3/#1-norm-%EC%9D%B4%EB%9E%80
[def32]: https://orbit3230.github.io/2024/03/14/LA_week2_3/#5-chebyshev-inequlity
[def33]: https://orbit3230.github.io/2024/03/14/LA_week2_3/#1-cauchy-schwarz-ineqaulity