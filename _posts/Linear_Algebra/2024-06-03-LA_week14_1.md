---
layout: post
title: "[선형대수학] 14주차 (1) - Least squares data fitting"
excerpt: "Setup, Data, Model, Least sqaures data fitting"

tags:
  - [선형대수학, Math]

toc: true

date: 2024-05-30
last_modified_at: 2024-05-30
---
## Least squares data fitting
### 1. Setup
- scalar `y`와 `n`-vector `x`가 model에 의해 연관지어진다고 믿자.  
`y ≈ f(x)`  

  - `x` : vector of independent variables 라고 불린다.  
  (***feature vector***)

  - `y` : outcome or response variable 이라고 불린다.  
  (***somthing we want to predict***)  

- `f : R`<sup>`n`</sup>`-> R`은 `x`와 `y`간의 관계를 제공한다.  

  => 하지만 우리는 `f`, 즉 `x`와 `y` 사이의 **'true' relationship을 제공하는 이 `f`를 모른다.**  

  <br>

### 2. Data
- 어떠한 **데이터**가 주어졌다.  
![data][def]  

  - `n`-vector `x`<sup>`(i)`</sup> (feature vector), scalar `y`<sup>`(i)`</sup>은 data sample `i`에 대한 것이다.  

  - `x`<sup>`(i)`</sup>, `y`<sup>`(i)`</sup>를 한 쌍으로하여 `i`<sup>`th`</sup>**data pair**라고 부른다.  
    - 또는 *observations*, *examples*, *samples* or *measurements* 라고도 불린다.  
    - `x`<sub>`j`</sub><sup>`(i)`</sup>는 `x`<sup>`(i)`</sup>의 `j`<sup>`th`</sup> feature이다.  

    <br>

### 3. Model
- Model `f̂ : R`<sup>`n`</sup>`-> R`은 `f`의 **guess** 혹은 **approximation**이다.  
  - *model*, *prediction function*, *predictor* 등 다양하게 불린다.  

- 예를들면, ***linear in the parameters*** 방법이 존재한다.  
![linear_in_the_parameters][def2]  
  - `f`<sub>`i`</sub>`: R`<sup>`n`</sup>`-> R` 은 우리가 선택한 basis functions (or feature mapping).  
  - `θ`<sub>`i`</sub> 들은 우리가 선택한 model parameters.  

- 우리의 목표는 `i = 1, 2, ..., N`에 대하여 `y`<sup>`(i)`</sup>`≈ f̂(x`<sup>`(i)`</sup>`)`에 근사하도록 하는 `f̂`를 찾는 것이 되겠다.  

- `ŷ`<sup>`(i)`</sup>`≈ f̂(x`<sup>`(i)`</sup>`)` 은 `y`<sup>`(i)`</sup>에 대한 prediction이다.  

<br>

### 4. Least squares data fitting
- Prediction error (or residual) : `r`<sup>`(i)`</sup>`= y`<sup>`(i)`</sup>`- ŷ`<sup>`(i)`</sup>

- `N`-vectors `y`<sup>`(i)`</sup>, `ŷ`<sup>`(i)`</sup>, 그리고 `r`<sup>`(i)`</sup>에 대하여,  
  - `y`<sup>`d`</sup>`= (y`<sup>`(1)`</sup>`, y`<sup>`(2)`</sup>`, ..., y`<sup>`(N)`</sup>`)` : vector of outcome
  - `ŷ`<sup>`d`</sup>`= (ŷ`<sup>`(1)`</sup>`, ŷ`<sup>`(2)`</sup>`, ..., ŷ`<sup>`(N)`</sup>`)` : vector of prediction
  - `r`<sup>`d`</sup>`= (r`<sup>`(1)`</sup>`, r`<sup>`(2)`</sup>`, ..., r`<sup>`(N)`</sup>`)` : vector of residuals  

- rms(r<sup>d</sup>) (RMS prediction error) 를 최소화하기 위한 model parameters `θ`<sub>`i`</sub>를 선정해야 한다.  
  - 이 문제는 least squares problem으로 결정될 수 있다.  

<br>

- Element `A`<sub>`ij`</sub>`= f`<sub>`i`</sub>`(x`<sup>`(i)`</sup>`)`인 `N x p` matrix `A`를 정의하자.  
즉, 그러므로 `ŷ`<sup>`d`</sup>`= Aθ`.  
![expression_1][def3]
![expression_2][def4]  

<br>

- 결국 Least square data fitting : sum of squares of the prediction errors를 최소화시키는 `θ`를 선택하는 것.  
(residual sum of squares, RSS)  

  - 만약 `A`의 columns들이 linearly independent 하다면, `θ̂`는 RSS를 최소화한다.  
  ![theta_hat_minimize_RSS][def5]  
  - RSS는 least squares가 `θ = θ̂`로 들어맞을 때 최소화된다.  
  - ![msse][def6] : minimum sum square error
  - ![mmse][def7] : minimum mean-square fitting error (MMSE)

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 김도형 교수님의 선형대수학 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details>

[def]: https://i.imgur.com/qtp4ezK.png
[def2]: https://i.imgur.com/lfwsILd.png
[def3]: https://i.imgur.com/qpuIBQm.png
[def4]: https://i.imgur.com/kn8fgnK.png
[def5]: https://i.imgur.com/hLngTgN.png
[def6]: https://i.imgur.com/RrOGK3c.png
[def7]: https://i.imgur.com/suk1RZk.png