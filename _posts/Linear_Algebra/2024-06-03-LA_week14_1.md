---
layout: post
title: "[선형대수학] 14주차 (1) - Least squares data fitting"
excerpt: "Setup, Data, Model, Least sqaures data fitting"

tags:
  - [선형대수학, Math]

toc: true

date: 2024-06-03
last_modified_at: 2024-06-10
---
## Least squares data fitting
### 1. Setup
- scalar `y`와 `n`-vector `x`가 model에 의해 연관지어진다고 믿자.  
`y ≈ f(x)`  

  - `x` : vector of independent variables 라고 불린다.  
  (***feature vector***)

  - `y` : outcome or response variable 이라고 불린다.  
  (***somthing we want to predict***)  

- `f : R`<sup>`n`</sup>`-> R`은 `x`와 `y`간의 관계를 제공한다.  

  => 하지만 우리는 `f`, 즉 `x`와 `y` 사이의 **'true' relationship을 제공하는 이 `f`를 모른다.**  

  <br>

### 2. Data
- 어떠한 **데이터**가 주어졌다.  
![data][def]  

  - `n`-vector `x`<sup>`(i)`</sup> (feature vector), scalar `y`<sup>`(i)`</sup>은 data sample `i`에 대한 것이다.  

  - `x`<sup>`(i)`</sup>, `y`<sup>`(i)`</sup>를 한 쌍으로하여 `i`<sup>`th`</sup>**data pair**라고 부른다.  
    - 또는 *observations*, *examples*, *samples* or *measurements* 라고도 불린다.  
    - `x`<sub>`j`</sub><sup>`(i)`</sup>는 `x`<sup>`(i)`</sup>의 `j`<sup>`th`</sup> feature이다.  

    <br>

### 3. Model
- Model `f̂ : R`<sup>`n`</sup>`-> R`은 `f`의 **guess** 혹은 **approximation**이다.  
  - *model*, *prediction function*, *predictor* 등 다양하게 불린다.  

- 예를들면, ***linear in the parameters*** 방법이 존재한다.  
![linear_in_the_parameters][def2]  
  - `f`<sub>`i`</sub>`: R`<sup>`n`</sup>`-> R` 은 우리가 선택한 basis functions (or feature mapping).  
  - `θ`<sub>`i`</sub> 들은 우리가 선택한 model parameters.  

- 우리의 목표는 `i = 1, 2, ..., N`에 대하여 `y`<sup>`(i)`</sup>`≈ f̂(x`<sup>`(i)`</sup>`)`에 근사하도록 하는 `f̂`를 찾는 것이 되겠다.  

- `ŷ`<sup>`(i)`</sup>`≈ f̂(x`<sup>`(i)`</sup>`)` 은 `y`<sup>`(i)`</sup>에 대한 prediction이다.  

<br>

### 4. Least squares data fitting
- Prediction error (or residual) : `r`<sup>`(i)`</sup>`= y`<sup>`(i)`</sup>`- ŷ`<sup>`(i)`</sup>

- `N`-vectors `y`<sup>`(i)`</sup>, `ŷ`<sup>`(i)`</sup>, 그리고 `r`<sup>`(i)`</sup>에 대하여,  
  - `y`<sup>`d`</sup>`= (y`<sup>`(1)`</sup>`, y`<sup>`(2)`</sup>`, ..., y`<sup>`(N)`</sup>`)` : vector of outcome
  - `ŷ`<sup>`d`</sup>`= (ŷ`<sup>`(1)`</sup>`, ŷ`<sup>`(2)`</sup>`, ..., ŷ`<sup>`(N)`</sup>`)` : vector of prediction
  - `r`<sup>`d`</sup>`= (r`<sup>`(1)`</sup>`, r`<sup>`(2)`</sup>`, ..., r`<sup>`(N)`</sup>`)` : vector of residuals  

- **rms(r<sup>d</sup>) (RMS prediction error)** 를 **최소화**하기 위한 **model parameters `θ`<sub>`i`</sub>를 선정**해야 한다.  
  - 이 문제는 **least squares problem으로 결정**될 수 있다.  

- Element `A`<sub>`ij`</sub>`= f`<sub>`i`</sub>`(x`<sup>`(i)`</sup>`)`인 `N x p` matrix `A`를 정의하자.  
즉, 그러므로 `ŷ`<sup>`d`</sup>`= Aθ`.  
![expression_1][def3]
![expression_2][def4]  

<br>

- 결국 ***Least square data fitting*** : **sum of squares of the prediction errors를 최소화시키는 `θ`를 선택**하는 것.  
(**residual sum of squares**, ***RSS***)  

  - 만약 `A`의 **columns들이 linearly independent** 하다면,  
  **pseudo-inverse를 적용**할 수 있게된다.  
  이제 아래 `θ̂`가 RSS를 최소화한다.  
  ![theta_hat_minimize_RSS][def5]  
  - **RSS**는 least squares가 **`θ = θ̂`로 들어맞을 때 최소화**된다.  
  - ![msse][def6] : minimum sum square error
  - ![mmse][def7] : minimum mean-square fitting error (***MMSE***)   

  <br>

### 5. Fitting a constant model
- Approximation Model `f̂`를 다시 가져와보자.  
![approximation_model_f_hat][def2]  
- 이번엔 가능한 간단한 경우를 가지고, 직접 판단해볼 것이다.  
  - `p = 1` 이고,  
  - 모든 `x`에 대하여 `f`<sub>`1`</sub>`(x) = 1` 인 경우를 놓고 보자.  
  - 즉, `f̂(x) = θ`<sub>`1`</sub> -> 상수가 되었다.  

- 따라서, `f̂(x) = θ`<sub>`1`</sub> 이므로, 상수 `θ`<sub>`1`</sub>를 아주 최적의 상수 값으로 설정하는 것이,  
곧 data `y`<sup>`(1)`</sup>`, ..., y`<sup>`(N)`</sup>을 잘 approximate 하는 것이 되겠다.  

- 위에서, Element `A`<sub>`ij`</sub>`= f`<sub>`i`</sub>`(x`<sup>`(i)`</sup>`)`인 `N x p` matrix `A`를 정의하여 Model을 다르게 표현할 수 있었다.  
![matrix_expression][def3]  

  - 그런데 `A` : `N x 1` matrix `1`(ones vector)가 되었으므로, 자연스럽게 linearly independent columns이다.  
  즉, pseudo-inverse를 적용할 수 있다.  
  ![apply_pseudo-inverse][def5]  
  => ![theta_hat_is_avg_of_approximation][def8]  
  (`N`<sup>`-1`</sup>` = 1/N` 이므로..)  
  
  - 즉, `f̂(x) = avg(y`<sup>`d`</sup>`)`  
  -> 해석하자면, 최적의 상수 값 설정은, 간단하게 주어진 데이터의 평균이라는 의미가 되겠다.  

  - 추가적으로 MMSE(minimum mean-square fitting error)  
  ![mmse][def7] 이므로,  
  MMSE = `std(y`<sup>`d`</sup>`)`<sup>`2`</sup>  
  RMS error = `std(y`<sup>`d`</sup>`)`<sup>  

  <br>

### 6. Fitting univariate functions  
- `n = 1`일 때, 우리는 function `f : R -> R`로 책정할 수 있겠다.  
- data `(x`<sub>`i`</sub>`, y`<sub>`i`</sub>`)`와, model function `ŷ = f̂(x)` 를 그려 표시해 볼 수 있다.  

<br>

#### Straight-line fit
- `p = 2`이고, `f`<sub>`1`</sub>`(x) = 1`, `f`<sub>`2`</sub>`(x) = x` 이라고 하자.  
- Model은 `f̂(x) = θ`<sub>`1`</sub>`+ θ`<sub>`2`</sub>`x` 의 형태가 된다.  
![model_linear_function][def9]  
  - Matrix `A`로 표현하자면, matrix `A`의 형태는 이러하다.  
  ![matrix_A_straight_line_fit][def10]  

- 만약 `A`가 linearly independent 하다면, (적어도 `x`<sup>`(i)`</sup> 간 두 개의 다른 값을 가진다면)  
최적의 straight line에 놓이는 parameters `(θ`<sub>`1`</sub>`, θ`<sub>`2`</sub>`)`는,  
다음과 같이 주어진 data에 fit 할것이다.  
![parameters_int_the_optimal_straight][def11]  

  - Gram matrix `A`<sup>`T`</sup>`A`  
  ![gram_matrix][def12]  

  - 2-vector `A`<sup>`T`</sup>`y`  
  ![2-vector][def13]  

  - 그런데 Gram Matrix `A`<sup>`T`</sup>`A`는 `2 x 2` Matrix이다.  
  앞서 우리는 `2 x 2` matrix의 inverse를 구하는 공식이 있었다.  
  Let's apply.   
    - formula  
    ![2x2_matrix_inverse_formula][def14]  

  - 이제, `(A`<sup>`T`</sup>`A)`<sup>`-1`</sup>`A`<sup>`T`</sup>`y`<sup>`d`</sup> 에 대한 식을 작성할 수 있게 되었다.  
  ![expression][def15]  

    - `θ̂`<sub>`1`   
    ![theta_hat_1_1][def16]  
    ![theta_hat_1_2][def17]  

    - `θ̂`<sub>`2`   
    ![theta_hat_2][def18]   

    - 즉, 우리가 원하는 Model `f̂(x) = θ`<sub>`1`</sub>`+ θ`<sub>`2`</sub>`x` 은,  
    ![model_finally][def19]  

    - `std(y`<sup>`d`</sup>`) ≠ 0 `일 때,  
    ![model_finally_another_form][def20]  
      - left-hand side : 예측한 결과값과 평균 결과값의 차이를 그것의 표준편차로 나눈 것
      - right-hand-side : correlation coefficient `ρ` x feature data에 대한 같은 양(left side)




<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 김도형 교수님의 선형대수학 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details>

[def]: https://i.imgur.com/qtp4ezK.png
[def2]: https://i.imgur.com/lfwsILd.png
[def3]: https://i.imgur.com/qpuIBQm.png
[def4]: https://i.imgur.com/kn8fgnK.png
[def5]: https://i.imgur.com/hLngTgN.png
[def6]: https://i.imgur.com/RrOGK3c.png
[def7]: https://i.imgur.com/suk1RZk.png
[def8]: https://i.imgur.com/ltJRHNX.png
[def9]: https://i.imgur.com/A6KB4aC.png
[def10]: https://i.imgur.com/Gevt2i2.png
[def11]: https://i.imgur.com/YIQV5tT.png
[def12]: https://i.imgur.com/tmMWe0X.png
[def13]: https://i.imgur.com/xX79HpV.png
[def14]: https://i.imgur.com/qkCfoA1.png
[def15]: https://i.imgur.com/uphbecm.png
[def16]: https://i.imgur.com/VdrIbqE.png
[def17]: https://i.imgur.com/BexFJmM.png
[def18]: https://i.imgur.com/rQsepzf.png
[def19]: https://i.imgur.com/c4XFDxH.png
[def20]: https://i.imgur.com/GSept0d.png