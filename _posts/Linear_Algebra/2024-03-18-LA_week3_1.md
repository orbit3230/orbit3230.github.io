---
layout: post
title: "[선형대수학] 3주차 (1) - Clustering"
excerpt: "Angle, Correlation coefficient, Clustering, Group Representative, k-means algorithm"

tags:
  - [선형대수학, Math]

toc: true

date: 2024-03-18
last_modified_at: 2024-03-18
---
## Angle
### 1. Angles
- 두 nonzero Vectors `a`와 `b` 간 Angle은 다음과 같이 정의된다.  
![angle][def]
  - **arccos(아크코사인)** 이란, cosine의 역함수이다.  

- 위 식을 다시 정리해보자면, 다음과 같이 표현할 수 있다. 이 표현을 기억하자.  
![angle_2][def2]

<br>

### 2. Classification of angles
- 만약 두 vector 간 angle이 주어졌다면,  
그 값에 따라 둘의 관계를 알아볼 수 있다.  

  - `θ = π/2` : `a`<sup>`T`</sup>`b = 0` 이므로,  
  두 vector는 서로 직각이며, **연관성이 떨어진다**고 볼 수 있다.
  - `θ = 0` : `a`<sup>`T`</sup>`b = ∥a∥∥b∥` 이므로,  
  두 vector는 서로 alined하며, **연관성이 높다 못해 똑같다**고 볼 수 있다.  
  - `θ = π` : `a`<sup>`T`</sup>`b = -∥a∥∥b∥` 이므로,  
  두 vector는 서로 anti-alined하며, **정 반대이다**고 볼 수 있다.
  - `θ ≤ π/2` : `a`<sup>`T`</sup>`b ≥ 0` 이므로,  
  두 vector는 서로 예각(acute angle)을 이루며, **연관성이 높다**고 볼 수 있다.  
  - `θ ≥ π/2` : `a`<sup>`T`</sup>`b ≤ 0` 이므로,  
  두 vector는 서로 둔각(obtuse angle)을 이루며, **연관성이 낮다**고 볼 수 있다.

  <br>

### 3. Spherical distance
<sub> 중요한 내용은 아니다.
- 만약 `a`와`b`가 구의 radius `R` 위에 있다면,  
구를 따라가는 distance는 `Rθ` 이다.
![spherical_distance][def3]

<br>

### 4. Document dissimilarity by angles
- [지난번에,][def4] Distance를 활용하여  
Word count histogram에 따른 Documents 간 유사성을 따져볼 수 있었다.  

- Distance와 마찬가지로, Angle을 활용해서도 두 데이터(Vector)가 얼마나 유사성을 띄는지 알아볼 수 있다.  
![document_dissimilarity_by_angles][def5]
  - 위에서 봤다시피, angle이 `0`에 가까울 수록 **연관성이 높으며**,  
  `π/2`에 가까울 수록 **연관성이 낮다**.  

  <br>

### 5. Norm of sum via angles
![angle_2][def2]
- 위 식을 응용하여, 두 vector 간 합의 **Norm** 값을 재정의해볼 수 있겠다.  
![norm_of_sum][def6]

  - 만약 `θ = 0` 이라면,  
  `∥x + y∥ = ∥x∥ + ∥y∥`이 성립할 수 있을 것이다.
  - 만약 `θ = π/2` 라면,  
  `∥x + y∥`<sup>`2`</sup>` = ∥x∥`<sup>`2`</sup>` + ∥y∥`<sup>`2`</sup>이 성립할 수 있을 것이다.  

- 따라서 `−1 ≤ cos θ ≤ 1`의 범위 안에서만 값이 결정되므로,  
아래와 같은 부등식이 성립하기도 한다.
![norm_of_sum_inequality][def7]  

<br>

### 6. Correlation coefficient
- Vector `a`와 `b`, 그리고 **de-meaned vector**는
![de-meaned_vector][def8]  
와 같다.  

- `a`와 `b`의 상관관계를 나타내는 계수 (**Correlation coefficient**) `ρ = cos θ`는 다음과 같이 작성할 수도 있다.  
![de-meaned_correlation_coefficient][def9]

  - `ρ = cos θ`의 결과 값을 분석하자면,  
    - `ρ = 0` -> `a`와 `b`는 uncorrelated(연관이 없다)
    - `ρ > 0.8` -> `a`와 `b`는 highly correlated(매우 연관성이 높다)
    - `ρ < -0.8` -> `a`와 `b`는 highly anti-correlated(매우 정반대의 성향을 띈다)

- 각각 highly correlated, highly anti-correlated, uncorrelated 한 상황을 그래프로 보자.
![correlation_graph][def10]

<br>

## Clustering
- N개의 `n`-vectors `x`<sub>`1`</sub>, `x`<sub>`2`</sub>, . . . , `x`<sub>`n`</sub> 들이 주어진다.

  - 우리의 목표는, 이들을 **`k`개의 Group** 혹은 **Cluster들로 나누는 것**이다.

    - 이 때, 같은 그룹의 vector들은 다른 것들보다 가까이에 있길 바란다.

- 이렇게 비슷한 위치(성질)의 vector들 끼리 그룹화 하는 것을 ***Clustering*** 이라고 한다.  

<br>

### 1. Clustering Applications
- Clustering이 적용되는 예시들을 보자.  

  - (1) 문서의 분류와 Topic 파악
    - `x`<sub>`i`</sub>는 document `i`에 대한 word count histogram이 되겠다.
  - (2) 환자 분류
    - `x`<sub>`i`</sub>는 환자 속성, 검사 결과, 증상 등이 되겠다.
  - (3) 시장 손님 분류
    - `x`<sub>`i`</sub>는 customer `i`에 대한 구매 내역이나 다른 속성들이 되겠다.
  - (4) 금융 부문
    - `x`<sub>`i`</sub>는 company `i`에 대한 재정적 속성에 대한 `n`-vector가 되겠다.

<br>

### 2. Clustering objective
- `1`, . . ., `k`는 group numbers이고 `c`는 `N`-vector이다.  
`c`<sub>`i`</sub>은 `x`<sub>`i`</sub>에 대한 group이다.  
(e.g., `N=5`, `k=3`, `c=(3, 1, 1, 1, 2)`)  

- `G`<sub>`j`</sub>는 group `j`에 해당하는 집합이고,  
`G`<sub>`j`</sub>`= {i|c`<sub>`i`</sub>`= j}` 으로 작성된다.
(e.g., `G`<sub>`1`</sub>`= {2, 3, 4}`, `G`<sub>`2`</sub>`= {5}`, `G`<sub>`3`</sub>`= {1}`)

- Group representative(그룹 대표) : `n`-vector `z`<sub>`1`</sub>, . . ., `z`<sub>`k`</sub>
  - 여기서 우리는 `∥x`<sub>`i`</sub>`- z`<sub>`c`<sub>`i`</sub></sub>`∥`가 최소한이 되길 원한다.  
  => 바로 이것이 **Clustering**에서 원하는 **목표**이다.

<br>

- **from** vector **to** associated representatives(`x`<sub>`i`</sub>`- z`<sub>`c`<sub>`i`</sub></sub>)의 distance에 대한 제곱의 평균(Mean square)는 다음과 같다.
![j_clust][def11]
  - 바로 이 `J`<sup>`clust`</sup>가 Clustering이 잘 된 정도를 나타내며,  
  값이 작을 수록 더 잘 clustering 된 것이다.  
- `J`<sup>`clust`</sup> 값은,  
**cluster assignment**(어떻게 그룹을 할당 하였는가)와
**choice of group representative**(대푯값을 무엇으로 잡았는가)에 의존한다.
  - 따라서, 다시말해 우리의 목표는  
  `J`<sup>`clust`</sup>값을 최소화 시킬 수 있는 clustering `c`<sub>`i`</sub>와, representatives `z`<sub>`j`를 찾는 일이 되겠다.

  <br>

## Algorithm
### 1. Partitioning the vectors given the representatives
- 주어진 대푯값으로 vector들을 나누자.  

- group representatives `z`<sub>`1`</sub>, . . ., `z`<sub>`k`</sub>이 고정되어 있다고 가정해보자.  
  - 어떻게 vector들을 그룹화할 수 있을까?  
  즉, 다시말해 `c`<sub>`1`</sub>, . . ., `c`<sub>`N`</sub>을 어떻게 선정할까?

![j_clust][def11]

- 잘 보면, `c`<sub>`i`</sub>의 선택은 `J`<sup>`clust`</sup> 속에서 오직  
`i`번째 term인 `(1/N)∥x`<sub>`i`</sub>`- z`<sub>`c`<sub>`i`</sub></sub>`∥`에만 영향을 끼친다.

- `c`<sub>`i`</sub>을 `∥x`<sub>`i`</sub>`- z`<sub>`j`</sub>`∥` 값을 최소화 시킬 수 있는 값 `j`가 되도록 선택한다.  
![minimize_j_clust][def12]
  - 이 말인 즉슨, **각각**의 vector들을 가장 **가까운 representative**로 대입하자는 의미이다.

  - 그렇다면, representative를 정하는 방법은 무엇일까?

  <br>

### 2. Choosing group representative given the partition
- `J`<sup>`clust`</sup>은 `J`<sub>`1`</sub>, . . ., `J`<sub>`k`</sub>의 합으로 이루어진다.
![j_clust_is_sum][def13]

  - 그리고 각 group에 대한 `J`<sub>`j`</sub>은 이렇게 작성할 수 있겠다.
  ![j_j][def14]
  
  - group representative `z`<sub>`j`</sub>의 선택은, 오직 해당 그룹의 `J`<sub>`j`</sub> 내에서만 영향을 미친다.

- 따라서 `z`<sub>`j`</sub>을 선택할 때는 group `j`에서 vector에 대한 **distance의 제곱의 평균(mean square)을 최소화**해야한다.

  - 이것은 바로 vector `x`<sub>`i`</sub>의 **그룹 내에서의 mean**(or average or centroid(중심))이다.  
  ![vector_mean][def15]

<br>

### 3. `k`-means algorithm
- 위 1번과 2번을 반복하는 알고리즘이, **`k`-means**라는 이름으로 유명한 알고리즘이다.

- 방식은,
  - (1) **updating the partition**
    - vector들의 그룹화를 `k`개의 그룹으로 다시한다.  
    그리고는 각각의 vector `x`<sub>`i`</sub>들을 대표값 vector `z`<sub>`1`</sub>, . . ., `z`<sub>`k`</sub>에 assign 시킨다.
  - (2) **updating representatives**
    - 각각의 그룹 `j = 1, ..., k`에 대하여, 대표값 `z`<sub>`j`</sub>를 group `j` 내 vector들의 평균으로 재설정한다.

- 이 순서대로 **step by step** 하다보면, `J`<sup>`clust`</sup>이 **점점 작아지면서 좋은 clustering**이 되어간다.  
(물론, 가장 작은 `J`<sup>`clust`</sup>를 보장하진 못한다. `k`-means 알고리즘은 heuristic하다.)
![convergence][def17]

  - 언제까지 이 행위를 반복하는가? **대표값 `z`<sub>`j`</sub>의 집합에 더 이상 변화가 없을 때 까지.**

- 첫 representative 값을 어떻게 정하냐에 따라서, 알고리즘의 결과로서 partition이 다르게 나올 수 있다.  

  - 이 알고리즘을 여러 번 수행하되, 첫 representative 값을 각기 다르게 설정하여 수행해보고,  
  가장 작게 나온 `J`<sup>`clust`</sup>을 가장 잘 나온 것으로 보고 채택하면 되겠다.
  ![iterations][def16]

  <br>

- 아래는 `k`-means 알고리즘의 step별 변화를 잘 보여주는 적용 예시이다.

![data][def18]
![iterator1][def19]
![iterator2][def20]
![iterator3][def21]
![iterator10][def22]

<br>
<br>
<br>
<br>
<details>
<summary>주의사항</summary>
<div markdown="1">

이 포스팅은 강원대학교 김도형 교수님의 선형대수학 수업을 들으며 내용을 정리 한 것입니다.  
수업 내용에 대한 저작권은 교수님께 있으니,  
다른 곳으로의 무분별한 내용 복사를 자제해 주세요.

</div>
</details>

[def]: https://i.imgur.com/2j6jPk6.png
[def2]: https://i.imgur.com/kTIeGJD.png
[def3]: https://i.imgur.com/u2BHYLJ.png
[def4]: https://orbit3230.github.io/2024/03/14/LA_week2_3/#4-document-dissimilarity%EC%9C%A0%EC%82%AC%EC%84%B1-%EA%B2%80%EC%82%AC
[def5]: https://i.imgur.com/aEy4HIH.png
[def6]: https://i.imgur.com/eEtOuk8.png
[def7]: https://i.imgur.com/8h7EYof.png
[def8]: https://i.imgur.com/1X6MEJT.png
[def9]: https://i.imgur.com/775J0vA.png
[def10]: https://i.imgur.com/xni7yC8.png
[def11]: https://i.imgur.com/SJa7cck.png
[def12]: https://i.imgur.com/hrBrGyL.png
[def13]: https://i.imgur.com/5LktpAp.png
[def14]: https://i.imgur.com/w0oXzVf.png
[def15]: https://i.imgur.com/2kfUtP7.png
[def16]: https://i.imgur.com/vftHrqm.png
[def17]: https://i.imgur.com/mkn5MFa.png
[def18]: https://i.imgur.com/4Hum2Vo.png
[def19]: https://i.imgur.com/kJh7w7P.png
[def20]: https://i.imgur.com/nY14Aup.png
[def21]: https://i.imgur.com/C4Fxmvq.png
[def22]: https://i.imgur.com/jAEyXIz.png